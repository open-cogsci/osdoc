{
  "Download": {
    "fr": "Télécharger",
    "de": "Herunterladen"
  },
  "<script>\nfunction startDownload(url) {\n\tdocument.getElementById('click-here').href = url\n\twindow.location.href = url\n\tdocument.getElementById('download-started').style.display = 'block'\n\tdocument.getElementById('download-started').scrollIntoView()\n}\n</script>\n\n<div class=\"info-box\" id=\"download-started\" markdown=\"1\" style=\"display:none;\">\n\n<h3>Your download should start shortly!</h3>\n\n<a role=\"button\" class=\"btn btn-success btn-align-left\" href=\"https://www.buymeacoffee.com/cogsci\">\n<span class=\"glyphicon glyphicon-heart\" aria-hidden=\"true\"></span>\nHelp us stay focused and buy us a coffee!\n</a>\n\nCoffee keeps us awake so that we can develop free software and answer your questions on the support forum!\n\nClick <a id=\"click-here\">here</a> if your download doesn't start.\n</div>\n\n\n## Overview\n\n<notranslate>\ntoc:\n exclude: [Overview]\n mindepth: 2\n maxdepth: 3\n</notranslate>\n\n\n## All download options\n\nThe latest $status$ version is $version$ *$codename$*, released on $release-date$ ([release notes](http://osdoc.cogsci.nl/$branch$/notes/$notes$)).\n\n\n### Windows\n\nThe Windows package is based on Python 3.11 for 64 bit systems. The installer and `.zip` packages are identical, except for the installation. Most people download the installer package (green button).\n\n<a role=\"button\" class=\"btn btn-success btn-align-left\" onclick=\"startDownload('$url-windows-exe-py3$')\">\n\t<b>Standard</b> Windows installer (.exe)\n</a>\n\n<a role=\"button\" class=\"btn btn-default btn-align-left\" onclick=\"startDownload('$url-windows-zip-py3$')\">\n\t<b>Standard</b> Windows no installation required (.zip)\n</a>\n\n\n### Mac OS\n\nThere are currently no prerelease packages of OpenSesame 4.0 for Mac OS. Please use conda or pip.\n{: .page-notification}\n\n[This article](https://support.apple.com/en-in/guide/mac-help/mh40616/mac) on the Mac OS support site explains how to override the security settings of Mac OS that will by default prevent OpenSesame from launching.\n\nThe package below is built for Intel processors but also runs on ARM (M1) processors.\n\n<a role=\"button\" class=\"btn btn-default btn-align-left\" onclick=\"startDownload('$url-osx-dmg-x64-py3$')\">\n\t<b>Python 3 for Intel x64</b> Mac OS package (.dmg)\n</a>\n\nTo install OpenSesame with [Homebrew](https://brew.sh/), run the following command in a terminal:\n\n```bash\nbrew install --cask opensesame\n```\n\n\n### Ubuntu\n\nPackages are developed and tested on Ubuntu 22.04 Jammy Jellyfish. Packages are only available for 22.04 and 22.10.\n\nIf you have OpenSesame 3.X installed, first deinstall all packages . This is required to avoid package conflicts due to slight renaming of some packages in OpenSesame 4.0.\n\n```bash\n# If necessary: uninstall OpenSesame 3.X\nsudo apt remove python3-opensesame python3-pyqode.python python3-pyqode.core python3-rapunzel python3-opensesame-extension* python3-opensesame-plugin*\n```\n\nNext, to add the required repositories to your software sources and install OpenSesame (and Rapunzel), run the following commands in a terminal:\n\n```bash\n# Add repository for stable packages\nsudo add-apt-repository ppa:smathot/cogscinl\n# Add repository for development packages\nsudo add-apt-repository ppa:smathot/milgram\n# Install OpenSesame 4.X packages plus useful extensions\nsudo apt install python3-opensesame python3-rapunzel python3-opensesame-extension-updater python3-pygaze python3-pygame python3-opensesame-extension-language_server\n```\n\nSome commonly used packages are not available through the PPA. You can install them through `pip`:\n\n```bash\n# Install optional packages that are only available through pip\npip install opensesame-extension-osweb opensesame-plugin-psychopy opensesame-plugin-media_player_mpy http://files.cogsci.nl/expyriment-0.10.0+opensesame2-py3-none-any.whl\n```\n\nPsychoPy is best installed through pip, because the Ubuntu package is currently broken. \n\n```bash\n# Install psychopy\npip install psychopy psychopy_sounddevice python-bidi arabic_reshaper\n```\n\n\n### PyPi (crossplatform)": {
    "fr": "<script>\nfunction startDownload(url) {\n\tdocument.getElementById('click-here').href = url\n\twindow.location.href = url\n\tdocument.getElementById('download-started').style.display = 'block'\n\tdocument.getElementById('download-started').scrollIntoView()\n}\n</script>\n\n<div class=\"info-box\" id=\"download-started\" markdown=\"1\" style=\"display:none;\">\n\n<h3>Votre téléchargement devrait commencer sous peu !</h3>\n\n<a role=\"button\" class=\"btn btn-success btn-align-left\" href=\"https://www.buymeacoffee.com/cogsci\">\n<span class=\"glyphicon glyphicon-heart\" aria-hidden=\"true\"></span>\nAidez-nous à rester concentrés et offrez-nous un café !\n</a>\n\nLe café nous permet de rester éveillés pour développer des logiciels gratuits et répondre à vos questions sur le forum d'assistance !\n\nCliquez <a id=\"click-here\">ici</a> si votre téléchargement ne démarre pas.\n</div>\n\n\n## Aperçu\n\n<notranslate>\ntoc:\n exclude: [Aperçu]\n mindepth: 2\n maxdepth: 3\n</notranslate>\n\n\n## Toutes les options de téléchargement\n\nLa dernière version $status$ est $version$ *$codename$*, sortie le $release-date$ ([notes de version](http://osdoc.cogsci.nl/$branch$/notes/$notes$)).\n\n\n### Windows\n\nLe package Windows est basé sur Python 3.11 pour les systèmes 64 bits. Les packages d'installation et `.zip` sont identiques, à l'exception de l'installation. La plupart des gens téléchargent le package d'installation (bouton vert).\n\n<a role=\"button\" class=\"btn btn-success btn-align-left\" onclick=\"startDownload('$url-windows-exe-py3$')\">\n\t<b>Standard</b> Installateur Windows (.exe)\n</a>\n\n<a role=\"button\" class=\"btn btn-default btn-align-left\" onclick=\"startDownload('$url-windows-zip-py3$')\">\n\t<b>Standard</b> Windows sans installation requise (.zip)\n</a>\n\n\n### Mac OS\n\nIl n'y a actuellement pas de packages de pré-version d'OpenSesame 4.0 pour Mac OS. Veuillez utiliser conda ou pip.\n{: .page-notification}\n\n[Cet article](https://support.apple.com/fr-fr/guide/mac-help/mh40616/mac) sur le site d'assistance Mac OS explique comment contourner les paramètres de sécurité de Mac OS qui empêchent par défaut le lancement d'OpenSesame.\n\nLe package ci-dessous est conçu pour les processeurs Intel mais fonctionne également sur les processeurs ARM (M1).\n\n<a role=\"button\" class=\"btn btn-default btn-align-left\" onclick=\"startDownload('$url-osx-dmg-x64-py3$')\">\n\t<b>Python 3 pour Intel x64</b> Package Mac OS (.dmg)\n</a>\n\nPour installer OpenSesame avec [Homebrew](https://brew.sh/), exécutez la commande suivante dans un terminal :\n\n```bash\nbrew install --cask opensesame\n```\n\n\n### Ubuntu\n\nLes packages sont développés et testés sur Ubuntu 22.04 Jammy Jellyfish. Les packages sont uniquement disponibles pour les versions 22.04 et 22.10.\n\nSi vous avez OpenSesame 3.X installé, désinstallez d'abord tous les packages. Ceci est nécessaire pour éviter les conflits de packages dus à un léger renommage de certains packages dans OpenSesame 4.0.\n\n```bash\n# Si nécessaire : désinstaller OpenSesame 3.X\nsudo apt remove python3-opensesame python3-pyqode.python python3-pyqode.core python3-rapunzel python3-opensesame-extension* python3-opensesame-plugin*\n```\n\nEnsuite, pour ajouter les dépôts nécessaires à vos sources de logiciels et installer OpenSesame (et Rapunzel), exécutez les commandes suivantes dans un terminal :\n\n```bash\n# Ajouter le dépôt pour les packages stables\nsudo add-apt-repository ppa:smathot/cogscinl\n# Ajouter le dépôt pour les packages de développement\nsudo add-apt-repository ppa:smathot/milgram\n# Installer les packages OpenSesame 4.X ainsi que les extensions utiles\nsudo apt install python3-opensesame python3-rapunzel python3-opensesame-extension-updater python3-pygaze python3-pygame python3-opensesame-extension-language_server\n```\n\nCertains packages couramment utilisés ne sont pas disponibles via le PPA. Vous pouvez les installer via `pip` :\n\n```bash\n# Installer les packages optionnels qui sont uniquement disponibles via pip\npip install opensesame-extension-osweb opensesame-plugin-psychopy opensesame-plugin-media_player_mpy http://files.cogsci.nl/expyriment-0.10.0+opensesame2-py3-none-any.whl\n```\n\nPsychoPy est préférablement installé via pip, car le package Ubuntu est actuellement défectueux. \n\n```bash\n# Installer psychopy\npip install psychopy psychopy_sounddevice python-bidi arabic_reshaper\n```\n\n\n### PyPi (multiplateforme)"
  },
  "All packages can be pip-installed. Note that OpenSesame is called `opensesame-core` on PyPi.\n\n```bash\npip install --pre opensesame-core rapunzel opensesame-extension-osweb opensesame-extension-updater opensesame-plugin-psychopy opensesame-plugin-media_player_mpy\npip install psychopy psychopy_sounddevice pygame http://files.cogsci.nl/expyriment-0.10.0+opensesame2-py3-none-any.whl https://github.com/smathot/PyGaze/releases/download/prerelease%2F0.8.0a3/python_pygaze-0.8.0a3-py3-none-any.whl\n```\n\nOnce you have installed all packages, you can simply run OpenSesame by (after having activated the correct environment) running:\n\n```bash\nopensesame\n```\n\nOr for the Rapunzel code editor:\n\n```bash\nrapunzel\n```\n\n\n### Anaconda (cross-platform)\n\nFirst, create a new Python environment for OpenSesame (optional):\n\n```bash\nconda create -n opensesame-py3\nconda activate opensesame-py3\n```\n\nNext, add the relevant channels (`cogsci`) and (`conda-forge`) and install all relevant packages. Make sure that `pyqode.core` and `pyqode.python` are >= 3.2 from the `cogsci` channel, and not the older versions from the `conda-forge` channel.\n\n```bash\nconda config --add channels conda-forge --add channels cogsci\nconda install opensesame opensesame-extension-osweb opensesame-extension-updater opensesame-plugin-psychopy rapunzel pygaze\n```\n\nSome packages are not available through conda. You can use `pip install` for these.\n\n```bash\npip install soundfile pygame psychopy psychopy-sounddevice http://files.cogsci.nl/expyriment-0.10.0+opensesame2-py3-none-any.whl\n```\n\nOnce you have installed all packages, you can simply run OpenSesame by (after having activated the correct environment) running:\n\n```bash\nopensesame\n```\n\nOr for the Rapunzel code editor:\n\n```bash\nrapunzel\n```\n\n\n### Older versions\n\nOlder versions can be downloaded from GitHub releases:\n\n- <https://github.com/open-cogsci/OpenSesame/releases>\n\n\n### Source code\n\nThe source code of OpenSesame is available on [GitHub](https://github.com/open-cogsci/OpenSesame).\n\n\n## Tips\n\n\n### Which version of Python to use?\n\nOpenSesame is currently built and tested with Python 3.11.0. Others versions of Python >=3.7 work but are not extensively tested. Python 2 is no longer supported. The last release that included a Python 2 package was 3.3.12, which can still be downloaded from the [release archive](https://github.com/open-cogsci/OpenSesame/releases/tag/release%2F3.3.12).\n\n\n### When (not) to update?\n\n- Update while developing and testing your experiment; it is always best to use the latest version of OpenSesame.\n- Do not update while running an experiment; that is, do not update while you are collecting data.\n- Run an experiment with the same version of OpenSesame that you used for developing and testing.\n\n\n### Manually upgrading packages\n\nOpenSesame is a regular Python environment, and you can upgrade packages with `pip` or `conda` as described here:\n\n- <https://rapunzel.cogsci.nl/manual/environment/>\n\n\n### Tips for system administrators\n\n- When a new major version of OpenSesame is released (with a version ending in 0, e.g. 3.1.0), it is generally followed quickly by one or two maintenance releases (e.g. 3.1.1 and 3.1.2) that address major bugs. Therefore, if you are installing OpenSesame on systems that you do not update often, it is best to wait until the second or third maintenance release (e.g. 3.0.2, 3.1.3, etc.). That way you minimize the risk of rolling out a version of OpenSesame that contains major bugs.\n- The Windows installer allows you to silently install OpenSesame using the `/S` flag.\n": {
    "fr": "Tous les packages peuvent être installés avec pip. Notez qu'OpenSesame s'appelle `opensesame-core` sur PyPi.\n\n```bash\npip install --pre opensesame-core rapunzel opensesame-extension-osweb opensesame-extension-updater opensesame-plugin-psychopy opensesame-plugin-media_player_mpy\npip install psychopy psychopy_sounddevice pygame http://files.cogsci.nl/expyriment-0.10.0+opensesame2-py3-none-any.whl https://github.com/smathot/PyGaze/releases/download/prerelease%2F0.8.0a3/python_pygaze-0.8.0a3-py3-none-any.whl\n```\n\nUne fois que vous avez installé tous les packages, vous pouvez simplement exécuter OpenSesame en (après avoir activé le bon environnement) exécutant :\n\n```bash\nopensesame\n```\n\nOu pour l'éditeur de code Rapunzel :\n\n```bash\nrapunzel\n```\n\n\n### Anaconda (multiplateforme)\n\nD'abord, créez un nouvel environnement Python pour OpenSesame (facultatif) :\n\n```bash\nconda create -n opensesame-py3\nconda activate opensesame-py3\n```\n\nEnsuite, ajoutez les canaux pertinents (`cogsci`) et (`conda-forge`) et installez tous les packages pertinents. Assurez-vous que `pyqode.core` et `pyqode.python` sont >= 3.2 à partir du canal `cogsci`, et non pas les anciennes versions à partir du canal `conda-forge`.\n\n```bash\nconda config --add channels conda-forge --add channels cogsci\nconda install opensesame opensesame-extension-osweb opensesame-extension-updater opensesame-plugin-psychopy rapunzel pygaze\n```\n\nCertains packages ne sont pas disponibles via conda. Vous pouvez utiliser `pip install` pour ceux-ci.\n\n```bash\npip install soundfile pygame psychopy psychopy-sounddevice http://files.cogsci.nl/expyriment-0.10.0+opensesame2-py3-none-any.whl\n```\n\nUne fois que vous avez installé tous les packages, vous pouvez simplement exécuter OpenSesame en (après avoir activé le bon environnement) exécutant :\n\n```bash\nopensesame\n```\n\nOu pour l'éditeur de code Rapunzel :\n\n```bash\nrapunzel\n```\n\n\n### Versions antérieures\n\nLes versions antérieures peuvent être téléchargées à partir des versions GitHub :\n\n- <https://github.com/open-cogsci/OpenSesame/releases>\n\n\n### Code source\n\nLe code source d'OpenSesame est disponible sur [GitHub](https://github.com/open-cogsci/OpenSesame).\n\n\n## Astuces\n\n\n### Quelle version de Python utiliser ?\n\nOpenSesame est actuellement construit et testé avec Python 3.11.0. D'autres versions de Python >= 3.7 fonctionnent, mais ne sont pas testées de manière approfondie. Python 2 n'est plus pris en charge. La dernière version qui incluait un package Python 2 était la 3.3.12, qui peut toujours être téléchargée à partir de l'[archive de versions](https://github.com/open-cogsci/OpenSesame/releases/tag/release%2F3.3.12).\n\n\n### Quand (ne pas) mettre à jour ?\n\n- Mettez à jour pendant le développement et les tests de votre expérience ; il est toujours préférable d'utiliser la dernière version d'OpenSesame.\n- Ne mettez pas à jour pendant le déroulement d'une expérience ; c'est-à-dire, ne mettez pas à jour pendant que vous collectez des données.\n- Exécutez une expérience avec la même version d'OpenSesame que celle que vous avez utilisée pour la développer et la tester.\n\n\n### Mise à niveau manuelle des packages\n\nOpenSesame est un environnement Python régulier, et vous pouvez mettre à jour les packages avec `pip` ou `conda` comme décrit ici :\n\n- <https://rapunzel.cogsci.nl/manual/environment/>\n\n\n### Conseils pour les administrateurs système\n\n- Lorsqu'une nouvelle version majeure d'OpenSesame est publiée (avec une version se terminant par 0, par exemple 3.1.0), elle est généralement suivie rapidement par une ou deux versions de maintenance (par exemple 3.1.1 et 3.1.2) qui résolvent les problèmes majeurs. Par conséquent, si vous installez OpenSesame sur des systèmes que vous ne mettez pas à jour souvent, il est préférable d'attendre la deuxième ou la troisième version de maintenance (par exemple 3.0.2, 3.1.3, etc.). Ainsi, vous minimisez le risque de déployer une version d'OpenSesame qui contient des bogues majeurs.\n- L'installateur Windows vous permet d'installer silencieusement OpenSesame en utilisant le flag `/S`."
  },
  "Binus University 2022 workshop": {
    "fr": "Atelier Binus University 2022"
  },
  "\n<notranslate>[TOC]</notranslate>\n\n\n## Practical information\n\n- Host: Binus University\n- Location: online\n- Dates: \n    - Day 1: Friday, April 1st, 2022, 08:00-12:00 CEST, 13:00 - 17:00 Jakarta\n    - Day 2: Wednesday, April 20th, 2022, 08:00-12:00 CEST, 13:00 - 17:00 Jakarta\n    - Day 3: Thursday, April 21st, 2022, 08:00-12:00 CEST, 13:00 - 17:00 Jakarta\n    - Day 4: Wednesday, April 27th, 2022, 08:00-12:00 CEST, 13:00 - 17:00 Jakarta\n- Presenter: Sebastiaan Mathôt\n- [Spreadsheet with participant overview](https://binusianorg.sharepoint.com/sites/WyLab/_layouts/15/guestaccess.aspx?guestaccesstoken=wvSvcpJoabwkk99w%2BuZhpJX58EgC6O4ug%2FgWYbN4H%2BI%3D&docid=2_15f25248401074bb79187e333dcb63088&rev=1&e=iPtB1U)\n\n\n## Description\n\nIn this four-day, hands-on, online workshop, you will learn how to implement psychological experiments with the open-source software OpenSesame. You will learn:\n\n- How to run experiments online as well as in a traditional laboratory set-up.\n- The limitations and advantages of online and laboratory-based experiments.\n- How to include eye tracking in laboratory-based experiments. (And a sneak-peak at eye-tracking in online experiments!)\n- How to analyze data collected from online and laboratory-based experiments.\n\nFinally, using the skills that you will learn during the workshop, you will design and implement an experiment for your own research, of course with assistance from us! For this purpose, please already think about what kind of experiment you'd like to create, and indicate this in the [participant spreadsheet](https://binusianorg.sharepoint.com/sites/WyLab/_layouts/15/guestaccess.aspx?guestaccesstoken=wvSvcpJoabwkk99w%2BuZhpJX58EgC6O4ug%2FgWYbN4H%2BI%3D&docid=2_15f25248401074bb79187e333dcb63088&rev=1&e=iPtB1U). If you're unsure what kind of experiment you'd like to create, take a look a the list of suggested experiments below.\n\nPlease install OpenSesame on your computer before the workshop. You can download OpenSesame for free from <https://osdoc.cogsci.nl/>. No prior experience with OpenSesame, Python, or JavaScript is required.\n\nI’m looking forward to meeting you all!\n\n— Sebastiaan\n\n\n## Day 1: Introduction (April 1)\n\nSlides: %static:attachments/binus2022/binus-day-1.pdf%\n\n- 13:00 – 14:30: __Introduction to OpenSesame__. A general introduction to the software OpenSesame, followed by a hands-on tutorial in which you will learn the basic concepts of the software.\n- Break\n- 15:00 – 16:00: __Introduction to OpenSesame (continued)__.\n- 16:00 – 17:00: __Free time to develop your own experiment__. What kind of experiment would you like to build for your own research? You will draft a design for your own experiment, which you will continue to work on during the rest of the workshop.\n\n\n## Day 2: Online experiments (April 20)\n\nSlides: %static:attachments/binus2022/binus-day-2.pdf%\n\n- 13:00 – 14:30: __Building an online task (cats, dogs, and capybaras)__. We will start this session with a general introduction to online experiments. This is followed by a hands-on tutorial in which you will implement a task that is suitable for running online. The tutorial includes various assignments of different difficulty levels.\n- Break\n- 15:00 – 16:00: __Using <https://mindprobe.eu> (a JATOS server) to run experiments online__. In this session, you will learn how to use mindprobe.eu, which is a free server that runs the open-source software JATOS, to actually run an experiment online. A mindprobe.eu account will be provided to each participant.\n- 16:00 – 17:00: __Free time to develop your own experiment__. During this session, you will continue to work on your own experiment.\n\n\n## Day 3 : Eye-tracking experiments (April 21)\n\nSlides: %static:attachments/binus2022/binus-day-3.pdf%": {
    "fr": "\n<notranslate>[TOC]</notranslate>\n\n\n## Informations pratiques\n\n- Hôte : Université Binus\n- Lieu : en ligne\n- Dates : \n    - Jour 1 : Vendredi 1er avril 2022, 08:00-12:00 CEST, 13:00 - 17:00 Jakarta\n    - Jour 2 : Mercredi 20 avril 2022, 08:00-12:00 CEST, 13:00 - 17:00 Jakarta\n    - Jour 3 : Jeudi 21 avril 2022, 08:00-12:00 CEST, 13:00 - 17:00 Jakarta\n    - Jour 4 : Mercredi 27 avril 2022, 08:00-12:00 CEST, 13:00 - 17:00 Jakarta\n- Présentateur : Sebastiaan Mathôt\n- [Tableur avec vue d'ensemble des participants](https://binusianorg.sharepoint.com/sites/WyLab/_layouts/15/guestaccess.aspx?guestaccesstoken=wvSvcpJoabwkk99w%2BuZhpJX58EgC6O4ug%2FgWYbN4H%2BI%3D&docid=2_15f25248401074bb79187e333dcb63088&rev=1&e=iPtB1U)\n\n\n## Description\n\nDans cet atelier pratique en ligne de quatre jours, vous apprendrez à mettre en œuvre des expériences psychologiques avec le logiciel open-source OpenSesame. Vous apprendrez :\n\n- Comment réaliser des expériences en ligne et dans un cadre de laboratoire traditionnel.\n- Les limitations et les avantages des expériences en ligne et en laboratoire.\n- Comment intégrer le suivi oculaire dans les expériences en laboratoire. (Et un aperçu du suivi oculaire dans les expériences en ligne !)\n- Comment analyser les données collectées lors d'expériences en ligne et en laboratoire.\n\nEnfin, en utilisant les compétences que vous apprendrez lors de l'atelier, vous concevrez et mettrez en œuvre une expérience pour votre propre recherche, bien sûr avec notre aide ! A cet effet, réfléchissez dès maintenant au type d'expérience que vous aimeriez créer et indiquez-le dans le [tableur des participants](https://binusianorg.sharepoint.com/sites/WyLab/_layouts/15/guestaccess.aspx?guestaccesstoken=wvSvcpJoabwkk99w%2BuZhpJX58EgC6O4ug%2FgWYbN4H%2BI%3D&docid=2_15f25248401074bb79187e333dcb63088&rev=1&e=iPtB1U). Si vous ne savez pas quel type d'expérience vous aimeriez créer, consultez la liste des expériences suggérées ci-dessous.\n\nVeuillez installer OpenSesame sur votre ordinateur avant l'atelier. Vous pouvez télécharger OpenSesame gratuitement à partir de <https://osdoc.cogsci.nl/>. Aucune expérience préalable avec OpenSesame, Python ou JavaScript n'est requise.\n\nJ'ai hâte de vous rencontrer tous !\n\n— Sebastiaan\n\n\n## Jour 1 : Introduction (1er avril)\n\nDiapositives: %static:attachments/binus2022/binus-day-1.pdf%\n\n- 13:00 - 14:30 : __Introduction à OpenSesame__. Une introduction générale au logiciel OpenSesame, suivie d'un tutoriel pratique dans lequel vous apprendrez les concepts de base du logiciel.\n- Pause\n- 15:00 - 16:00 : __Introduction à OpenSesame (suite)__.\n- 16:00 - 17:00 : __Temps libre pour développer votre propre expérience__. Quel type d'expérience aimeriez-vous créer pour votre propre recherche ? Vous rédigerez un plan pour votre propre expérience, sur lequel vous continuerez à travailler pendant le reste de l'atelier.\n\n\n## Jour 2 : Expériences en ligne (20 avril)\n\nDiapositives: %static:attachments/binus2022/binus-day-2.pdf%\n\n- 13:00 - 14:30 : __Création d'une tâche en ligne (chats, chiens et capybaras)__. Nous commencerons cette session par une introduction générale aux expériences en ligne. Cela sera suivi d'un tutoriel pratique dans lequel vous mettrez en œuvre une tâche adaptée pour être réalisée en ligne. Le tutoriel comprend divers devoirs de niveaux de difficulté différents.\n- Pause\n- 15:00 - 16:00 : __Utilisation de <https://mindprobe.eu> (un serveur JATOS) pour réaliser des expériences en ligne__. Au cours de cette session, vous apprendrez à utiliser mindprobe.eu, qui est un serveur gratuit fonctionnant avec le logiciel open-source JATOS, pour réaliser réellement une expérience en ligne. Un compte mindprobe.eu sera fourni à chaque participant.\n- 16:00 - 17:00 : __Temps libre pour développer votre propre expérience__. Durant cette session, vous continuerez à travailler sur votre propre expérience.\n\n\n## Jour 3 : Expériences de suivi oculaire (21 avril)\n\nDiapositives: %static:attachments/binus2022/binus-day-3.pdf%"
  },
  "- 13:00 – 14:30: __Building a self-paced reading task with eye-tracking__. We will start this session with a general introduction to eye tracking. This is followed by a hands-on tutorial in which you will implement a self-paced reading task with basic eye tracking. We will focus on using the EyeLink, which is a specific eye tracker. However, concepts and techniques are largely also applicable to other eye trackers. We will also briefly look at eye tracking in online experiment with WebGazer.js.\n- Break\n- 15:00 – 16:00: __Gaze-contingent eye-tracking.__ You will learn how to implement experiments that react to the eye movements of the participant, that is, gaze-contingent experiments.\n- 16:00 – 17:00: __Free time to develop your own experiment.__ During this session, you will continue to work on your own experiment.\n\n\n## Day 4: Data analysis (April 27)\n\nSlides: %static:attachments/binus2022/binus-day-4.pdf%\n\n- 13:00 – 14:30: __Getting data ready for analysis.__ We will start this session with a general explanation of how data is structured, both for experiments that are conducted online and for experiments that are conducted in a traditional laboratory set-up. Next, we will see how to transform this data into a format that lends itself to statistical analysis in the free statistical software JASP. Specifically, we will learn how data from multiple participants can be merged into a single .csv spreadsheet; we will also learn how data from an online experiment can be downloaded from JATOS and converted to a .csv spreadsheet; finally, we will learn how to create a so-called ‘pivot table’, which lends itself to analysis in JASP.\n- Break\n- 15:00 – 16:30: __Conducting a statistical analysis__. We will use the open-source software JASP to perform statistical tests.\n- 16:30 – 17:00: __Q&A__. We will close the workshop with time for questions and remarks.\n\n\n## Suggested experiments\n\nA list of experiments that are easy to implement, with references to papers that contain a clear results section.\n\n- Attention network task (ANT)\n    - Fan, J., McCandliss, B. D., Sommer, T., Raz, A., & Posner, M. I. (2002). Testing the efficiency and independence of attentional networks. *Journal of cognitive neuroscience*, *14*(3), 340-347.\n- Posner cueing task\n    - For a version that focuses specifically on inhibition of return (IOR), see: Klein, R. M. (2000). Inhibition of return. *Trends in cognitive sciences*, *4*(4), 138-147.\n- Implicit association test (IAT)\n    - For a replication study, see Johnson, D. J., Ampofo, D., Erbas, S. A., Robey, A., Calvert, H., Garriques, V., ... & Dougherty, M. (2021). Cognitive Control and the Implicit Association Test: A Replication of Siegel, Dougherty, and Huber (2012). *Collabra: Psychology*, *7*(1), 27356.\n    - See also <https://osdoc.cogsci.nl/3.3/tutorials/iat>\n- Color categorization, a test for the Sapir-Whorf hypothesis. Note that stimulus creation might be challenging\n    - Brown, A. M., Lindsey, D. T., & Guckes, K. M. (2011). Color names, color categories, and color-cued visual search: Sometimes, color perception is not categorical. *Journal of vision*, *11*(12), 2-2.\n- Cognitive reflection task\n    - Sirota, M., & Juanchich, M. (2018). Effect of response format on cognitive reflection: Validating a two-and four-option multiple choice question version of the Cognitive Reflection Test. *Behavior Research Methods*, *50*(6), 2511-2522.\n- Attentional capture task.\n    - Theeuwes, J. (1992). Perceptual selectivity for color and form. *Perception & Psychophysics*, *51*(6), 599–606. <https://doi.org/10.3758/BF03211656>\n- Visual-search task.\n    - Treisman, A. M., & Gelade, G. (1980). A feature-integration theory of attention. *Cognitive Psychology*, *12*(1), 97–136. <https://doi.org/10.1016/0010-0285(80)90005-5>\n    - See also <https://osdoc.cogsci.nl/3.3/tutorials/intermediate-javascript>\n": {
    "fr": "- 13h00 – 14h30 : __Création d'une tâche de lecture auto-rythmée avec eye-tracking__. Nous commencerons cette session par une introduction générale à l'eye-tracking. Nous poursuivrons ensuite par un tutoriel pratique dans lequel vous mettrez en place une tâche de lecture auto-rythmée avec un suivi oculaire de base. Nous nous concentrerons sur l'utilisation de l'EyeLink, qui est un dispositif de suivi oculaire spécifique. Cependant, les concepts et techniques sont également largement applicables à d'autres dispositifs de suivi oculaire. Nous examinerons également brièvement l'eye-tracking dans les expériences en ligne avec WebGazer.js.\n- Pause\n- 15h00 – 16h00 : __Eye-tracking basé sur le regard.__ Vous apprendrez à mettre en oeuvre des expériences qui réagissent aux mouvements des yeux du participant, c'est-à-dire des expériences basées sur le regard.\n- 16h00 – 17h00 : __Temps libre pour développer votre propre expérience.__ Au cours de cette session, vous continuerez à travailler sur votre propre expérience.\n\n## Jour 4 : Analyse des données (27 avril)\n\nDiapositives : %static:attachments/binus2022/binus-day-4.pdf%\n\n- 13h00 – 14h30 : __Préparation des données pour l'analyse.__ Nous commencerons cette session par une explication générale de la façon dont les données sont structurées, tant pour les expériences menées en ligne que pour celles menées dans un laboratoire traditionnel. Ensuite, nous verrons comment transformer ces données en un format qui se prête à une analyse statistique dans le logiciel statistique gratuit JASP. Plus précisément, nous apprendrons comment les données de plusieurs participants peuvent être fusionnées dans un seul tableur .csv ; nous apprendrons également comment les données d'une expérience en ligne peuvent être téléchargées à partir de JATOS et converties en feuille de calcul .csv ; enfin, nous apprendrons à créer ce que l'on appelle une « table pivot », qui se prête à l'analyse dans JASP.\n- Pause\n- 15h00 – 16h30 : __Conduite d'une analyse statistique.__ Nous utiliserons le logiciel open source JASP pour effectuer des tests statistiques.\n- 16h30 – 17h00 : __Q&R.__ Nous clôturerons l'atelier avec un temps pour les questions et les remarques.\n\n## Expériences suggérées\n\nUne liste d'expériences faciles à mettre en œuvre, avec des références aux articles qui contiennent une section de résultats claire.\n\n- Tâche du réseau d'attention (ANT)\n    - Fan, J., McCandliss, B. D., Sommer, T., Raz, A., & Posner, M. I. (2002). Testing the efficiency and independence of attentional networks. *Journal of cognitive neuroscience*, *14*(3), 340-347.\n- Tâche d'amorçage de Posner\n    - Pour une version qui se concentre spécifiquement sur l'inhibition du retour (IOR), voir : Klein, R. M. (2000). Inhibition of return. *Trends in cognitive sciences*, *4*(4), 138-147.\n- Test d'association implicite (IAT)\n    - Pour une étude de réplication, voir Johnson, D. J., Ampofo, D., Erbas, S. A., Robey, A., Calvert, H., Garriques, V., ... & Dougherty, M. (2021). Cognitive Control and the Implicit Association Test: A Replication of Siegel, Dougherty, and Huber (2012). *Collabra: Psychology*, *7*(1), 27356.\n    - Voir également <https://osdoc.cogsci.nl/3.3/tutorials/iat>\n- Catégorisation des couleurs, un test pour l'hypothèse Sapir-Whorf. Notez que la création de stimuli peut être difficile\n    - Brown, A. M., Lindsey, D. T., & Guckes, K. M. (2011). Color names, color categories, and color-cued visual search: Sometimes, color perception is not categorical. *Journal of vision*, *11*(12), 2-2.\n- Tâche de réflexion cognitive\n    - Sirota, M., & Juanchich, M. (2018). Effect of response format on cognitive reflection: Validating a two-and four-option multiple choice question version of the Cognitive Reflection Test. *Behavior Research Methods*, *50*(6), 2511-2522.\n- Tâche de capture attentionnelle.\n    - Theeuwes, J. (1992). Perceptual selectivity for color and form. *Perception & Psychophysics*, *51*(6), 599–606. <https://doi.org/10.3758/BF03211656>\n- Tâche de recherche visuelle.\n    - Treisman, A. M., & Gelade, G. (1980). A feature-integration theory of attention. *Cognitive Psychology*, *12*(1), 97–136. <https://doi.org/10.1016/0010-0285(80)90005-5>\n    - Voir également <https://osdoc.cogsci.nl/3.3/tutorials/intermediate-javascript>"
  },
  "ESCoP 2017 workshop": {
    "fr": "Atelier ESCoP 2017"
  },
  "<notranslate>[TOC]</notranslate>\n\n\n## About the workshop\n\nThis OpenSesame workshop took place as a pre-conference event before ESCoP 2017.\n\nThe workshop consisted of two main parts. In the first part, corresponding to the Tutorial below, we created a complete experiment together. In the second part, corresponding to the Extra assignments below, the workshop participants improved this experiment by themselves, based on a few suggestions.\n\nYou can download the full experiment, including the solutions of the extra assignments here:\n\n- <http://osf.io/jw7dr>\n\n### When?\n\n- September 3rd, 2017\n- JASP: 09:00 - 12:00\n- OpenSesame: 13:00 - 16:00\n\n### Where?\n\n- University Potsdam\n- Campus III - Griebnitzsee\n- Building 6 (Lecture hall building)\n- August-Bebel-Straße 89\n- 14482 Potsdam\n- Germany\n\n### More info\n\n- Conference site: <http://www.escop2017.org/program>\n- JASP site: <https://jasp-stats.org/>\n\n\n## Screencast\n\n<notranslate>\nvideo:\n source: youtube\n id: EscopTutorial\n videoid: ICa0vPoYrYw\n width: 640\n height: 360\n caption: |\n  The tutorial in video form.\n</notranslate>\n\n\n## The tutorial\n\n<notranslate>\nfigure:\n id: FigMeowingCapybara\n source: meowing-capybara.png\n caption: |\n  Don't be fooled by meowing capybaras! ([Source][capybara_photo])\n</notranslate>\n\n<notranslate>[TOC]</notranslate>\n\nWe will create a simple animal-filled multisensory integration task, in which participants see a picture of a dog, cat, or capybara. A meow or a bark is played while the picture is shown. The participant reports whether a dog or a cat is shown, by pressing the right or the left key. No response should be given when a capybara is shown: those are catch trials.\n\nTo make things more fun, we will design the experiment so that you can run it on [OSWeb](http://osweb.cogsci.nl/), an online runtime for OpenSesame experiments (which is still a work in progress, but it works for basic experiments).\n\nWe make two simple predictions:\n\n- Participants should be faster to identify dogs when a barking sound is played, and faster to identify cats when a meowing sound is played. In other words, we expect a multisensory congruency effect.\n- When participants see a capybara, they are more likely to report seeing a dog when they hear a bark, and more likely to report seeing a cat when they hear a meow. In other words, false alarms are biased by the sound.\n\n\n### Step 1: Download and start OpenSesame\n\nOpenSesame is available for Windows, Linux, Mac OS, and Android (runtime only). This tutorial is written for OpenSesame 3.1.X, and you can use either the version based on Python 2.7 (default) or Python 3.5. You can download OpenSesame from here:\n\n- %link:download%\n\nWhen you start OpenSesame, you will be given a choice of template experiments, and (if any) a list of recently opened experiments (see %FigStartUp).\n\n<notranslate>\nfigure:\n id: FigStartUp\n source: start-up.png\n caption: |\n  The OpenSesame window on start-up.\n</notranslate>\n\nThe *Extended template* provides a good starting point for creating Android-based experiments. However, in this tutorial we will create the entire experiment from scratch. Therefore, we will continue with the 'default template', which is already loaded when OpenSesame is launched (%FigDefaultTemplate). Therefore, simply close the 'Get started!' and (if shown) 'Welcome!' tabs.\n\n<notranslate>\nfigure:\n id: FigDefaultTemplate\n source: default-template.png\n caption: |\n  The structure of the 'Default template' as seen in the overview area.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Background box 1: Basics**": {
    "fr": "## À propos de l'atelier\n\nCet atelier OpenSesame a eu lieu en tant qu'événement pré-conférence avant ESCoP 2017.\n\nL'atelier se composait de deux parties principales. Dans la première partie, correspondant au tutoriel ci-dessous, nous avons créé une expérience complète ensemble. Dans la deuxième partie, correspondant aux missions supplémentaires ci-dessous, les participants à l'atelier ont amélioré cette expérience par eux-mêmes, selon quelques suggestions.\n\nVous pouvez télécharger l'expérience complète, y compris les solutions des missions supplémentaires ici :\n\n- <http://osf.io/jw7dr>\n\n### Quand ?\n\n- 3 septembre 2017\n- JASP : 09:00 - 12:00\n- OpenSesame : 13:00 - 16:00\n\n### Où ?\n\n- Université de Potsdam\n- Campus III - Griebnitzsee\n- Bâtiment 6 (Bâtiment de l'amphithéâtre)\n- August-Bebel-Straße 89\n- 14482 Potsdam\n- Allemagne\n\n### Plus d'infos\n\n- Site de la conférence : <http://www.escop2017.org/program>\n- Site JASP : <https://jasp-stats.org/>\n\n\n## Screencast\n\n<notranslate>\nvidéo:\n source: youtube\n id: EscopTutorial\n videoid: ICa0vPoYrYw\n largeur: 640\n hauteur: 360\n légende: |\n  Le tutoriel sous forme de vidéo.\n</notranslate>\n\n\n## Le tutoriel\n\n<notranslate>\nfigure:\n id: FigMeowingCapybara\n source: meowing-capybara.png\n légende: |\n  Ne te laisse pas berner par les capybaras qui miaulent ! ([Source][capybara_photo])\n</notranslate>\n\n<notranslate>[TOC]</notranslate>\n\nNous allons créer une tâche simple d'intégration multisensorielle remplie d'animaux, dans laquelle les participants voient une photo d'un chien, d'un chat ou d'un capybara. Un miaulement ou un aboiement est joué pendant que la photo est montrée. Le participant indique si un chien ou un chat est montré, en appuyant sur la touche droite ou gauche. Aucune réponse ne doit être donnée lorsqu'un capybara est montré : ce sont des essais surprises.\n\nPour rendre les choses plus amusantes, nous concevrons l'expérience de manière à ce que vous puissiez la réaliser sur [OSWeb](http://osweb.cogsci.nl/), un runtime en ligne pour les expériences OpenSesame (qui est encore en cours de développement, mais fonctionne pour les expériences de base).\n\nNous formulons deux prédictions simples :\n\n- Les participants devraient être plus rapides pour identifier les chiens lorsqu'un aboiement est joué, et plus rapides pour identifier les chats lorsqu'un miaulement est joué. En d'autres termes, nous nous attendons à un effet de congruence multisensorielle.\n- Lorsque les participants voient un capybara, ils sont plus susceptibles de signaler avoir vu un chien lorsqu'ils entendent un aboiement, et plus susceptibles de signaler avoir vu un chat lorsqu'ils entendent un miaulement. En d'autres termes, les fausses alertes sont biaisées par le son.\n\n\n### Étape 1 : Téléchargez et démarrez OpenSesame\n\nOpenSesame est disponible pour Windows, Linux, Mac OS et Android (runtime uniquement). Ce tutoriel est écrit pour OpenSesame 3.1.X, et vous pouvez utiliser la version basée sur Python 2.7 (par défaut) ou Python 3.5. Vous pouvez télécharger OpenSesame ici :\n\n- %link:download%\n\nLorsque vous démarrez OpenSesame, vous aurez le choix entre des expériences modèles et (le cas échéant) une liste d'expériences récemment ouvertes (voir %FigStartUp).\n\n<notranslate>\nfigure:\n id: FigStartUp\n source: start-up.png\n légende: |\n  La fenêtre OpenSesame au démarrage.\n</notranslate>\n\nLe *modèle étendu* offre un bon point de départ pour créer des expériences basées sur Android. Cependant, dans ce tutoriel, nous créerons l'ensemble de l'expérience à partir de zéro. Par conséquent, nous continuerons avec le \"modèle par défaut\", qui est déjà chargé lorsque OpenSesame est lancé (%FigDefaultTemplate). Fermez simplement les onglets \"Get started!\" et (si affichés) \"Welcome!\".\n\n<notranslate>\nfigure:\n id: FigDefaultTemplate\n source: default-template.png\n légende: |\n  La structure du \"modèle par défaut\" telle que vue dans la zone de présentation.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Encadré 1 : Bases**"
  },
  "OpenSesame experiments are collections of *items*. An item is a small chunk of functionality that, for example, can be used to present visual stimuli (the SKETCHPAD item) or to record key presses (the KEYBOARD_RESPONSE item). Items have a type and a name. For example, you might have two items of the type KEYBOARD_RESPONSE with the names *t1_response* and *t2_response*. To make the distinction between item types and item names clear, we will use THIS_STYLE for types, and *this style* for names.\n\nTo give structure to your experiment, two types of items are especially important: the LOOP and the SEQUENCE. Understanding how you can combine LOOPs and SEQUENCEs to build experiments is perhaps the trickiest part of working with OpenSesame, so let's get that out of the way first.\n\nA LOOP is where, in most cases, you define your independent variables. In a LOOP you can create a table in which each column corresponds to a variable, and each row corresponds to a single run of the 'item to run'. To make this more concrete, let's consider the following *block_loop* (unrelated to this tutorial):\n\n<notranslate>\nfigure:\n id: FigLoopTable\n source: loop-table.png\n caption: |\n  An example of variables defined in a loop table. (This example is not related to the experiment created in this tutorial.)\n</notranslate>\n\nThis *block_loop* will execute *trial_sequence* four times. Once while `soa` is 100 and `target` is 'F', once while `soa` is 100 and `target` is 'H', etc. The order in which the rows are walked through is random by default, but can also be set to sequential in the top-right of the tab.\n\nA SEQUENCE consists of a series of items that are executed one after another. A prototypical SEQUENCE is the *trial_sequence*, which corresponds to a single trial. For example, a basic *trial_sequence* might consist of a SKETCHPAD, to present a stimulus, a KEYBOARD_RESPONSE, to collect a response, and a LOGGER, to write the trial information to the log file.\n\n<notranslate>\nfigure:\n id: FigExampleSequence\n source: example-sequence.png\n caption: |\n  An example of a SEQUENCE item used as a trial sequence. (This example is not related to the experiment created in this tutorial.)\n</notranslate>\n\nYou can combine LOOPs and SEQUENCEs in a hierarchical way, to create trial blocks, and practice and experimental phases. For example, the *trial_sequence* is called by the *block_loop*. Together, these correspond to a single block of trials. One level up, the *block_sequence* is called by the *practice_loop*. Together, these correspond to the practice phase of the experiment.\n\n</div>\n\n\n### Step 2: Add a block_loop and trial_sequence\n\nThe default template starts with three items: A NOTEPAD called *getting_started*, a SKETCHPAD called *welcome*, and a SEQUENCE called *experiment*. We don't need *getting_started* and *welcome*, so let's remove these right away. To do so, right-click on these items and select 'Delete'. Don't remove *experiment*, because it is the entry for the experiment (i.e. the first item that is called when the experiment is started).\n\nOur experiment will have a very simple structure. At the top of the hierarchy is a LOOP, which we will call *block_loop*. The *block_loop* is the place where we will define our independent variables (see also Background box 1). To add a LOOP to your experiment, drag the LOOP icon from the item toolbar onto the *experiment* item in the overview area.\n\nA LOOP item needs another item to run; usually, and in this case as well, this is a SEQUENCE. Drag the SEQUENCE item from the item toolbar onto the *new_loop* item in the overview area. OpenSesame will ask whether you want to insert the SEQUENCE into or after the LOOP. Select 'Insert into new_loop'.": {
    "fr": "Les expériences OpenSesame sont des collections d'éléments. Un élément est un petit morceau de fonctionnalité qui, par exemple, peut être utilisé pour présenter des stimuli visuels (l'élément SKETCHPAD) ou pour enregistrer des pressions de touches (l'élément KEYBOARD_RESPONSE). Les éléments ont un type et un nom. Par exemple, vous pourriez avoir deux éléments du type KEYBOARD_RESPONSE avec les noms *t1_response* et *t2_response*. Pour bien distinguer les types et les noms d'éléments, nous utiliserons CE_STYLE pour les types et *ce style* pour les noms.\n\nPour donner une structure à votre expérience, deux types d'éléments sont particulièrement importants : la boucle (LOOP) et la séquence (SEQUENCE). Comprendre comment combiner les LOOPs et les SEQUENCEs pour construire des expériences est peut-être la partie la plus délicate du travail avec OpenSesame, alors abordons ce point en premier.\n\nUne LOOP est l'endroit où, dans la plupart des cas, vous définissez vos variables indépendantes. Dans une LOOP, vous pouvez créer un tableau où chaque colonne correspond à une variable et chaque ligne correspond à une seule exécution de l’”élément à exécuter”. Pour rendre cela plus concret, considérons la *block_loop* suivante (sans rapport avec ce tutoriel) :\n\n<notranslate>\nfigure:\n id: FigLoopTable\n source: loop-table.png\n caption: |\n  Un exemple de variables définies dans un tableau de boucle (LOOP). (Cet exemple n'est pas lié à l'expérience créée dans ce tutoriel.)\n</notranslate>\n\nCe *block_loop* exécutera *trial_sequence* quatre fois. Une fois avec `soa` à 100 et `target` à 'F', une fois avec `soa` à 100 et `target` à 'H', etc. L'ordre dans lequel les lignes sont parcourues est aléatoire par défaut, mais peut aussi être défini en séquentiel en haut à droite de l'onglet.\n\nUne SEQUENCE est composée d'une série d'éléments qui sont exécutés les uns après les autres. Une SEQUENCE prototypique est *trial_sequence*, qui correspond à une seule tentative. Par exemple, une *trial_sequence* basique pourrait contenir un SKETCHPAD, pour présenter un stimulus, un KEYBOARD_RESPONSE, pour recueillir une réponse, et un LOGGER, pour écrire les informations de l'essai dans le fichier journal.\n\n<notranslate>\nfigure:\n id: FigExampleSequence\n source: example-sequence.png\n caption: |\n  Un exemple d'un élément SEQUENCE utilisé comme séquence d'essai. (Cet exemple n'est pas lié à l'expérience créée dans ce tutoriel.)\n</notranslate>\n\nVous pouvez combiner les LOOPs et les SEQUENCEs de manière hiérarchique pour créer des blocs d'essais et des phases de pratique et d'expérimentation. Par exemple, *trial_sequence* est appelée par *block_loop*. Ensemble, ils correspondent à un seul bloc d'essais. À un niveau supérieur, *block_sequence* est appelée par *practice_loop*. Ensemble, ils correspondent à la phase de pratique de l'expérience.\n\n</div>\n\n\n### Étape 2 : Ajout d'un block_loop et d'un trial_sequence\n\nLe modèle par défaut commence avec trois éléments : un NOTEPAD appelé *getting_started*, un SKETCHPAD appelé *welcome* et une SEQUENCE appelée *experiment*. Nous n'avons pas besoin de *getting_started* et de *welcome*, alors supprimons-les tout de suite. Pour ce faire, faites un clic droit sur ces éléments et sélectionnez \"Supprimer\". Ne supprimez pas *experiment* car il s'agit de l'entrée de l'expérience (c'est-à-dire le premier élément qui est appelé lorsque l'expérience démarre).\n\nNotre expérience aura une structure très simple. Le sommet de la hiérarchie est une LOOP, que nous appellerons *block_loop*. C'est dans *block_loop* que nous définirons nos variables indépendantes (voir également l'encadré Contexte 1). Pour ajouter une LOOP à votre expérience, faites glisser l'icône LOOP de la barre d'outils des éléments sur l'élément *experiment* dans la zone d'aperçu.\n\nUn élément LOOP a besoin d'un autre élément pour s'exécuter; en général et dans ce cas également, il s'agit d'une SEQUENCE. Faites glisser l'élément SEQUENCE de la barre d'outils des éléments sur l'élément *new_loop* dans la zone d'aperçu. OpenSesame vous demandera si vous souhaitez insérer la SEQUENCE dans ou après la LOOP. Sélectionnez \"Insérer dans new_loop\"."
  },
  "By default, items have names such as *new_sequence*, *new_loop*, *new_sequence_2*, etc. These names are not very informative, and it is good practice to rename them. Item names must consist of alphanumeric characters and/ or underscores. To rename an item, double-click on the item in the overview area. Rename *new_sequence* to *trial_sequence* to indicate that it will correspond to a single trial. Rename *new_loop* to *block_loop* to indicate that will correspond to a block of trials.\n\nThe overview area of our experiment now looks as in %FigStep3.\n\n<notranslate>\nfigure:\n id: FigStep3\n source: step3.png\n caption: |\n  The overview area at the end of Step 2.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Background box 3: Unused items**\n\n__Tip__ — Deleted items are still available in the Unused Items bin, until you select 'Permanently delete unused items' in the Unused Items tab. You can re-add deleted items to your experiment by dragging them out of the Unused Items bin into a SEQUENCE or LOOP.\n\n</div>\n\n### Step 3: Import images and sound files\n\nFor this experiment, we will use images of cats, dogs, and capybaras. We will also use sound samples of meows and barks. You can download all the required files from here:\n\n- %static:attachments/cats-dogs-capybaras/stimuli.zip%\n\nDownload `stimuli.zip` and extract it somewhere (to your desktop, for example). Next, in OpenSesame, click on the 'Show file pool' button in the main toolbar (or: Menu →View → Show file pool). This will show the file pool, by default on the right side of the window. The easiest way to add the stimuli to the file pool is by dragging them from the desktop (or wherever you have extracted the files to) into the file pool. Alternatively, you can click on the '+' button in the file pool and add files using the file-selection dialog that appears. The file pool will automatically be saved with your experiment.\n\nAfter you have added all stimuli, your file pool looks as in %FigStep4.\n\n<notranslate>\nfigure:\n id: FigStep4\n source: step4.png\n caption: |\n  The file pool at the end of Step 3.\n</notranslate>\n\n### Step 4: Define the experimental variables in the block_loop\n\nConceptually, our experiment has a fully crossed 3×2 design: We have three types of visual stimuli (cats, dogs, and capybaras) which occur in combination with two types of auditory stimuli (meows and barks). However, we have five exemplars for each stimulus type: five meow sounds, five capybara pictures, etc. From a technical point of view, it therefore makes sense to treat our experiment as a 5×5×3×2 design, in which picture number and sound number are factors with five levels.\n\nOpenSesame is very good at generating full-factorial designs. First, open *block_loop* by clicking on it in the overview area. Next, click on the Full-Factorial Design button. This will open a wizard for generating full-factorial designs, which works in a straightforward way: Every column corresponds to an experimental variable (i.e. a factor). The first row is the name of the variable, the rows below contain all possible values (i.e. levels). In our case, we can specify our 5×5×3×2 design as shown in %FigLoopWizard.\n\n<notranslate>\nfigure:\n id: FigLoopWizard\n source: loop-wizard.png\n caption: |\n  The loop wizard generates full-factorial designs.\n</notranslate>\n\nAfter clicking 'Ok', you will see that there is now a LOOP table with four rows, one for each experimental variable. There are 150 cycles (=5×5×3×2), which means that we have 150 unique trials. Your LOOP table now looks as in %FigStep5.\n\n<notranslate>\nfigure:\n id: FigStep5\n source: step5.png\n caption: |\n  The LOOP table at the end of Step 4.\n</notranslate>\n\n### Step 5: Add items to the trial sequence\n\nOpen *trial_sequence*, which is still empty. It's time to add some items! Our basic *trial_sequence* is:": {
    "fr": "Par défaut, les éléments ont des noms tels que *new_sequence*, *new_loop*, *new_sequence_2*, etc. Ces noms ne sont pas très informatifs et il est recommandé de les renommer. Les noms d'éléments doivent être constitués de caractères alphanumériques et/ou de traits de soulignement. Pour renommer un élément, double-cliquez dessus dans la zone d'aperçu. Renommez *new_sequence* en *trial_sequence* pour indiquer qu'il correspondra à un essai unique. Renommez *new_loop* en *block_loop* pour indiquer qu'il correspondra à un bloc d'essais.\n\nLa zone d'aperçu de notre expérience ressemble maintenant à celle figurée dans %FigStep3.\n\n<notranslate>\nfigure:\n id: FigStep3\n source: step3.png\n caption: |\n  La zone d'aperçu à la fin de l'étape 2.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Encadré 3 : Éléments inutilisés**\n\n__Astuce__ — Les éléments supprimés sont toujours disponibles dans la corbeille des éléments inutilisés, jusqu'à ce que vous sélectionniez \"Supprimer définitivement les éléments inutilisés\" dans l'onglet des éléments inutilisés. Vous pouvez remettre les éléments supprimés dans votre expérience en les faisant glisser depuis la corbeille des éléments inutilisés vers une SÉQUENCE ou LOOP.\n\n</div>\n\n### Étape 3 : Importer des images et des fichiers sonores\n\nPour cette expérience, nous utiliserons des images de chats, de chiens et de capybaras. Nous utiliserons également des échantillons sonores de miaulements et d'aboiements. Vous pouvez télécharger tous les fichiers requis ici :\n\n- %static:attachments/cats-dogs-capybaras/stimuli.zip%\n\nTéléchargez `stimuli.zip` et extrayez-le quelque part (sur votre bureau, par exemple). Ensuite, dans OpenSesame, cliquez sur le bouton \"Afficher le pool de fichiers\" dans la barre d'outils principale (ou : Menu → Affichage → Afficher le pool de fichiers). Cela affichera le pool de fichiers, par défaut sur le côté droit de la fenêtre. La façon la plus simple d'ajouter les stimuli au pool de fichiers est de les faire glisser depuis le bureau (ou depuis l'endroit où vous avez extrait les fichiers) vers le pool de fichiers. Vous pouvez également cliquer sur le bouton '+' dans le pool de fichiers et ajouter des fichiers en utilisant la boîte de dialogue de sélection de fichiers qui apparaît. Le pool de fichiers sera automatiquement enregistré avec votre expérience.\n\nAprès avoir ajouté tous les stimuli, votre pool de fichiers ressemblera à celui de la %FigStep4.\n\n<notranslate>\nfigure:\n id: FigStep4\n source: step4.png\n caption: |\n  Le pool de fichiers à la fin de l'étape 3.\n</notranslate>\n\n### Étape 4 : Définir les variables expérimentales dans le block_loop\n\nConceptuellement, notre expérience a un plan croisé complet 3x2 : nous avons trois types de stimuli visuels (chats, chiens et capybaras) qui apparaissent en combinaison avec deux types de stimuli auditifs (miaulements et aboiements). Cependant, nous avons cinq exemplaires pour chaque type de stimulus : cinq sons de miaulement, cinq photos de capybara, etc. D'un point de vue technique, il est donc logique de considérer notre expérience comme un plan 5x5x3x2, dans lequel le numéro d'image et le numéro de son sont des facteurs à cinq niveaux.\n\nOpenSesame est très efficace pour générer des plans factoriels complets. Tout d'abord, ouvrez *block_loop* en cliquant dessus dans la zone d'aperçu. Ensuite, cliquez sur le bouton Full-Factorial Design. Cela ouvrira un assistant pour générer des plans factoriels complets, qui fonctionne de manière simple : chaque colonne correspond à une variable expérimentale (c'est-à-dire un facteur). La première ligne est le nom de la variable, les lignes ci-dessous contiennent toutes les valeurs possibles (c'est-à-dire les niveaux). Dans notre cas, nous pouvons spécifier notre plan 5×5×3×2 comme indiqué dans %FigLoopWizard.\n\n<notranslate>\nfigure:\n id: FigLoopWizard\n source: loop-wizard.png\n caption: |\n  L'assistant de boucle génère des plans factoriels complets.\n</notranslate>\n\nAprès avoir cliqué sur \"Ok\", vous verrez qu'il y a maintenant une table LOOP avec quatre lignes, une pour chaque variable expérimentale. Il y a 150 cycles (=5×5×3×2), ce qui signifie que nous avons 150 essais uniques. Votre table LOOP ressemble maintenant à celle de la %FigStep5.\n\n<notranslate>\nfigure:\n id: FigStep5\n source: step5.png\n caption: |\n  La table LOOP à la fin de l'étape 4.\n</notranslate>\n\n### Étape 5 : Ajouter des éléments à la séquence d'essais\n\nOuvrez *trial_sequence*, qui est encore vide. Il est temps d'ajouter des éléments ! Notre *trial_sequence* de base est :"
  },
  "1. A SKETCHPAD to display a central fixation dot for 500 ms\n2. A SAMPLER to play an animal sound\n3. A SKETCHPAD to display an animal picture\n4. A KEYBOARD_RESPONSE to collect a response\n5. A LOGGER to write the data to file\n\nTo add these items, simply drag them one by one from the item toolbar into the *trial_sequence*. If you accidentally drop items in the wrong place, you can simply re-order them by dragging and dropping. Once all items are in the correct order, give each of them a sensible name. The overview area now looks as in %FigStep6.\n\n<notranslate>\nfigure:\n id: FigStep6\n source: step6.png\n caption: |\n  The overview area at the end of Step 5.\n</notranslate>\n\n### Step 6: Define the central fixation dot\n\nClick on *fixation_dot* in the overview area. This opens a basic drawing board that you can use to design your visual stimuli. To draw a central fixation dot, first click on the crosshair icon, and then click on the center of the display, i.e. at position (0, 0).\n\nWe also need to specify for how long the fixation dot is visible. To do so, change the duration from 'keypress' to 495 ms, in order to specify a 500 ms duration. (See Background box 4 for an explanation.)\n\nThe *fixation_dot* item now looks as in %FigStep7.\n\n<notranslate>\nfigure:\n id: FigStep7\n source: step7.png\n caption: |\n  The *fixation_dot* item at the end of Step 6.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Background box 4: Selecting the correct duration**\n\nWhy specify a duration of 495 if we want a duration of 500 ms? The reason for this is that the actual display-presentation duration is always rounded up to a value that is compatible with your monitor's refresh rate. This may sound complicated, but for most purposes the following rules of thumb are sufficient:\n\n1. Choose a duration that is possible given your monitor's refresh rate. For example, if your monitor's refresh rate is 60 Hz, this means that every frame lasts 16.7 ms (=1000 ms/60 Hz). Therefore, on a 60 Hz monitor, you should always select a duration that is a multiple of 16.7 ms, such as 16.7, 33.3, 50, 100, etc.\n2. In the duration field of the SKETCHPAD specify a duration that is a few milliseconds shorter than what you're aiming for. So if you want to present a SKETCHPAD for 50 ms, choose a duration of 45. If you want to present a SKETCHPAD for 1000 ms, choose a duration of 995. Etcetera.\n\nFor a detailed discussion of experimental timing, see:\n\n- %link:timing%\n\n</div>\n\n### Step 7: Define the animal sound\n\nOpen *animal_sound*. The SAMPLER item provides a number of options, the most important being the sound file that should be played. Click on the browse button to open the file-pool selection dialog, and select one of the sound files, such as `bark1.ogg`.\n\nOf course, we don't want to play the same sound over-and-over again! Instead, we want to select a sound based on the variables `sound` and `sound_nr` that we have defined in the *block_loop* (Step 5). To do this, simply replace the part of the string that you want to have depend on a variable by the name of that variable between square brackets. More specifically, 'bark1.ogg' becomes '[sound][sound_nr].ogg', because we want to replace 'bark' by the value of the variable `sound` and '1' by the value of `sound_nr`.\n\nWe also need to change the duration of the SAMPLER. By default, the duration is 'sound', which means that the experiment will pause while the sound is playing. Change the duration to 0. This does not mean that the sound will be played for only 0 ms, but that the experiment will advance right away to the next item, while the sound continues to play in the background. The item *animal_sound* now looks as shown in %FigStep8.\n\n<notranslate>\nfigure:\n id: FigStep8\n source: step8.png\n caption: |\n  The item *animal_sound* at the end of Step 7.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Background box 5: Variables**\n\nFor more information about using variables, see:\n\n- %link:manual/variables%\n\n</div>": {
    "fr": "1. Un SKETCHPAD pour afficher un point de fixation central pendant 500 ms\n2. Un SAMPLER pour jouer un son d'animal\n3. Un SKETCHPAD pour afficher une image d'animal\n4. Une KEYBOARD_RESPONSE pour collecter une réponse\n5. Un LOGGER pour écrire les données dans un fichier\n\nPour ajouter ces éléments, faites-les simplement glisser un par un de la barre d'outils des éléments dans la *trial_sequence*. Si vous déposez accidentellement des éléments au mauvais endroit, vous pouvez simplement les réorganiser en les faisant glisser et déposer. Une fois tous les éléments dans le bon ordre, donnez-leur un nom sensé. La zone d'aperçu ressemble maintenant à celle de %FigStep6.\n\n<notranslate>\nfigure:\n id: FigStep6\n source: step6.png\n caption: |\n  La zone d'aperçu à la fin de l'étape 5.\n<notranslate>\n\n### Étape 6 : Définir le point de fixation central\n\nCliquez sur *fixation_dot* dans la zone d'aperçu. Cela ouvre une planche à dessin de base que vous pouvez utiliser pour concevoir vos stimuli visuels. Pour dessiner un point de fixation central, cliquez d'abord sur l'icône en forme de croix, puis sur le centre de l'affichage, c'est-à-dire à la position (0, 0).\n\nNous devons également préciser combien de temps le point de fixation est visible. Pour ce faire, changez la durée de 'keypress' à 495 ms, afin de spécifier une durée de 500 ms. (Voir l'encadré 4 pour une explication.)\n\nL'élément *fixation_dot* ressemble maintenant à %FigStep7.\n\n<notranslate>\nfigure:\n id: FigStep7\n source: step7.png\n caption: |\n  L'élément *fixation_dot* à la fin de l'étape 6.\n<notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Encadré 4 : Choisir la bonne durée**\n\nPourquoi choisir une durée de 495 si nous voulons une durée de 500 ms ? La raison en est que la durée réelle de présentation de l'affichage est toujours arrondie à une valeur compatible avec le taux de rafraîchissement de votre moniteur. Cela peut sembler compliqué, mais pour la plupart des besoins, les règles suivantes sont suffisantes :\n\n1. Choisissez une durée possible étant donné le taux de rafraîchissement de votre moniteur. Par exemple, si le taux de rafraîchissement de votre moniteur est de 60 Hz, cela signifie que chaque image dure 16,7 ms (= 1000 ms/60 Hz). Par conséquent, sur un moniteur de 60 Hz, vous devez toujours choisir une durée multiple de 16,7 ms, comme 16,7, 33,3, 50, 100, etc.\n2. Dans le champ de durée du SKETCHPAD, spécifiez une durée de quelques millisecondes plus courte que celle que vous visez. Ainsi, si vous voulez présenter un SKETCHPAD pendant 50 ms, choisissez une durée de 45. Si vous voulez présenter un SKETCHPAD pendant 1000 ms, choisissez une durée de 995. Etc.\n\nPour une discussion détaillée sur le minutage des expériences, voir :\n\n- %link:timing%\n\n</div>\n\n### Étape 7 : Définir le son de l'animal\n\nOuvrez *animal_sound*. L'élément SAMPLER fournit un certain nombre d'options, la plus importante étant le fichier sonore à jouer. Cliquez sur le bouton de navigation pour ouvrir la boîte de dialogue de sélection du fichier, et sélectionnez l'un des fichiers sonores, tels que `bark1.ogg`.\n\nBien sûr, nous ne voulons pas jouer le même son encore et encore ! Au lieu de cela, nous voulons sélectionner un son en fonction des variables `sound` et `sound_nr` que nous avons définies dans la *block_loop* (étape 5). Pour ce faire, remplacez simplement la partie de la chaîne sur laquelle vous voulez que la variable dépende par le nom de cette variable entre crochets. Plus précisément, 'bark1.ogg' devient '[sound][sound_nr].ogg', car nous voulons remplacer 'bark' par la valeur de la variable `sound` et '1' par la valeur de `sound_nr`.\n\nNous devons également changer la durée du SAMPLER. Par défaut, la durée est 'sound', ce qui signifie que l'expérience sera interrompue pendant la lecture du son. Changez la durée à 0. Cela ne signifie pas que le son sera lu pendant seulement 0 ms, mais que l'expérience passera immédiatement à l'élément suivant, pendant que le son continue de jouer en arrière-plan. L'élément *animal_sound* ressemble maintenant à celui de %FigStep8.\n\n<notranslate>\nfigure:\n id: FigStep8\n source: step8.png\n caption: |\n  L'élément *animal_sound* à la fin de l'étape 7.\n<notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Encadré 5 : Variables**\n\nPour plus d'informations sur l'utilisation des variables, voir :\n\n- %link:manual/variables%\n\n</div>"
  },
  "### Step 8: Define the animal picture\n\nOpen *animal_picture*. Select the image tool by clicking on the button with the landscape-like icon. Click on the center (0, 0) of the display. In the File Pool dialog that appears, select `capybara1.png`. The capybara's sideways glance will now lazily stare at you from the center of the display. But of course, we don't always want to show the same capybara. Instead, we want to have the image depend on the variables `animal` and `pic_nr` that we have defined in the *block_loop* (Step 5).\n\nWe can use essentially the same trick as we did for *animal_sound*, although things work slightly differently for images. First, right-click on the capybara and select 'Edit script'. This allows you to edit the following line of OpenSesame script that corresponds to the capybara picture:\n\n\tdraw image center=1 file=\"capybara1.png\" scale=1 show_if=always x=0 y=0 z_index=0\n\nNow change the name of image file from 'capybara.png' to '[animal][pic_nr].png':\n\n\tdraw image center=1 file=\"[animal][pic_nr].png\" scale=1 show_if=always x=0 y=0 z_index=0\n\nClick on 'Ok' to apply the change. The capybara is now gone, replaced by a placeholder image, and OpenSesame tells you that one object is not shown, because it is defined using variables. Don't worry, it will be shown during the experiment!\n\nTo remind the participant of the task, also add two response circles, one marked 'dog' on the left side of the screen, and one marked 'cat' on the right side. I'm sure you will able to figure out how to do this with the SKETCHPAD drawing tools. My rendition is shown in %FigStep9. Note that these response circles are purely visual, and we still need to explicitly define the response criteria (see Step 10).\n\nFinally, set 'Duration' field to '0'. This does not mean that the picture is presented for only 0 ms, but that the experiment will advance to the next item (*response*) right away. Since *response* waits for a response, but doesn't change what's on the screen, the target will remain visible until a response has been given.\n\n<notranslate>\nfigure:\n id: FigStep9\n source: step9.png\n caption: |\n  The *animal_picture* SKETCHPAD at the end of Step 8.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Background box 6: Image formats**\n\n__Tip__ -- OpenSesame can handle a wide variety of image formats. However, some (non-standard) `.bmp` formats are known to cause trouble. If you find that a `.bmp` image is not shown, you may want to consider using a different format, such as `.png`. You can convert images easily with free tools such as [GIMP].\n\n</div>\n\n\n### Step 9: Define the response\n\nOpen the *response* item. This is a KEYBOARD_RESPONSE item, which collects a single key press. There are a few options:\n\n- __Correct response__ — let's skip this for now; we'll get back to it in Step 10.\n- __Allowed responses__ is a semicolon-separated list of keys that are accepted. Let's set this to *left;right* to indicate that only the left and right arrow keys are accepted. (The *escape* key pauses the experiment, and is always accepted!)\n- __Timeout__ indicates a duration after which the response will be set to 'None', and the experiment will continue. A timeout is important in our experiment, because participants need to have the opportunity to *not* respond when they see a capybara. So let's set the timeout to 2000.\n- __Flush pending keypresses__ indicates that we should only accept new key presses. This is best left enabled (it is by default).\n\n<notranslate>\nfigure:\n id: FigStep10\n source: step10.png\n caption: |\n  The *response* KEYBOARD_RESPONSE at the end of Step 9.\n</notranslate>\n\n\n### Step 10: Define the correct response": {
    "fr": "### Étape 8: Définir l'image d'animal\n\nOuvrez *animal_picture*. Sélectionnez l'outil Image en cliquant sur le bouton avec l'icône en forme de paysage. Cliquez sur le centre (0, 0) de l'affichage. Dans la boîte de dialogue File Pool qui apparaît, sélectionnez `capybara1.png`. Le regard de côté du capybara vous fixe maintenant paresseusement depuis le centre de l'écran. Mais bien sûr, nous ne voulons pas toujours montrer le même capybara. Au lieu de cela, nous voulons que l'image dépende des variables `animal` et `pic_nr` que nous avons définies dans le *block_loop* (Étape 5).\n\nNous pouvons utiliser essentiellement la même astuce que celle utilisée pour *animal_sound*, bien que les choses fonctionnent légèrement différemment pour les images. Faites d'abord un clic droit sur le capybara et sélectionnez \"Modifier le script\". Cela vous permet de modifier la ligne suivante de script OpenSesame qui correspond à l'image du capybara :\n\n\tdraw image center=1 file=\"capybara1.png\" scale=1 show_if=always x=0 y=0 z_index=0\n\nMaintenant, changez le nom du fichier image de 'capybara.png' à '[animal][pic_nr].png':\n\n\tdraw image center=1 file=\"[animal][pic_nr].png\" scale=1 show_if=always x=0 y=0 z_index=0\n\nCliquez sur 'Ok' pour appliquer le changement. Le capybara a maintenant disparu, remplacé par une image de remplacement, et OpenSesame vous indique qu'un objet n'est pas affiché, car il est défini à l'aide de variables. Ne vous inquiétez pas, il sera affiché pendant l'expérience !\n\nPour rappeler la tâche au participant, ajoutez également deux cercles de réponse, un marqué \"chien\" sur le côté gauche de l'écran et un marqué \"chat\" sur le côté droit. Je suis sûr que vous pourrez comprendre comment faire cela avec les outils de dessin de SKETCHPAD. Mon interprétation est montrée dans %FigStep9. Notez que ces cercles de réponse sont purement visuels, et nous devons encore définir explicitement les critères de réponse (voir Étape 10).\n\nEnfin, définissez le champ \"Durée\" à \"0\". Cela ne signifie pas que l'image est présentée pendant seulement 0 ms, mais que l'expérience passera à l'élément suivant (*response*) immédiatement. Comme *response* attend une réponse, mais ne change pas ce qui est affiché à l'écran, la cible restera visible jusqu'à ce qu'une réponse soit donnée.\n\n<notranslate>\nfigure:\n id: FigStep9\n source: step9.png\n caption: |\n  Le SKETCHPAD *animal_picture* à la fin de l'étape 8.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Encadré 6: Formats d'image**\n\n__Astuce__ -- OpenSesame peut gérer une grande variété de formats d'image. Cependant, certains formats `.bmp` (non standard) sont connus pour poser des problèmes. Si vous trouvez qu'une image `.bmp` n'est pas affichée, vous pouvez envisager d'utiliser un format différent, tel que `.png`. Vous pouvez convertir facilement les images avec des outils gratuits tels que [GIMP].\n\n</div>\n\n\n### Étape 9 : Définir la réponse\n\nOuvrez l'élément *response*. Il s'agit d'un élément KEYBOARD_RESPONSE, qui collecte une seule pression de touche. Il y a quelques options :\n\n- __Réponse correcte__ — passons cela pour l'instant ; nous y reviendrons à l'étape 10.\n- __Réponses autorisées__ est une liste séparée par des points-virgules des touches acceptées. Définissons cela sur *left;right* pour indiquer que seules les touches fléchées gauche et droite sont acceptées. (La touche *escape* met l'expérience en pause et est toujours acceptée !)\n- __Délai__ indique une durée après laquelle la réponse sera définie sur \"Aucune\" et l'expérience continuera. Un délai est important dans notre expérience, car les participants doivent avoir la possibilité de *ne pas* répondre lorsqu'ils voient un capybara. Réglons donc le délai d'attente à 2000.\n- __Vider les touches en attente__ indique que nous devons accepter uniquement les nouvelles pressions de touches. Il est préférable de laisser cette option activée (elle l'est par défaut).\n\n<notranslate>\nfigure:\n id: FigStep10\n source: step10.png\n caption: |\n  La KEYBOARD_RESPONSE *response* à la fin de l'étape 9.\n</notranslate>\n\n\n### Étape 10: Définir la réponse correcte"
  },
  "So far, we haven't defined the correct response for each trial. This is done by defining a `correct_response` variable. You can do this either by creating a `correct_response` column in a LOOP table (here the *block_loop*) and entering the correct responses manually, or by specifying the correct response in a Python INLINE_SCRIPT item—which is what we will do here.\n\nFirst, drag an INLINE_SCRIPT item from the item toolbar and insert it at the top of the *trial_sequence*. (Don't forget to give it a sensible name!) You now see a text editor with two tabs: a *Run* tab, and a *Prepare* tab. You can enter Python code in both tabs, but this code is executed during different phases of the experiment. The *Prepare* phase is executed first whenever a SEQUENCE is executed; this gives all items in the SEQUENCE a chance to perform time consuming operations that could otherwise slow down the experiment at time-sensitive moments. Next, the *Run* phase is executed; this is where the action happens, such as showing a display, collecting a response, etc.\n\nFor more information, see:\n\n- %link:prepare-run%\n\nDefining a correct response is a clear example of something that should be done in the *Prepare* phase. The following script will do the trick:\n\n~~~ .python\nif var.animal == 'dog':\n\tvar.correct_response = 'left'\nelif var.animal == 'cat':\n\tvar.correct_response = 'right'\nelif var.animal == 'capybara':\n\tvar.correct_response = None # A timeout is coded as None!\nelse:\n\traise ValueError('Invalid animal: %s' % var.animal)\n~~~\n\nThis code is almost plain English, but a few pointers may be useful:\n\n- In Python script, experimental variables are not referred to using square brackets (`[my_variable]`), as they are elsewhere in OpenSesame, but as properties of the `var` object (i.e. `var.my_variable`).\n- We also consider the possibility that the animal is neither a dog, a cat, nor a capybara. Of course this should never happen, but by taking this possibility into account, we protect ourselves against typos and other bugs. This is called 'defensive programming'.\n\n\n### Step 11: Define the logger\n\nWe don't need to configure the LOGGER, because its default settings are fine; but let's take a look at it anyway. Click on *logger* in the overview area to open it. You see that the option 'Log all variables (recommended)' is selected. This means that OpenSesame logs everything, which is fine.\n\n<div class='info-box' markdown='1'>\n\n**Background box 8: Always check your data!**\n\n__The one tip to rule them all__ — Always triple-check whether all necessary variables are logged in your experiment! The best way to check this is to run the experiment and investigate the resulting log files.\n\n</div>\n\n### Step 12: Add per-trial feedback\n\nIt is good practice to inform the participant of whether the response was correct or not. To avoid disrupting the flow of the experiment, this type of immediate feedback should be as unobtrusive as possible. Here, we will do this by briefly showing a green fixation dot after a correct response, and a red fixation dot after an incorrect response.\n\nFirst, add two new SKETCHPADs to the end of the *trial_sequence*. Rename the first one to *feedback_correct* and the second one to *feedback_incorrect*. Of course, we want to run only one of these items on any given trial, depending on whether or not the response was correct. To do this, we can make use of the built-in variable `correct`, which has the value 0 after an incorrect response, and 1 after a correct response. (Provided that we have defined `correct_response`, which we did in Step 11.) To tell the *trial_sequence* that the *feedback_correct* item should be called only when the response is correct, we use the following run-if statement:\n\n\t[correct] = 1\n\nThe square brackets around `correct` indicate that this is the name of a variable, and not simply the string 'correct'. Analogously, we use the following run-if statement for the *feedback_incorrect* item:\n\n\t[correct] = 0": {
    "fr": "Jusqu'à présent, nous n'avons pas défini la réponse correcte pour chaque essai. Cela se fait en définissant une variable `correct_response`. Vous pouvez le faire en créant une colonne `correct_response` dans un tableau LOOP (ici, le *block_loop*) et en entrant manuellement les réponses correctes, ou en spécifiant la réponse correcte dans un élément PYTHON INLINE_SCRIPT, ce que nous ferons ici.\n\nPremièrement, faites glisser un élément INLINE_SCRIPT à partir de la barre d'outils des éléments et insérez-le en haut du *trial_sequence*. (N'oubliez pas de lui donner un nom significatif!) Vous voyez maintenant un éditeur de texte avec deux onglets: un onglet *Run* et un onglet *Prepare*. Vous pouvez entrer du code Python dans les deux onglets, mais ce code est exécuté pendant différentes phases de l'expérience. La phase *Prepare* est exécutée en premier chaque fois qu'une SEQUENCE est exécutée ; cela donne à tous les éléments de la SEQUENCE la possibilité d'effectuer des opérations longues qui pourraient autrement ralentir l'expérience à des moments sensibles au temps. Ensuite, la phase *Run* est exécutée ; c'est là que se déroule l'action, comme montrer un affichage, collecter une réponse, etc.\n\nPour plus d'informations, consultez:\n\n- %link:prepare-run %\n\nDéfinir une réponse correcte est un exemple clair de quelque chose qui doit être fait dans la phase *Prepare*. Le script suivant fera l'affaire:\n\n~~~ .python\nif var.animal == 'chien':\n\tvar.correct_response = 'gauche'\nelif var.animal == 'chat':\n\tvar.correct_response = 'droite'\nelif var.animal == 'capybara':\n\tvar.correct_response = None # Un délai d'attente est codé comme None !\nelse:\n\traise ValueError('Animal invalide : %s' % var.animal)\n~~~\n\nCe code est presque en anglais simple, mais quelques conseils peuvent être utiles:\n\n- Dans le script Python, les variables expérimentales ne sont pas mentionnées en utilisant des crochets (`[my_variable]`), comme elles le sont ailleurs dans OpenSesame, mais en tant que propriétés de l'objet `var` (c'est-à-dire `var.my_variable`).\n- Nous considérons également la possibilité que l'animal ne soit ni un chien, ni un chat, ni un capybara. Bien sûr, cela ne doit jamais arriver, mais en tenant compte de cette possibilité, nous nous protégeons contre les fautes de frappe et autres bugs. Cela s'appelle la 'programmation défensive'.\n\n\n### Étape 11: Définir le journal\n\nNous n'avons pas besoin de configurer le LOGGER, car ses paramètres par défaut sont corrects ; mais jetons-y un coup d'œil de toute façon. Cliquez sur *logger* dans la zone d'aperçu pour l'ouvrir. Vous voyez que l'option 'Log all variables (recommended)' est sélectionnée. Cela signifie qu'OpenSesame enregistre tout, ce qui est très bien.\n\n<div class='info-box' markdown='1'>\n\n**Boîte d'informations 8 : Vérifiez toujours vos données !**\n\n__Le seul conseil pour les gouverner tous__ — Vérifiez toujours si toutes les variables nécessaires sont enregistrées dans votre expérience! La meilleure façon de vérifier cela est d'exécuter l'expérience et d'examiner les fichiers de journal résultants.\n\n</div>\n\n### Étape 12: Ajouter un retour d'information par essai\n\nIl est de bon ton d'informer le participant de la réponse était correcte ou non. Pour éviter de perturber le déroulement de l'expérience, ce type de rétroaction immédiate doit être aussi discret que possible. Ici, nous le ferons en montrant brièvement un point de fixation vert après une réponse correcte et un point de fixation rouge après une réponse incorrecte.\n\nPremièrement, ajoutez deux nouveaux SKETCHPADs à la fin du *trial_sequence*. Renommez le premier en *feedback_correct* et le second en *feedback_incorrect*. Bien sûr, nous ne voulons exécuter qu'un seul de ces éléments lors d'un essai donné, en fonction de la réponse correcte ou non. Pour ce faire, nous pouvons utiliser la variable intégrée `correct`, qui a la valeur 0 après une réponse incorrecte et 1 après une réponse correcte. (À condition que nous ayons défini `correct_response`, ce que nous avons fait à l'étape 11.) Pour dire au *trial_sequence* que l'élément *feedback_correct* doit être appelé uniquement lorsque la réponse est correcte, nous utilisons la déclaration run-if suivante:\n\n\t[correct] = 1\n\nLes crochets autour de `correct` indiquent qu'il s'agit du nom d'une variable, et non simplement de la chaîne 'correct'. De même, nous utilisons la déclaration run-if suivante pour l'élément *feedback_incorrect*:\n\n\t[correct] = 0"
  },
  "We still need to give content to the *feedback_correct* and *feedback_incorrect* items. To do this, simply open the items and draw a green or red fixation dot in the center. Also, don't forget to change the durations from 'keypress' to some brief interval, such as 195.\n\nThe *trial_sequence* now looks as shown in %FigStep13.\n\n<notranslate>\nfigure:\n id: FigStep13\n source: step13.png\n caption: |\n  The *trial_sequence* at the end of Step 12.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Background box 9: Conditional statements**\n\nFor more information about conditional 'if' statements, see:\n\n- %link:manual/variables%\n\n</div>\n\n### Step 13: Add instructions and goodbye screens\n\nA good experiment always start with an instruction screen, and ends by thanking the participant for his or her time. The easiest way to do this in OpenSesame is with `form_text_display` items.\n\nDrag two `form_text_display`s into the main *experiment* SEQUENCE. One should be at the very start, and renamed to *form_instructions*. The other should be at the very end, and renamed to *form_finished*. Now simply add some appropriate text to these forms, for example as shown in %FigStep14.\n\n<notranslate>\nfigure:\n id: FigStep14\n source: step14.png\n caption: |\n  The *form_instructions* item at the end of Step 13.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Background box 10: Text**\n\n__Tip__ -- Forms, and text more generally, support a subset of HTML tags to allow for text formatting (i.e. colors, boldface, etc.). This is described here:\n\n- %link:visual%\n\n</div>\n\n### Step 15: Finished!\n\nYour experiment is now finished! Click on the 'Run fullscreen' (`Control+R`) button in the main toolbar to give it a test run. You can also upload the experiment to OSWeb (<http://osweb.cogsci.nl/>) and run it online!\n\n<div class='info-box' markdown='1'>\n\n**Background box 11: Quick run**\n\n__Tip__ — A test run is executed even faster by clicking the orange 'Run in window' button, which doesn't ask you how to save the logfile (and should therefore only be used for testing purposes).\n\n</div>\n\n\n## Extra assignments\n\nThe solutions to these extra assingments can be found in the [experiment file](http://osf.io/jw7dr).\n\n### Extra 1: Add an instruction and goodbye screen\n\nTips:\n\n- SKETCHPAD and FORM_TEXT_DISPLAY can present text\n- Good instructions are brief and concrete\n\n### Extra 2: Analyze the data\n\nTips:\n\n- Run the experiment once on yourself\n- Open the data file in Excel, LibreOffice, or JASP\n\n### Extra 3: Divide the trials into multiple blocks\n\nTips:\n\n- Use a break-if statement to break the loop after (say) 15 trials: `([count_trial_sequence]+1) % 15 = 0`\n- Add a new LOOP-SEQUENCE structure above the *block_loop* to repeat a block of trials multiple times\n- Disable the 'Evaluate on first cycle' option in the *block_loop* so that the break-if statement isn't evaluated when the `count_trial_sequence` variable doesn't yet exist\n- Enable the 'Resume after break' option in the *block_loop* to randomly sample without replacement from the LOOP table\n\n### Extra 4: Add accuracy and average response time feedback after every block\n\nFirst do Extra 3!\n\nTips:\n\n- Use a FEEDBACK item to provide feedback\n- The variables `acc` and `avg_rt` contain the running accuracy and average reaction time\n\n### Extra 5: Counterbalance the response rule\n\nTips:\n\n- The variable `subject_parity` is 'even' or 'odd'\n- This requires a simple INLINE_SCRIPT\n- Make sure that the instructions match the response rule!\n\n\n## References\n\nMathôt, S., Schreij, D., & Theeuwes, J. (2012). OpenSesame: An open-source, graphical experiment builder for the social sciences. *Behavior Research Methods*, *44*(2), 314–324. doi:10.3758/s13428-011-0168-7\n{: .reference}": {
    "fr": "Nous avons encore besoin de donner du contenu aux éléments *feedback_correct* et *feedback_incorrect*. Pour ce faire, ouvrez simplement les éléments et tracez un point de fixation vert ou rouge au centre. N'oubliez pas non plus de changer les durées de 'keypress' à un intervalle bref, comme 195.\n\nLa *trial_sequence* ressemble maintenant à ce qui est montré dans %FigStep13.\n\n<notranslate>\nfigure:\n id: FigStep13\n source: step13.png\n caption: |\n  La *trial_sequence* à la fin de l'étape 12.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Encadré 9 : Instructions conditionnelles**\n\nPour plus d'informations sur les instructions conditionnelles 'if', consultez :\n\n- %link:manual/variables%\n\n</div>\n\n### Étape 13 : Ajouter des instructions et des écrans d'au revoir\n\nUne bonne expérience commence toujours par un écran d'instruction et se termine par des remerciements au participant pour le temps qu'il a consacré. La manière la plus simple de faire cela dans OpenSesame consiste à utiliser des éléments `form_text_display`.\n\nFaites glisser deux `form_text_display` dans la séquence principale de l' *experiment*. L'un doit être placé tout au début et renommé en *form_instructions*. L'autre doit être placé à la toute fin et renommé en *form_finished*. Ajoutez maintenant un texte approprié à ces formulaires, par exemple comme indiqué dans %FigStep14.\n\n<notranslate>\nfigure:\n id: FigStep14\n source: step14.png\n caption: |\n  L'élément *form_instructions* à la fin de l'étape 13.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Encadré 10 : Texte**\n\n__Astuce__ -- Les formulaires, et plus généralement le texte, prennent en charge un sous-ensemble de balises HTML pour permettre la mise en forme du texte (c'est-à-dire les couleurs, le gras, etc.). Ceci est décrit ici :\n\n- %link:visual%\n\n</div>\n\n### Étape 15 : Terminé !\n\nVotre expérience est maintenant terminée ! Cliquez sur le bouton \"Exécuter en plein écran\" (`Control+R`) dans la barre d'outils principale pour effectuer un test. Vous pouvez également télécharger l'expérience sur OSWeb (<http://osweb.cogsci.nl/>) et l'exécuter en ligne !\n\n<div class='info-box' markdown='1'>\n\n**Encadré 11 : Exécution rapide**\n\n__Astuce__ — Une exécution de test est réalisée encore plus rapidement en cliquant sur le bouton orange \"Exécuter dans une fenêtre\", qui ne vous demande pas comment sauvegarder le fichier journal (et ne doit donc être utilisé qu'à des fins de test).\n\n</div>\n\n\n## Travaux pratiques supplémentaires\n\nLes solutions à ces travaux pratiques supplémentaires se trouvent dans le [fichier de l'expérience](http://osf.io/jw7dr).\n\n### Supplément 1 : Ajouter un écran d'instruction et d'au revoir\n\nConseils :\n\n- Les éléments SKETCHPAD et FORM_TEXT_DISPLAY peuvent présenter du texte\n- De bonnes instructions sont brèves et concrètes\n\n### Supplément 2 : Analyser les données\n\nConseils :\n\n- Exécutez l'expérience une fois sur vous-même\n- Ouvrez le fichier de données dans Excel, LibreOffice, ou JASP\n\n### Supplément 3 : Diviser les essais en plusieurs blocs\n\nConseils :\n\n- Utilisez une instruction break-if pour interrompre la boucle après (disons) 15 essais : `([count_trial_sequence]+1) % 15 = 0`\n- Ajoutez une nouvelle structure LOOP-SEQUENCE au-dessus de la *block_loop* pour répéter un bloc d'essais plusieurs fois\n- Désactivez l'option \"Évaluer au premier cycle\" dans la *block_loop* afin que l'instruction break-if ne soit pas évaluée lorsque la variable `count_trial_sequence` n'existe pas encore\n- Activez l'option \"Reprendre après la pause\" dans la *block_loop* pour échantillonner aléatoirement sans remplacement à partir de la table LOOP\n\n### Supplément 4 : Ajouter un retour d'information sur la précision et le temps de réponse moyen après chaque bloc\n\nFaites d'abord le supplément 3 !\n\nConseils :\n\n- Utilisez un élément FEEDBACK pour donner un retour d'information\n- Les variables `acc` et `avg_rt` contiennent la précision en cours et le temps de réaction moyen\n\n### Supplément 5 : Contrebalancer la règle de réponse\n\nConseils :\n\n- La variable `subject_parity` est 'pair' ou 'impair'\n- Cela nécessite un simple script INLINE_SCRIPT\n- Assurez-vous que les instructions correspondent à la règle de réponse !\n\n## Références\n\nMathôt, S., Schreij, D., & Theeuwes, J. (2012). OpenSesame: An open-source, graphical experiment builder for the social sciences. *Behavior Research Methods*, *44*(2), 314–324. doi:10.3758/s13428-011-0168-7\n{: .reference}"
  },
  "[OpenSesame runtime for Android]: /getting-opensesame/android\n[slides]: /attachments/rovereto2014-workshop-slides.pdf\n[modulo]: http://en.wikipedia.org/wiki/Modulo_operation\n[pdf]: /rovereto2014/index.pdf\n[gimp]: http://www.gimp.org/\n[capybara_photo]: https://commons.wikimedia.org/wiki/File:Capybara_Hattiesburg_Zoo_(70909b-58)_2560x1600.jpg\n": {
    "fr": "[OpenSesame runtime pour Android]: /getting-opensesame/android\n[diapositives]: /attachments/rovereto2014-workshop-slides.pdf\n[modulo]: http://fr.wikipedia.org/wiki/Opération_modulo\n[pdf]: /rovereto2014/index.pdf\n[gimp]: http://www.gimp.org/\n[photo_de_capybara]: https://commons.wikimedia.org/wiki/File:Capybara_Hattiesburg_Zoo_(70909b-58)_2560x1600.jpg"
  },
  "Kurt Lewin Institute Workshop 2020 Part 2": {
    "fr": "Atelier de l'Institut Kurt Lewin 2020 Partie 2"
  },
  "## Part 2 Kurt Lewin Institute Workshop 2020\n\n<notranslate>\nfigure:\n id: KLI\n source: KLI.png\n</notranslate>\n\n## Gaze cuing\n\nIn this tutorial, you will create a gaze-cuing experiment, as introduced by [Friesen and Kingstone (1998)][references]. In this experiment, a face is presented in the center of the screen (%FigGazeCuing). This face looks either to the right or to the left. A target letter (an 'F' or an 'H') is presented to the left or right of the face. A distractor stimulus (the letter 'X') is presented on the other side of the face. The task is to indicate as quickly as possible whether the target letter is an 'F' or an 'H'. In the *congruent* condition, the face looks at the target. In the *incongruent* condition, the face looks at the distractor.\n\n\n<notranslate>\nfigure:\n id: FigGazeCuing\n source: gaze-cuing.png\n caption: |\n  The gaze-cuing paradigm [(Friesen and Kingstone, 1998)][references] that you will implement in this tutorial. This example depicts a trial in the incongruent condition, because the smiley looks at the distractor ('X') and not at the target ('F').\n</notranslate>\n\n\n## Prediction\n\nAs you may have guessed, the typical finding is that participants respond faster in the congruent condition, compared to to the incongruent condition, even though the direction of gaze is not predictive of the target location. This shows that our attention is automatically guided by other people's gaze, even in situations where this doesn't serve any purpose. (And even when the face is just a smiley!)\n\nThe experiment consists of a practice and an experimental phase. Visual feedback will be presented after every block of trials, and a sound will be played after every incorrect response.\n\n## Experimental design\n\nThis design:\n\n- is *within-subject*, because all participants do all conditions\n- is *fully-crossed* (or full factorial), because all combinations of conditions occur\n- has three factors (or independent variables):\n    - *gaze side* with two levels (left, right)\n    - *target side* with two levels (left, right)\n    - *target letter* with two levels (F, H)\n\n<notranslate>\nfigure:\n id: conditions\n source: conditions.png\n caption: |\n  The factors in the current experiment are fully crossed. This figure shows the four combinations of the factors *gaze side* and *target side*.\n</notranslate>\n\n## Step 1: Create the main sequence\n\nWhen you start OpenSesame, you'll see a 'Get started!' tab, which shows you a list of templates as well as recently opened experiments (%GetStarted). As before, we will use the 'Extended template'.\n\n<notranslate>\nfigure:\n id: GetStarted\n source: get-started.png\n caption: |\n  OpenSesame's welcome window. Here, we use the 'extended template'.\n</notranslate>\n\nAfter opening the extended template, we start by saving our experiment. To do this, click *File* -> *Save*, brows to the appropriate folder and give your experiment a meaningful name.\n\n<notranslate>\nfigure:\n id: FigGetStarted\n source: get-started.png\n caption: |\n  The 'Get started' dialog on OpenSesame start-up.\n</notranslate>\n\n\n<div class='info-box' markdown='1'>\n\n__Tip__ -- %Hierarchy schematically shows the structure of the experiment that you will create. If you get confused during the tutorial, you can refer to %Hierarchy to see where you are.\n\n<notranslate>\nfigure:\n id: Hierarchy\n source: hierarchy.png\n caption: |\n  A schematic representation of the structure of the 'Gaze cuing' experiment.\n</notranslate>\n\n</div>\n\n\n__Removing some items from the overvew area__\n\nAs a start, remove the following items from the experimental hierarchy (right mouse click -> remove or shortcult `Del`):\n\n- *about_this_template*\n- *instructions*\n- *end_of_practice*\n- *end_of_experiment*\n\nNext, remove the items from the 'Unused items' bin, by clicking on this part of the overview area and clicking 'Permanently delete unused items'.\n\n__Append a `form_text_display` item for the instruction display__": {
    "fr": "## Partie 2 Atelier Kurt Lewin Institute 2020\n\n<notranslate>\nfigure:\n id: KLI\n source: KLI.png\n</notranslate>\n\n## Orientation du regard\n\nDans ce tutoriel, vous allez créer une expérience d'orientation du regard, comme présenté par [Friesen et Kingstone (1998)][references]. Dans cette expérience, un visage est présenté au centre de l'écran (%FigGazeCuing). Ce visage regarde soit à droite, soit à gauche. Une lettre cible (un 'F' ou un 'H') est présentée à gauche ou à droite du visage. Un stimulus distracteur (la lettre 'X') est présenté de l'autre côté du visage. La tâche consiste à indiquer le plus rapidement possible si la lettre cible est un 'F' ou un 'H'. Dans la condition *congruente*, le visage regarde la cible. Dans la condition *incongruente*, le visage regarde le distracteur.\n\n<notranslate>\nfigure:\n id: FigGazeCuing\n source: gaze-cuing.png\n caption: |\n  Le paradigme d'orientation du regard [(Friesen et Kingstone, 1998)][references] que vous allez mettre en œuvre dans ce tutoriel. Cet exemple dépeint un essai dans la condition incongruente, car le smiley regarde le distracteur ('X') et non la cible ('F').\n</notranslate>\n\n## Prédiction\n\nComme vous l'aurez deviné, la découverte typique est que les participants répondent plus rapidement dans la condition congruente, par rapport à la condition incongruente, même si la direction du regard n'est pas prédictive de l'emplacement de la cible. Cela montre que notre attention est automatiquement guidée par le regard des autres, même dans des situations où cela ne sert à rien. (Et même quand le visage est juste un smiley !)\n\nL'expérience se compose d'une phase de pratique et d'une phase expérimentale. Un feedback visuel sera présenté après chaque bloc d'essais et un son sera joué après chaque réponse incorrecte.\n\n## Conception expérimentale\n\nCe design :\n- est *intra-sujet*, car tous les participants font toutes les conditions\n- est *entièrement croisé* (ou factoriel complet), car toutes les combinaisons de conditions se produisent\n- a trois facteurs (ou variables indépendantes) :\n    - *côté du regard* avec deux niveaux (gauche, droite)\n    - *côté de la cible* avec deux niveaux (gauche, droite)\n    - *lettre cible* avec deux niveaux (F, H)\n\n<notranslate>\nfigure:\n id: conditions\n source: conditions.png\n caption: |\n  Les facteurs de l'expérience actuelle sont entièrement croisés. Cette figure montre les quatre combinaisons des facteurs *côté du regard* et *côté de la cible*.\n</notranslate>\n\n## Étape 1 : Créer la séquence principale\n\nLorsque vous lancez OpenSesame, vous verrez un onglet 'Commencer !' qui vous montre une liste de modèles ainsi que des expériences récemment ouvertes (%GetStarted). Comme précédemment, nous utiliserons le modèle 'Extended template'.\n\n<notranslate>\nfigure:\n id: GetStarted\n source: get-started.png\n caption: |\n  Fenêtre de bienvenue d'OpenSesame. Ici, nous utilisons le modèle \"extended template\".\n</notranslate>\n\nAprès avoir ouvert le modèle étendu, nous commençons par enregistrer notre expérience. Pour ce faire, cliquez sur *Fichier* -> *Enregistrer*, parcourez le dossier approprié et donnez un nom significatif à votre expérience.\n\n<notranslate>\nfigure:\n id: FigGetStarted\n source: get-started.png\n caption: |\n  La boîte de dialogue \"Commencer\" au démarrage d'OpenSesame.\n</notranslate>\n\n\n<div class='info-box' markdown='1'>\n\n__Astuce__ -- %Hierarchy montre schématiquement la structure de l'expérience que vous allez créer. Si vous êtes confus pendant le tutoriel, vous pouvez vous référer à %Hierarchy pour voir où vous en êtes.\n\n<notranslate>\nfigure:\n id: Hierarchy\n source: hierarchy.png\n caption: |\n  Représentation schématique de la structure de l'expérience \"Orientation du regard\".\n</notranslate>\n\n</div>\n\n\n__Suppression de certains éléments de la zone de survol__\n\nPour commencer, supprimez les éléments suivants de la hiérarchie expérimentale (clic droit de la souris -> supprimer ou raccourci `Del`) :\n\n- *about_this_template*\n- *instructions*\n- *end_of_practice*\n- *end_of_experiment*\n\nEnsuite, supprimez les éléments de la corbeille \"Éléments inutilisés\" en cliquant sur cette partie de la zone de présentation et en cliquant sur \"Supprimer définitivement les éléments inutilisés\".\n\n__Ajouter un élément `form_text_display` pour l'affichage des instructions__"
  },
  "As the name suggests, a `form_text_display` is a form that displays text. We are going to use a `form_text_display` (instead of a `sketchpad` item) to give instructions to the participant at the beginning of the experiment.\n\nDrag a `form_text_display` from the item toolbar (under 'Form') onto the *experiment* sequence in the overview area. When you let go, a new `form_text_display` item will be inserted into the *experiment* sequence. Rename this item to *instructions*. Make sure the item appears at the very beginning of the experiment.\n\n__Append a new `form_text_display` item, for the goodbye message__\n\nWhen the experiment is finished, we should say goodbye to the participant. For this we need another `form_text_display` item. Drag a `form_text_display` from the item toolbar onto *experimental_loop*. In the pop-up menu that appears, select 'Insert after experimental_loop'. Rename this item to *goodbye*.\n\nThe overview area of your experiment now looks like %FigStep1. This would be a good time to save your experiment (shortcut: `Ctrl+S`).\n\n<notranslate>\nfigure:\n id: FigStep1\n source: step1.png\n caption: |\n  The overview area at the end of the step 1.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n__Background box__\n\n__Tip__ — If you don't like having many tabs open, you can close all tabs except the currently opened one by clicking on the 'Close other tabs' button in the main toolbar (shortcut: `Ctrl+T`).\n\n</div>\n\n## Step 2: Fill the block loop with independent variables\n\nAs the name suggests, *block_loop* corresponds to a single block of trials. In the previous step we created the *block_loop*, but we still need to define the independent variables that will be varied within the block. Our experiment has three independent variables:\n\n- __gaze_cue__ can be 'left' or 'right'.\n- __target_pos__ (the position of the target) can be '-300' or '300'. These values reflect the X-coordinate of the target in pixels (0 = center). Using the coordinates directly, rather than 'left' and 'right', will be convenient when we create the target displays (see Step 5).\n- __target_letter__ (the target letter) can be 'F' or 'H'.\n\nTherefore, our experiment has 2 x 2 x 2 = 8 levels. Although 8 levels is not that many (most experiments will have more), we don't need to enter all possible combinations by hand. Click on *block_loop* in the overview to open its tab. Now click on the 'Full-factorial design' button. In the variable wizard, you simply define all variables by typing the name in the first row and the levels in the rows below the name (see %FigVariableWizard). If you select 'Ok', you will see that *block_loop* has been filled with all 8 possible combinations.\n\nIn the resulting loop table, each row corresponds to one run of *trial_sequence*. Because, in our case, one run of *trial_sequence* corresponds to one trial, each row in our loop table corresponds to one trial. Each column corresponds to one variable, which can have a different value on each trial.\n\n<notranslate>\nfigure:\n id: FigVariableWizard\n source: variable-wizard.png\n caption: |\n  The loop variable wizard in Step 2.\n</notranslate>\n\nBut we are not done yet. We need to add three more variables: the location of the distractor, the correct response, and the congruency.": {
    "fr": "Comme son nom l'indique, un `form_text_display` est un formulaire qui affiche du texte. Nous allons utiliser un `form_text_display` (à la place d'un élément `sketchpad`) pour donner les instructions au participant au début de l'expérience.\n\nFaites glisser un `form_text_display` de la barre d'outils des éléments (sous 'Form') sur la séquence *experiment* dans la zone d'aperçu. Lorsque vous relâchez, un nouvel élément `form_text_display` est inséré dans la séquence *experiment*. Renommez cet élément en *instructions*. Assurez-vous que l'élément apparaît au tout début de l'expérience.\n\n__Ajouter un nouvel élément `form_text_display`, pour le message d'au revoir__\n\nLorsque l'expérience est terminée, nous devons dire au revoir au participant. Pour cela, nous avons besoin d'un autre élément `form_text_display`. Faites glisser un `form_text_display` de la barre d'outils des éléments sur *experimental_loop*. Dans le menu contextuel qui apparaît, sélectionnez 'Insérer après experimental_loop'. Renommez cet élément en *goodbye*.\n\nLa zone d'aperçu de votre expérience ressemble maintenant à %FigStep1. C'est le moment de sauvegarder votre expérience (raccourci : `Ctrl+S`).\n\n<notranslate>\nfigure:\n id: FigStep1\n source: step1.png\n caption: |\n  La zone d'aperçu à la fin de l'étape 1.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n__Encadré__\n\n__Astuce__ — Si vous n'aimez pas avoir de nombreux onglets ouverts, vous pouvez fermer tous les onglets sauf celui en cours en cliquant sur le bouton 'Fermer les autres onglets' dans la barre d'outils principale (raccourci : `Ctrl+T`).\n\n</div>\n\n## Étape 2 : Remplissez la boucle de bloc avec des variables indépendantes\n\nComme son nom l'indique, *block_loop* correspond à un seul bloc d'essais. À l'étape précédente, nous avons créé la *block_loop*, mais nous devons encore définir les variables indépendantes qui seront modifiées au sein du bloc. Notre expérience a trois variables indépendantes :\n\n- __gaze_cue__ peut être 'left' ou 'right'.\n- __target_pos__ (la position de la cible) peut être '-300' ou '300'. Ces valeurs reflètent la coordonnée X de la cible en pixels (0 = centre). Utiliser les coordonnées directement, plutôt que 'left' et 'right', sera pratique lorsque nous créerons les affichages cibles (voir étape 5).\n- __target_letter__ (la lettre cible) peut être 'F' ou 'H'.\n\nNotre expérience a donc 2 x 2 x 2 = 8 niveaux. Bien que 8 niveaux ne soient pas si nombreux (la plupart des expériences en auront plus), il ne faut pas entrer toutes les combinaisons possibles à la main. Cliquez sur *block_loop* dans l'aperçu pour ouvrir son onglet. Cliquez maintenant sur le bouton 'Full-factorial design'. Dans l'assistant de variables, vous définissez simplement toutes les variables en tapant le nom dans la première ligne et les niveaux dans les lignes en-dessous du nom (voir %FigVariableWizard). Si vous sélectionnez 'Ok', vous verrez que *block_loop* a été rempli avec les 8 combinaisons possibles.\n\nDans le tableau de boucle résultant, chaque ligne correspond à une exécution de *trial_sequence*. Dans notre cas, une exécution de *trial_sequence* correspond à un essai, chaque ligne de notre tableau de boucle correspond à un essai. Chaque colonne correspond à une variable, qui peut avoir une valeur différente à chaque essai.\n\n<notranslate>\nfigure:\n id: FigVariableWizard\n source: variable-wizard.png\n caption: |\n  L'assistant de variables de boucle à l'étape 2.\n</notranslate>\n\nMais nous n'avons pas encore terminé. Nous devons ajouter trois autres variables : l'emplacement du distracteur, la réponse correcte et la congruence."
  },
  "- __dist_pos__ -- On the first row of the first empty column, enter 'dist_pos'. This automatically adds a new experimental variable named 'dist_pos'. In the rows below, enter '300' wherever 'target_pos' is -300, and '-300' wherever 'target_pos' is 300. In other words, the target and the distractor should be positioned opposite from each other.\n- __correct_response__ -- Create another variable, in another empty column, with the name 'correct_response'. Set 'correct_response' to 'z' where 'target_letter' is 'F', and to 'm' where 'target_letter' is 'H'. This means that:\n    - The participant should press the 'z' key if he/she sees an 'F'\n    - and the 'm' key if he/she sees an 'H'.\n- __congruency__ -- Create another variable with the name 'congruency'. Set 'congruency' to 'congruent' where 'target_pos' is '-300' and 'gaze_cue' is 'left', and where 'target_pos' is '300' and 'gaze_cue' is 'right'. In other words, a trial is congruent if the face looks at the target. Set 'congruency' to 'incronguent' for the trials on which the face looks at the distractor. The 'congruency' variable is not necessary to run the experiment; however, it is useful for analyzing the data later on.\n\nWe need to do one last thing. 'Repeat' is currently set to '1,00'. This means that each cycle will be executed once. So the block now consists of 8 trials, which is a bit short. A reasonable length for a block of trials is 24, so set 'Repeat' to 3,00 (3 repeats x 8 cycles = 24 trials). You don't need to change 'Order', because 'random' is exactly what we want.\n\nThe *block_loop* now looks like %FigStep3. Remember to save your experiment regularly.\n\n<notranslate>\nfigure:\n id: FigStep3\n source: step3.png\n caption: \"The *block_loop* at the end of Step 3.\"\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n__Tip__ -- You can set 'Repeat' to a non-integer number. For example, by setting 'Repeat' to '0,5', only half the trials (randomly selected) are executed.\n\n</div>\n\n## Step 3: Add images and sound files to the file pool\n\nFor our stimuli, we will use images from file. In addition, we will play a sound if the participant makes an error. For this we need a sound file.\n\nYou can download the required files here (in most webbrowsers you can right-click the links and choose 'Save Link As' or a similar option):\n\n- [gaze_neutral.png](/img/beginner-tutorial/gaze_neutral.png)\n- [gaze_left.png](/img/beginner-tutorial/gaze_left.png)\n- [gaze_right.png](/img/beginner-tutorial/gaze_right.png)\n- [incorrect.ogg](/img/beginner-tutorial/incorrect.ogg)\n\nAfter you have downloaded these files (to your desktop, for example), you can add them to the file pool. If the file pool is not already visible (by default on the right side of the window), click on the 'Show file pool' button in the main toolbar (shortcut: `Ctrl+P`). The easiest way to add the four files to the file pool is to drag them from the desktop (or wherever you have downloaded the files to) into the file pool. Alternatively, you can click on the '+' button in the file pool and add files using the file select dialog that appears. The file pool will be automatically saved with your experiment.\n\nYour file pool now looks like %FigStep4. Remember to save your experiment regularly.\n\n<notranslate>\nfigure:\n id: FigStep4\n source: step4.png\n caption: \"The file pool at the end of Step 4.\"\n</notranslate>\n\n## Step 4: Fill the trial sequence with items\n\nA trial in our experiment looks as follows (see %TrialSeq):\n\n1. __Fixation dot__ -- 750 ms, `sketchpad` item\n2. __Neutral gaze__ -- 750 ms, `sketchpad` item\n3. __Gaze cue__ -- 500 ms, `sketchpad` item\n4. __Target__  -- 0 ms, `sketchpad` item\n5. __Response collection__ \t-- `keyboard_response` item\n6. __Play a sound if response was incorrect__ --  `sampler` item\n7. __Log response to file__ -- `logger` item\n\n\n<notranslate>\nfigure:\n id: TrialSeq\n source: trial_sequence_gaze_cuing.png\n caption: \"A typical trial sequence in the gaze-cuing experiment.\"\n</notranslate>": {
    "fr": "- __dist_pos__ -- Sur la première ligne de la première colonne vide, entrez 'dist_pos'. Cela ajoute automatiquement une nouvelle variable expérimentale nommée 'dist_pos'. Dans les lignes ci-dessous, saisissez '300' partout où 'target_pos' est -300, et '-300' partout où 'target_pos' est 300. En d'autres termes, la cible et le distractor doivent être positionnés à l'opposé l'un de l'autre.\n- __correct_response__ -- Créez une autre variable, dans une autre colonne vide, avec le nom 'correct_response'. Définissez 'correct_response' sur 'z' lorsque 'target_letter' est 'F', et sur 'm' lorsque 'target_letter' est 'H'. Cela signifie que :\n    - Le participant doit appuyer sur la touche 'z' s'il/elle voit un 'F'\n    - et la touche 'm' s'il/elle voit un 'H'.\n- __congruency__ -- Créez une autre variable avec le nom 'congruency'. Définissez 'congruency' sur 'congruent' lorsque 'target_pos' est '-300' et que 'gaze_cue' est 'left', et lorsque 'target_pos' est '300' et que 'gaze_cue' est 'right'. En d'autres termes, un essai est congruent si le visage regarde la cible. Définissez 'congruency' sur 'incronguent' pour les essais où le visage regarde le distractor. La variable 'congruency' n'est pas nécessaire pour exécuter l'expérience ; cependant, elle est utile pour analyser les données ultérieurement.\n\nNous devons faire une dernière chose. 'Répéter' est actuellement réglé sur '1,00'. Cela signifie que chaque cycle sera exécuté une fois. Le bloc se compose donc de 8 essais, ce qui est un peu court. Une longueur raisonnable pour un bloc d'essais est de 24, alors réglez 'Répéter' sur 3,00 (3 répétitions x 8 cycles = 24 essais). Vous n'avez pas besoin de changer 'Ordre', car 'random' est exactement ce que nous voulons.\n\nLe *block_loop* ressemble maintenant à %FigStep3. N'oubliez pas de sauvegarder régulièrement votre expérience.\n\n<notranslate>\nfigure:\n id: FigStep3\n source: step3.png\n caption: \"Le *block_loop* à la fin de l'étape 3.\"\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n__Astuce__ -- Vous pouvez définir 'Répéter' sur un nombre non entier. Par exemple, en définissant 'Repeat' sur '0,5', seuls la moitié des essais (sélectionnés au hasard) sont exécutés.\n\n</div>\n\n## Étape 3 : Ajouter des images et des fichiers sonores au pool de fichiers\n\nPour nos stimuli, nous utiliserons des images provenant de fichiers. De plus, nous jouerons un son si le participant commet une erreur. Pour cela, nous avons besoin d'un fichier sonore.\n\nVous pouvez télécharger les fichiers requis ici (dans la plupart des navigateurs web, vous pouvez faire un clic droit sur les liens et choisir 'Enregistrer le lien sous' ou une option similaire) :\n\n- [gaze_neutral.png](/img/beginner-tutorial/gaze_neutral.png)\n- [gaze_left.png](/img/beginner-tutorial/gaze_left.png)\n- [gaze_right.png](/img/beginner-tutorial/gaze_right.png)\n- [incorrect.ogg](/img/beginner-tutorial/incorrect.ogg)\n\nUne fois que vous avez téléchargé ces fichiers (sur votre bureau, par exemple), vous pouvez les ajouter au pool de fichiers. Si le pool de fichiers n'est pas déjà visible (par défaut à droite de la fenêtre), cliquez sur le bouton 'Afficher le pool de fichiers' dans la barre d'outils principale (raccourci : `Ctrl+P`). La façon la plus simple d'ajouter les quatre fichiers au pool de fichiers est de les faire glisser du bureau (ou d'où vous les avez téléchargés) dans le pool de fichiers. Vous pouvez également cliquer sur le bouton '+' dans le pool de fichiers et ajouter des fichiers à l'aide de la boîte de dialogue de sélection de fichiers qui apparaît. Le pool de fichiers sera automatiquement enregistré avec votre expérience.\n\nVotre pool de fichiers ressemble maintenant à %FigStep4. N'oubliez pas de sauvegarder régulièrement votre expérience.\n\n<notranslate>\nfigure:\n id: FigStep4\n source: step4.png\n caption: \"Le pool de fichiers à la fin de l'étape 4.\"\n</notranslate>\n\n## Étape 4 : Remplir la séquence d'essais avec des éléments\n\nUn essai dans notre expérience se déroule comme suit (voir %TrialSeq) :\n\n1. __Point de fixation__ -- 750 ms, élément `sketchpad`\n2. __Regard neutre__ -- 750 ms, élément `sketchpad`\n3. __Repère de regard__ -- 500 ms, élément `sketchpad`\n4. __Cible__  -- 0 ms, élément `sketchpad`\n5. __Collecte des réponses__ -- élément `keyboard_response`\n6. __Jouer un son si la réponse est incorrecte__ -- élément `sampler`\n7. __Enregistrer la réponse dans un fichier__ -- élément `logger`\n\n<notranslate>\nfigure:\n id: TrialSeq\n source: trial_sequence_gaze_cuing.png\n caption: \"Une séquence d'essais typique dans l'expérience de l'orientation du regard.\"\n</notranslate>"
  },
  "As you can see in the overview area, our *trial_sequence* already contains one `sketchpad`, as well as a `keyboard_response` item and and a `logger`.\n\n\nTo add the remaining items:\n\nPick up a `sketchpad` from the item toolbar and drag it into the *trial_sequence*. Repeat this two more times, so that *trial_sequence* contains four `sketchpad`s. Next, select and append a `sampler` item. Make sure the `sampler` item appears right after the *keyboard_response*, but before the *logger*.\n\nAgain, we will rename the new items, to make sure that the *trial_sequence* is easy to understand. Rename:\n\n- the first sketchpad to *fixation_dot*\n- the second sketchpad to *neutral_gaze*\n- the third sketchpad to *gaze_cue*\n- the fourth sketchpad to *target*\n- the `sampler` item to *incorrect_sound*\n\nThe *incorrect_sound* item should only be executed if an error was made. To do this, we need to change the 'Run if …' statement to `[correct] = 0` in the *trial_sequence* tab. This works, because the *keyboard_response* item automatically creates a `correct` variable, which is set to `1` (correct), `0` (incorrect), or `undefined` (this relies on the `correct_response` variable that was defined in Step 3). The square brackets indicate that `correct` should be interpreted as the name of a variable and not as text. To change a run-if statement, double click on it (shortcut: `F3`).\n\nThe *trial_sequence* now looks like %FigStep5.\n\n<notranslate>\nfigure:\n id: FigStep5\n source: step5.png\n caption: \"The *trial_sequence* at the end of Step 5.\"\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n__Tip__ -- Variables and conditional \"if\" statements are very powerful! To learn more about them, see:\n\n- %link:manual/variables%\n\n</div>\n\n## Step 5: Draw the sketchpad items\n\nThe `sketchpad` items that we have created in Step 5 are still blank. It's time to do some drawing!\n\n__Draw the fixation dot__\n\n- Open the *fixation_dot* tab by clicking on this item in the overview area. Because we chose the 'Extended template', OpenSesame already created a fixation point for us. The only thing we need to change is how long the fixation dot will remain on screen\n- Click on the 'Duration' box and change its value to 750 (see %TrialSeq).\n\n__Draw the neutral gaze__\n\nOpen the *neutral_gaze* item. Now select the `image tool` by clicking on the button with the moon-mountain-landscape-like icon. Click on the center of the screen (0, 0). The 'Select file from pool' dialog will appear. Select the file `gaze_neutral.png` and click on the 'Select' button. The neutral gaze image will now stare at you from the center of the screen! Finally, like before, change the 'Duration' field from 'keypress' to '750'.\n\n__Draw the gaze cue__\n\nOpen the *gaze_cue* item, and again select the `image` tool. Click on the center of the screen (0, 0) and select the file `gaze_left.png`.\n\nObviously, we are not done yet, because the gaze cue should not always be 'left', but should depend on the variable `gaze_cue`, which we have defined in Step 3. However, by drawing the `gaze_left.png` image to the sketchpad, we have generated a script that needs only a tiny modification to make sure that the proper image is shown. Click on the 'Select view' button at the top-right of the *gaze_cue* tab and select 'View script'. You will now see the script that corresponds to the sketchpad that we have just created:\n\n~~~ .python\nset duration keypress\nset description \"Displays stimuli\"\ndraw image center=1 file=\"gaze_left.png\" scale=1 show_if=always x=0 y=0 z_index=0\n~~~\n\nThe only thing that we need to do is replace `gaze_left.png` with `gaze_[gaze_cue].png`. This means that OpenSesame uses the variable `gaze_cue` (which has the values `left` and `right`) to determine which image should be shown.\n\nWhile we are at it, we might as well change the duration to '500'. The script now looks like this:": {
    "fr": "Comme vous pouvez le voir dans la zone d'aperçu, notre *trial_sequence* contient déjà un `sketchpad`, ainsi qu'un élément `keyboard_response` et un `logger`.\n\n\nPour ajouter les éléments restants :\n\nPrenez un `sketchpad` dans la barre d'outils d'éléments et faites-le glisser sur le *trial_sequence*. Répétez cette opération deux fois de plus, de sorte que *trial_sequence* contient quatre `sketchpad`s. Ensuite, sélectionnez et ajoutez un élément `sampler`. Assurez-vous que l'élément `sampler` apparaît juste après le *keyboard_response*, mais avant le *logger*.\n\nDe nouveau, nous renommerons les nouveaux éléments, pour nous assurer que le *trial_sequence* est facile à comprendre. Renommez :\n\n- le premier sketchpad en *fixation_dot*\n- le deuxième sketchpad en *neutral_gaze*\n- le troisième sketchpad en *gaze_cue*\n- le quatrième sketchpad en *target*\n- l'élément `sampler` en *incorrect_sound*\n\nL'élément *incorrect_sound* ne doit s'exécuter que si une erreur a été commise. Pour ce faire, nous devons modifier l'instruction \"Run if …\" en `[correct] = 0` dans l'onglet *trial_sequence*. Cela fonctionne, car l'élément *keyboard_response* crée automatiquement une variable `correct`, qui est définie sur `1` (correct), `0` (incorrect) ou `undefined` (cela repose sur la variable `correct_response` qui a été définie à l'étape 3). Les crochets indiquent que `correct` doit être interprété comme le nom d'une variable et non comme du texte. Pour modifier une instruction run-if, double-cliquez dessus (raccourci : `F3`).\n\nLe *trial_sequence* ressemble maintenant à %FigStep5.\n\n<notranslate>\nfigure:\n id: FigStep5\n source: step5.png\n caption: \"Le *trial_sequence* à la fin de l'étape 5.\"\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n__Astuce__ -- Les variables et les instructions conditionnelles \"si\" sont très puissantes ! Pour en savoir plus à leur sujet, consultez :\n\n- %link:manual/variables%\n\n</div>\n\n## Étape 5 : Dessinez les éléments sketchpad\n\nLes éléments `sketchpad` que nous avons créés à l'étape 5 sont encore vierges. Il est temps de dessiner !\n\n__Dessinez le point de fixation__\n\n- Ouvrez l'onglet *fixation_dot* en cliquant sur cet élément dans la zone d'aperçu. Parce que nous avons choisi le « modèle étendu », OpenSesame a déjà créé un point de fixation pour nous. La seule chose que nous devons changer est la durée pendant laquelle le point de fixation restera à l'écran\n- Cliquez sur la case « Duration » et changez sa valeur à 750 (voir %TrialSeq).\n\n__Dessinez le regard neutre__\n\nOuvrez l'élément *neutral_gaze*. Sélectionnez maintenant l'`image tool` en cliquant sur le bouton avec l'icône en forme de paysage lunaire et montagneux. Cliquez sur le centre de l'écran (0, 0). La boîte de dialogue \"Select file from pool\" apparaîtra. Sélectionnez le fichier `gaze_neutral.png` et cliquez sur le bouton \"Sélectionner\". L'image du regard neutre vous regardera maintenant depuis le centre de l'écran ! Enfin, comme précédemment, changez le champ \"Duration\" de \"keypress\" à \"750\".\n\n__Dessinez le regard directionnel__\n\nOuvrez l'élément *gaze_cue* et sélectionnez à nouveau l'`image tool`. Cliquez sur le centre de l'écran (0, 0) et sélectionnez le fichier `gaze_left.png`.\n\nÉvidemment, nous n'avons pas encore terminé, car le regard directionnel ne doit pas toujours être « à gauche », mais doit dépendre de la variable `gaze_cue`, que nous avons définie à l'étape 3. Cependant, en dessinant l'image `gaze_left.png` sur le sketchpad, nous avons généré un script qui nécessite juste une petite modification pour s'assurer que l'image appropriée est affichée. Cliquez sur le bouton « Select view » en haut à droite de l'onglet *gaze_cue* et sélectionnez « View script ». Vous verrez maintenant le script qui correspond au sketchpad que nous venons de créer :\n\n~~~ .python\nset duration keypress\nset description \"Displays stimuli\"\ndraw image center=1 file=\"gaze_left.png\" scale=1 show_if=always x=0 y=0 z_index=0\n~~~\n\nLa seule chose que nous devons faire est de remplacer `gaze_left.png` par `gaze_[gaze_cue].png`. Cela signifie qu'OpenSesame utilise la variable `gaze_cue` (qui a les valeurs `left` et `right`) pour déterminer quelle image doit être affichée.\n\nPendant que nous y sommes, nous pouvons également changer la durée à « 500 ». Le script ressemble maintenant à ceci :"
  },
  "~~~ .python\nset duration 500\nset description \"Displays stimuli\"\ndraw image center=1 file=\"gaze_[gaze_cue].png\" scale=1 show_if=always x=0 y=0 z_index=0\n~~~\n\nClick the 'Apply and close' button at the top right to apply your changes to the script and return to the regular item controls. OpenSesame will warn you that the image cannot be shown, because it is defined using variables, and a placeholder image will be shown instead. Don't worry, the correct image will be shown during the experiment!\n\n__Draw the target__\n\nWe want three objects to be part of the target display: the target letter, the distractor letter, and the gaze cue (see %FigGazeCuing). As before, we will start by creating a static display using the `sketchpad` editor. After this, we will only need to make minor changes to the script so that the exact display depends on the variables.\n\nClick on *target* in the overview to open the target tab and like before, draw the `gaze_left.png` image at the center of the screen. Now select the `draw text tool` by clicking on the button with the 'A' icon. The default font size is 18 px, which is a bit small for our purpose, so change the font size to 32 px. Now click on (-320, 0) in the sketchpad (the X-coordinate does not need to be exactly 320, since we will change this to a variable anyway). Enter \"[target_letter]\" in the dialog that appears, to draw the target letter (when drawing text, you can use variables directly). Similarly, click on (320, 0) and draw an 'X' (the distractor is always an 'X').\n\nNow open the script editor by clicking on the 'Select view' button at the top-right of the tab and selecting 'View script'. The script looks like this:\n\n~~~ .python\nset duration keypress\nset description \"Displays stimuli\"\ndraw image center=1 file=\"gaze_left.png\" scale=1 show_if=always x=0 y=0 z_index=0\ndraw textline center=1 color=black font_bold=no font_family=mono font_italic=no font_size=32 html=yes show_if=always text=\"[target_letter]\" x=-320 y=0 z_index=0\ndraw textline center=1 color=black font_bold=no font_family=mono font_italic=no font_size=32 html=yes show_if=always text=X x=320 y=0 z_index=0\n~~~\n\nLike before, change `gaze_left.png` to `gaze_[gaze_cue].png`. We also need to make the position of the target and the distractor depend on the variables `target_pos` and `dist_pos` respectively. To do this, simply change `-320` to `[target_pos]` and `320` to `[dist_pos]`. Make sure that you leave the `0`, which is the Y-coordinate. The script now looks like this:\n\n~~~ .python\nset duration \"keypress\"\nset description \"Displays stimuli\"\ndraw image center=1 file=\"gaze_[gaze_cue].png\" scale=1 show_if=always x=0 y=0 z_index=0\ndraw textline center=1 color=black font_bold=no font_family=mono font_italic=no font_size=32 html=yes show_if=always text=\"[target_letter]\" x=[target_pos] y=0 z_index=0\ndraw textline center=1 color=black font_bold=no font_family=mono font_italic=no font_size=32 html=yes show_if=always text=X x=[dist_pos] y=0 z_index=0\n~~~\n\nClick on the 'Apply and close' button to apply the script and go back to the regular item controls.\n\nFinally, set the 'Duration' field to '0'. This does not mean that the target is presented for only 0 ms, but that the experiment will advance to the next item (the *keyboard_response*) right away. Since the *keyboard_response* waits for a response, but doesn't change what's on the screen, the target will remain visible until a response has been given.\n\nRemember to save your experiment regularly.\n\n<div class='info-box' markdown='1'>\n\n__Tip__ -- Again, make sure that the (foreground) color is set to black. Otherwise you will draw white on white and won't see anything!\n\n</div>\n\n## Step 7: Configure the keyboard response item\n\nClick on *keyboard_response* in the overview to open its tab. You see three options: Correct response, Allowed responses, and Timeout.": {
    "fr": "~~~ .python\nset duration 500\nset description \"Affiche les stimuli\"\ndraw image center=1 file=\"gaze_[gaze_cue].png\" scale=1 show_if=toujours x=0 y=0 z_index=0\n~~~\n\nCliquez sur le bouton 'Appliquer et fermer' en haut à droite pour appliquer vos modifications au script et revenir aux contrôles d'élément habituels. OpenSesame vous avertira que l'image ne peut pas être affichée, car elle est définie à l'aide de variables, et une image fictive sera affichée à la place. Ne vous inquiétez pas, la bonne image sera affichée pendant l'expérience!\n\n__Dessiner la cible__\n\nNous voulons que trois objets fassent partie de l'affichage de la cible: la lettre cible, la lettre distractrice et l'indice du regard (voir %FigGazeCuing). Comme précédemment, nous commencerons par créer un affichage statique avec l'éditeur `sketchpad`. Ensuite, nous devrons seulement apporter de légères modifications au script pour que l'affichage exact dépende des variables.\n\nCliquez sur *cible* dans l'aperçu pour ouvrir l'onglet cible et comme auparavant, dessinez l'image `gaze_left.png` au centre de l'écran. Sélectionnez ensuite l'`outil de dessin de texte` en cliquant sur le bouton avec l'icône \"A\". La taille de police par défaut est de 18 px, ce qui est un peu petit pour notre objectif, changez donc la taille de la police à 32 px. Cliquez maintenant sur (-320, 0) dans le sketchpad (la coordonnée X n'a pas besoin d'être exactement 320, car nous la changerons de toute façon pour une variable). Entrez \"[target_letter]\" dans la boîte de dialogue qui apparaît, pour dessiner la lettre cible (lors du dessin du texte, vous pouvez utiliser directement des variables). De même, cliquez sur (320, 0) et dessinez un 'X' (la distractrice est toujours un 'X').\n\nOuvrez maintenant l'éditeur de script en cliquant sur le bouton 'Sélectionner la vue' en haut à droite de l'onglet et en sélectionnant 'Afficher le script'. Le script ressemble à cela :\n\n~~~ .python\nset duration keypress\nset description \"Affiche les stimuli\"\ndraw image center=1 file=\"gaze_left.png\" scale=1 show_if=toujours x=0 y=0 z_index=0\ndraw textline center=1 color=black font_bold=non font_family=mono font_italic=non font_size=32 html=yes show_if=toujours text=\"[target_letter]\" x=-320 y=0 z_index=0\ndraw textline center=1 color=black font_bold=non font_family=mono font_italic=non font_size=32 html=yes show_if=toujours text=X x=320 y=0 z_index=0\n~~~\n\nComme auparavant, changez `gaze_left.png` en `gaze_[gaze_cue].png`. Nous devons également rendre la position de la cible et du distracteur dépendante des variables `target_pos` et `dist_pos` respectivement. Pour ce faire, changez simplement `-320` en `[target_pos]` et `320` en `[dist_pos]`. Assurez-vous de laisser le `0`, qui est la coordonnée Y. Le script ressemble maintenant à cela :\n\n~~~ .python\nset duration \"keypress\"\nset description \"Affiche les stimuli\"\ndraw image center=1 file=\"gaze_[gaze_cue].png\" scale=1 show_if=toujours x=0 y=0 z_index=0\ndraw textline center=1 color=black font_bold=non font_family=mono font_italic=non font_size=32 html=yes show_if=toujours text=\"[target_letter]\" x=[target_pos] y=0 z_index=0\ndraw textline center=1 color=black font_bold=non font_family=mono font_italic=non font_size=32 html=yes show_if=toujours text=X x=[dist_pos] y=0 z_index=0\n~~~\n\nCliquez sur le bouton 'Appliquer et fermer' pour appliquer le script et revenir aux contrôles d'élément habituels.\n\nEnfin, définissez le champ 'Durée' sur '0'. Cela ne signifie pas que la cible est présentée pendant seulement 0 ms, mais que l'expérience passe à l'élément suivant (la *réponse au clavier*) immédiatement. Étant donné que la *réponse au clavier* attend une réponse, mais ne modifie pas ce qui est à l'écran, la cible restera visible jusqu'à ce qu'une réponse ait été donnée.\n\nN'oubliez pas d'enregistrer régulièrement votre expérience.\n\n<div class='info-box' markdown='1'>\n\n__Astuce__ -- Encore une fois, assurez-vous que la couleur (de premier plan) est réglée sur noir. Sinon, vous dessinerez du blanc sur du blanc et vous ne verrez rien!\n\n</div>\n\n## Étape 7: Configurer l'élément de réponse au clavier\n\nCliquez sur *keyboard_response* dans l'aperçu pour ouvrir son onglet. Vous voyez trois options: Réponse correcte, Réponses autorisées et Délai."
  },
  "We have already set the `correct_response` variable in Step 3. Unless we explicitly specify a correct response, OpenSesame automatically uses the `correct_response` variable if it is available. Therefore, we don't need to change the 'Correct response' field here.\n\nWe do need to set the allowed responses. Enter 'z;m' in the allowed-responses field (or other keys if you have chosen different response keys). The semicolon is used to separate responses. The `keyboard_response` item now only accepts 'z' and 'm' keys. All other key presses are ignored, with the exception of 'escape', which pauses the experiment.\n\nWe also want to set a timeout, which is the maximum interval that the KEYBOARD_RESPONSE waits before deciding that the response is incorrect and setting the 'response' variable to 'None'. '2000' (ms) is a good value.\n\nThe content of the *keyboard_response* item now looks like %FigStep7.\n\n<notranslate>\nfigure:\n id: FigStep7\n source: step7.png\n caption: \"The *keyboard_response* at the end of Step 7.\"\n</notranslate>\n\n## Step 8: Configure the incorrect (sampler) item\n\nThe *incorrect_sound* item doesn't need much work: We only need to select the sound that should be played. Click on *incorrect_sound* in the overview to open its tab. Click on the 'Browse' button and select `incorrect.ogg` from the file pool.\n\nThe sampler now looks like %FigStep8.\n\n<notranslate>\nfigure:\n id: FigStep8\n source: step8.png\n caption: \"The *incorrect_sound* item at the end of Step 8.\"\n</notranslate>\n\n## Step 9: Configure the variable logger\n\nActually, we don't need to configure the `logger` item, but let's take a look at it anyway. Click on *logger* in the overview to open its tab. You see that the option 'Log all variables (recommended)' is selected. This means that OpenSesame logs everything, which is fine.\n\n<div class='info-box' markdown='1'>\n\n__The one tip to rule them all__ -- Always triple-check whether all the necessary variables are logged in your experiment! The best way to check this is to run the experiment and investigate the resulting log files.\n\n</div>\n\n## Step 10: Draw the feedback item\n\nAfter every block of trials, we want to present feedback to the participant to let him/ her know how well he/ she is doing. Therefore, in Step 2, we added a `feedback` item, simply named *feedback* to the end of *block_sequence*.\n\nClick on *feedback* in the overview to open its tab, select the draw text tool, change the foreground color to 'black' (if it isn't already), and click at (0, 0). Now enter the following text:\n\n    End of block\n\n    Your average response time was [avg_rt] ms\n    Your accuracy was [acc] %\n\n    Press any key to continue\n\nBecause we want the feedback item to remain visible as long as the participant wants (i.e. until he/ she presses a key), we leave 'Duration' field set to 'keypress'.\n\nThe feedback item now looks like %FigStep_10.\n\n<notranslate>\nfigure:\n id: FigStep_10\n source: step10.png\n caption: \"The feedback item at the end of Step 10.\"\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n__Background box__\n\n__Feedback and variables__ -- Response items automatically keep track of the accuracy and average response time of the participant in the variables 'acc' (synonym: 'accuracy') and 'avg_rt' (synonym: 'average_response_time') respectively. See also:\n\n- %link:manual/variables%\n\n__Tip__ -- Make sure that the (foreground) color is set to black. Otherwise you will draw white on white and won't see anything!\n\n</div>\n\n## Step 11: Set the length of the practice phase and experimental phase\n\nWe have previously created the *practice_loop* and *experiment_loop* items, which both call *block_sequence* (i.e., a block of trials). However, right now they call *block_sequence* only once, which means that both the practice and the experimental phase consist of only a single block of trials.\n\nClick on *practice_loop* to open its tab and set 'Repeat' to '2,00'. This means that the practice phase consists of two blocks.": {
    "fr": "Nous avons déjà défini la variable `correct_response` à l'étape 3. À moins de spécifier explicitement une réponse correcte, OpenSesame utilise automatiquement la variable `correct_response` si elle est disponible. Par conséquent, nous n'avons pas besoin de modifier le champ 'Réponse correcte' ici.\n\nNous devons définir les réponses autorisées. Entrez 'z;m' dans le champ des réponses autorisées (ou d'autres touches si vous avez choisi des touches de réponse différentes). Le point-virgule est utilisé pour séparer les réponses. L'élément `keyboard_response` accepte maintenant uniquement les touches 'z' et 'm'. Toutes les autres pressions de touches sont ignorées, à l'exception de 'échap', qui met en pause l'expérience.\n\nNous souhaitons également définir un délai d'attente, qui est l'intervalle maximum que KEYBOARD_RESPONSE attend avant de décider que la réponse est incorrecte et de définir la variable 'response' sur 'None'. '2000' (ms) est une bonne valeur.\n\nLe contenu de l'élément *keyboard_response* ressemble maintenant à %FigStep7.\n\n<notranslate>\nfigure:\n id: FigStep7\n source: step7.png\n caption: \"Le *keyboard_response* à la fin de l'étape 7.\"\n</notranslate>\n\n## Étape 8 : Configurez l'élément incorrect (sampler)\n\nL'élément *incorrect_sound* ne nécessite pas beaucoup de travail : il suffit de sélectionner le son à jouer. Cliquez sur *incorrect_sound* dans l'aperçu pour ouvrir son onglet. Cliquez sur le bouton 'Parcourir' et sélectionnez `incorrect.ogg` dans la banque de fichiers.\n\nL'échantillonneur ressemble maintenant à %FigStep8.\n\n<notranslate>\nfigure:\n id: FigStep8\n source: step8.png\n caption: \"L'élément *incorrect_sound* à la fin de l'étape 8.\"\n</notranslate>\n\n## Étape 9 : Configurez l'enregistreur de variables\n\nEn réalité, nous n'avons pas besoin de configurer l'élément `logger`, mais jetons-y un coup d'œil. Cliquez sur *logger* dans l'aperçu pour ouvrir son onglet. Vous voyez que l'option 'Log all variables (recommended)' est sélectionnée. Cela signifie qu'OpenSesame enregistre tout, ce qui est très bien.\n\n<div class='info-box' markdown='1'>\n\n__Le conseil ultime__ -- Vérifiez toujours trois fois si toutes les variables nécessaires sont enregistrées dans votre expérience ! La meilleure façon de vérifier cela est de lancer l'expérience et d'examiner les fichiers journaux résultants.\n\n</div>\n\n## Étape 10 : Dessiner l'élément de feedback\n\nAprès chaque bloc d'essais, nous souhaitons présenter un feedback au participant pour lui faire savoir comment il se débrouille. C'est pourquoi, à l'étape 2, nous avons ajouté un élément `feedback`, simplement nommé *feedback* à la fin  de *block_sequence*.\n\nCliquez sur *feedback* dans l'aperçu pour ouvrir son onglet, sélectionnez l'outil de dessin de texte, changez la couleur de premier plan en 'noir' (si ce n'est pas déjà le cas) et cliquez sur (0, 0). Entrez maintenant le texte suivant :\n\n    Fin du bloc\n\n    Votre temps de réponse moyen était de [avg_rt] ms\n    Votre précision était de [acc] %\n\n    Appuyez sur n'importe quelle touche pour continuer\n\nComme nous voulons que l'élément de feedback reste visible aussi longtemps que le participant le souhaite (c'est-à-dire jusqu'à ce qu'il appuie sur une touche), nous laissons le champ 'Durée' défini sur 'keypress'.\n\nL'élément de feedback ressemble maintenant à %FigStep_10.\n\n<notranslate>\nfigure:\n id: FigStep_10\n source: step10.png\n caption: \"L'élément de feedback à la fin de l'étape 10.\"\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n__Boîte d'arrière-plan__\n\n__Rétroaction et variables__ -- Les éléments de réponse suivent automatiquement la précision et le temps de réponse moyen du participant dans les variables 'acc' (synonyme : 'précision') et 'avg_rt' (synonyme : 'average_response_time') respectivement. Voir aussi :\n\n- %link:manual/variables%\n\n__Conseil__ -- Assurez-vous que la couleur (de premier plan) est réglée sur noir. Sinon, vous dessinerez du blanc sur blanc et vous ne verrez rien !\n\n</div>\n\n## Étape 11 : Définir la durée de la phase de pratique et de la phase expérimentale\n\nNous avons précédemment créé les éléments *practice_loop* et *experiment_loop*, qui appellent tous deux *block_sequence* (c'est-à-dire un bloc d'essais). Cependant, pour le moment, ils appellent *block_sequence* seulement une fois, ce qui signifie que les phases de pratique et expérimentale se composent d'un seul bloc d'essais.\n\nCliquez sur *practice_loop* pour ouvrir son onglet et définissez 'Répéter' sur '2,00'. Cela signifie que la phase de pratique se compose de deux blocs."
  },
  "Click on *experimental_loop* to open its tab and set 'Repeat' to '8,00'. This means that the experimental phase consists of eight blocks.\n\n<div class='info-box' markdown='1'>\n\n__Tip__ -- You can create a variable `practice` in both *practice_loop* and *experimental_loop* and set it to 'yes' and 'no' respectively. This is an easy way of keeping track of which trials were part of the practice phase.\n\n</div>\n\n## Step 12: Write the instruction, end_of_practice and end_of_experiment forms\n\nI think you can handle this step your own! Simply open the appropriate items and add some text to present instructions, an end-of-practice message, and an end-of-experiment message.\n\n## Step 13: Run the experiment!\n\nYou're done! Click on the 'Run in window' (shortcut: `Ctrl+W`) or 'Run fullscreen' (shortcut: `Ctrl+R`) buttons in the toolbar to run your experiment.\n\n<div class='info-box' markdown='1'>\n\n__Background box__\n\n__Tip__ -- A test run is executed even faster by clicking the orange 'Run in window' button (shortcut: `Ctrl+Shift+W`), which doesn't ask you how to save the logfile (and should therefore only be used for testing purposes).\n\n</div>\n\n## References\n\n<div class='reference' markdown='1'>\n\nFriesen, C. K., & Kingstone, A. (1998). The eyes have it! Reflexive orienting is triggered by nonpredictive gaze. *Psychonomic Bulletin & Review*, *5*, 490–495. doi:10.3758/BF03208827\n\nMathôt, S., Schreij, D., & Theeuwes, J. (2012). OpenSesame: An open-source, graphical experiment builder for the social sciences. *Behavior Research Methods*, *44*(2), 314-324. doi:10.3758/s13428-011-0168-7\n\n\n</div>\n\n[references]: #references\n[gpl]: http://www.gnu.org/licenses/gpl-3.0.en.html\n[gimp]: http://www.gimp.org/\n[audacity]: http://audacity.sourceforge.net/\n[python inline scripting]: /python/about\n": {
    "fr": "Cliquez sur *experimental_loop* pour ouvrir son onglet et définir 'Répéter' sur '8,00'. Cela signifie que la phase expérimentale comprend huit blocs.\n\n<div class='info-box' markdown='1'>\n\n__Astuce__ -- Vous pouvez créer une variable `practice` dans *practice_loop* et *experimental_loop* et la définir respectivement sur 'oui' et 'non'. C'est un moyen simple de suivre les essais faisant partie de la phase de pratique.\n\n</div>\n\n## Étape 12: Rédigez les formulaires d'instruction, de fin_de_pratique et de fin_d'expérience\n\nJe pense que vous pouvez gérer cette étape tout seul ! Ouvrez simplement les éléments appropriés et ajoutez du texte pour présenter des instructions, un message de fin de pratique et un message de fin d'expérience.\n\n## Étape 13: Lancez l'expérience !\n\nC'est terminé ! Cliquez sur les boutons 'Run in window' (raccourci : `Ctrl+W`) ou 'Run fullscreen' (raccourci : `Ctrl+R`) dans la barre d'outils pour lancer votre expérience.\n\n<div class='info-box' markdown='1'>\n\n__Boîte d'arrière-plan__\n\n__Astuce__ -- Un test est exécuté encore plus rapidement en cliquant sur le bouton orange 'Run in window' (raccourci : `Ctrl+Shift+W`), qui ne vous demandera pas comment enregistrer le fichier journal (et ne doit donc être utilisé qu'à des fins de test).\n\n</div>\n\n## Références\n\n<div class='reference' markdown='1'>\n\nFriesen, C. K., & Kingstone, A. (1998). The eyes have it! Reflexive orienting is triggered by nonpredictive gaze. *Psychonomic Bulletin & Review*, *5*, 490–495. doi:10.3758/BF03208827\n\nMathôt, S., Schreij, D., & Theeuwes, J. (2012). OpenSesame: An open-source, graphical experiment builder for the social sciences. *Behavior Research Methods*, *44*(2), 314-324. doi:10.3758/s13428-011-0168-7\n\n</div>\n\n[références]: #références\n[gpl]: http://www.gnu.org/licenses/gpl-3.0.en.html\n[gimp]: http://www.gimp.org/\n[audacity]: http://audacity.sourceforge.net/\n[python inline scripting]: /python/about"
  },
  "OpenSesame": {
    "fr": "OpenSesame"
  },
  "\nOpenSesame is a program to create experiments for psychology, neuroscience, and experimental economics. The latest $status$ version is $version$ *$codename$*, released on $release-date$ ([release notes](http://osdoc.cogsci.nl/$branch$/notes/$notes$)).\n\n<div class=\"btn-group\" role=\"group\" aria-label=\"...\">\n  <a role=\"button\" class=\"btn btn-success\" href=\"%url:download%\">\n\t\t<span class=\"glyphicon glyphicon-download\" aria-hidden=\"true\"></span>\n\t\tDownload\n\t </a>\n  <a role=\"button\" class=\"btn btn-success\" href=\"%url:beginner%\">\n  <span class=\"glyphicon glyphicon-education\" aria-hidden=\"true\"></span>\n  \tTutorial\n  </a>\n  <a role=\"button\" class=\"btn btn-success\" href=\"https://professional.cogsci.nl/\">\n  <span class=\"glyphicon glyphicon-comment\" aria-hidden=\"true\"></span>\n  Get support</a>\n</div>\n\n## Features\n\n- __A user-friendly interface__ — a modern, professional, and easy-to-use graphical [interface](%link:manual/interface%)\n- __Online experiments__ — run your experiment in a browser with [OSWeb](%link:manual/osweb/workflow%)\n- __Python__ — add the power of [Python](%link:manual/python/about%) to your experiment\n- __JavaScript__ — add the power of [JavaScript](%link:manual/python/about%) to your experiment\n- __Use your devices__ — use your [eye tracker](%link:pygaze%), [button box](%link:buttonbox%), [EEG equipment](%link:parallel%), and more.\n- __Free__ — released under the GPL3\n- __Crossplatform__ — Windows, Mac OS, and Linux\n\n## Citations\n\nMathôt, S., Schreij, D., & Theeuwes, J. (2012). OpenSesame: An open-source, graphical experiment builder for the social sciences. *Behavior Research Methods*, *44*(2), 314-324. doi:10.3758/s13428-011-0168-7\n\nMathôt, S., & March, J. (2022). Conducting linguistic experiments online with OpenSesame and OSWeb. *Language Learning*. doi:10.1111/lang.12509\n<br /><small>[Related preprint (not identical to published manuscript)](https://doi.org/10.31234/osf.io/wnryc)</small>\n": {
    "fr": "OpenSesame est un programme pour créer des expériences en psychologie, neurosciences et économie expérimentale. La dernière version $status$ est la version $version$ *$codename$*, publiée le $release-date$ ([notes de version](http://osdoc.cogsci.nl/$branch$/notes/$notes$)).\n\n<div class=\"btn-group\" role=\"group\" aria-label=\"...\">\n  <a role=\"button\" class=\"btn btn-success\" href=\"%url:download%\">\n\t\t<span class=\"glyphicon glyphicon-download\" aria-hidden=\"true\"></span>\n\t\tTélécharger\n\t </a>\n  <a role=\"button\" class=\"btn btn-success\" href=\"%url:beginner%\">\n  <span class=\"glyphicon glyphicon-education\" aria-hidden=\"true\"></span>\n  \tTutoriel\n  </a>\n  <a role=\"button\" class=\"btn btn-success\" href=\"https://professional.cogsci.nl/\">\n  <span class=\"glyphicon glyphicon-comment\" aria-hidden=\"true\"></span>\n  Obtenir de l'aide</a>\n</div>\n\n## Fonctionnalités\n\n- __Une interface conviviale__ — une [interface](%link:manual/interface%) graphique moderne, professionnelle et facile à utiliser\n- __Expériences en ligne__ — exécutez votre expérience dans un navigateur avec [OSWeb](%link:manual/osweb/workflow%)\n- __Python__ — ajoutez la puissance de [Python](%link:manual/python/about%) à votre expérience\n- __JavaScript__ — ajoutez la puissance de [JavaScript](%link:manual/python/about%) à votre expérience\n- __Utilisez vos appareils__ — utilisez votre [eye-tracker](%link:pygaze%), [boîtier à boutons](%link:buttonbox%), [équipement EEG](%link:parallel%), et plus encore.\n- __Gratuit__ — publié sous licence GPL3\n- __Multiplateforme__ — Windows, Mac OS et Linux\n\n## Références\n\nMathôt, S., Schreij, D., & Theeuwes, J. (2012). OpenSesame : Un constructeur d'expériences open-source et graphique pour les sciences sociales. *Behavior Research Methods*, *44*(2), 314-324. doi:10.3758/s13428-011-0168-7\n\nMathôt, S., & March, J. (2022). Réalisation d'expériences linguistiques en ligne avec OpenSesame et OSWeb. *Language Learning*. doi: 10.1111/lang.12509\n<br /><small>[Pré-impression associée (pas identique au manuscrit publié)](https://doi.org/10.31234/osf.io/wnryc)</small>"
  },
  "Siena 2018 workshop (Day 2)": {
    "fr": "Atelier Siena 2018 (Jour 2)"
  },
  "\n<notranslate>[TOC]</notranslate>\n\n## About the workshop\n\nThis OpenSesame workshop will take place at the University of Siena on February 22 and 23, 2018. This booklet corresponds to day 2.\n\nThe workshop consisted of two main parts. In the first part, corresponding to the Tutorial below, we created a complete experiment together. In the second part, corresponding to the Extra assignments below, the workshop participants improved this experiment by themselves, based on a few suggestions.\n\n- For Day 1, see: <http://osdoc.cogsci.nl/3.2/siena2018day1>\n\n\n## About this tutorial\n\nThis tutorial assumes a basic knowledge of OpenSesame and, for some parts, Python. Therefore, if you're not familiar with OpenSesame or Python, I recommend that you walk through the beginner and intermediate tutorials before continuing with this tutorial:\n\n- %link:beginner%\n- %link:intermediate%\n\nIn this tutorial, you will learn the following:\n\n- Eye tracking with PyGaze\n- Doing things in parallel with `coroutines`\n- Using advanced `loop` operations\n\n\n## About the experiment\n\nIn this tutorial, we will implement a *visual-world paradigm*, which was introduced by Cooper (1974; for a review see also Huettig, Rommers, and Meyer, 2011). In this paradigm, participants hear a spoken sentence, while they are looking at a display with several objects. We will use four separate objects presented in the four quadrants of the display (%FigParadigm).\n\n\n<notranslate>\nfigure:\n id: FigParadigm\n source: visual-world-paradigm.svg\n caption: >\n  A schematic of our trial sequence. This is an example of a Full Match trial, because the target object (the apple) is directly mentioned in the spoken sentence. Stimuli taken from the [BOSS](https://sites.google.com/site/bosstimuli/) stimuli (Brodier et al., 2010).\n</notranslate>\n\n\nThe spoken sentence refers to one or more of the objects. For example, an apple (the target object) may be shown while the spoken sentence \"at breakfast, the girl ate an apple\" is played back. In this case, the target matches the sentence fully. The sentence may also refer indirectly to a shown object. For example, an apple (again the target object) may be shown while the spoken sentence \"at breakfast, the girl ate a banana\" is played back. In this case, the target matches the sentence semantically, because a banana and an apple are both fruits that a girl may eat at breakfast.\n\nDuring the experiment, eye position is recorded, and the proportion of fixations on target and non-target objects is measured over time. The typical finding is then that the eyes are drawn toward target objects; that is, participants look mostly at objects that are directly or indirectly referred to by the spoken sentence. And the more direct the reference, the stronger this effect.\n\nNow let's make this more formal. Our experiment will have the following design:\n\n- One factor (Target Match) with two levels (Full or Semantic), varied within subjects. In the Full Match condition, the target object is directly mentioned in the sentence. In the Semantic Match condition, the target object is semantically related to an object that is mentioned in the sentence.\n- We have 16 spoken sentences and sixteen target objects. Every sentence and every target object is shown twice: once in the Full Match condition, and once in the Semantic Match condition.\n- We have 16 × 3 = 48 distractor objects, each of which (like the targets) is shown twice.\n- Each trial starts with a fixation dot for 1 s, followed by the presentation of the stimuli, followed 1 s later by the onset of the spoken sentence. The trial ends 5 s later.\n\n\n## The tutorial\n\n\n### Step 1: Download and start OpenSesame\n\nOpenSesame is available for Windows, Linux, Mac OS (experimental), and Android (runtime only). This tutorial is written for OpenSesame 3.2.X *Kafkaesque Koffka*. To be able to use PyGaze, you should download the Python 2.7 version (which is the default). You can download OpenSesame from here:\n\n- %link:download%": {
    "fr": "<notranslate>[TOC]</notranslate>\n\n## À propos de l'atelier\n\nCet atelier OpenSesame se déroulera à l'Université de Sienne les 22 et 23 février 2018. Ce livret correspond au jour 2.\n\nL'atelier se composait de deux parties principales. Dans la première partie, correspondant au tutoriel ci-dessous, nous avons créé une expérience complète ensemble. Dans la deuxième partie, correspondant aux missions supplémentaires ci-dessous, les participants à l'atelier ont amélioré cette expérience par eux-mêmes, sur la base de quelques suggestions.\n\n- Pour le jour 1, voir : <http://osdoc.cogsci.nl/3.2/siena2018day1>\n\n\n## À propos de ce tutoriel\n\nCe tutoriel suppose une connaissance de base d'OpenSesame et, pour certaines parties, de Python. Par conséquent, si vous n'êtes pas familier avec OpenSesame ou Python, je vous recommande de passer par les tutoriels pour débutants et intermédiaires avant de continuer avec ce tutoriel :\n\n- %link:beginner%\n- %link:intermediate%\n\nDans ce tutoriel, vous apprendrez ce qui suit :\n\n- Eye tracking avec PyGaze\n- Faire des choses en parallèle avec `coroutines`\n- Utilisation de `loop` avancé\n\n\n## À propos de l'expérience\n\nDans ce tutoriel, nous mettrons en œuvre un *paradigme du monde visuel*, qui a été introduit par Cooper (1974 ; pour une revue voir aussi Huettig, Rommers et Meyer, 2011). Dans ce paradigme, les participants entendent une phrase parlée, alors qu'ils regardent un écran avec plusieurs objets. Nous utiliserons quatre objets distincts présentés dans les quatre quadrants de l'affichage (%FigParadigm).\n\n\n<notranslate>\nfigure :\n id: FigParadigm\n source: visual-world-paradigm.svg\ncaption: >\n  Un schéma de notre séquence d'essai. Il s'agit d'un exemple d'essai de correspondance complète, car l'objet cible (la pomme) est directement mentionné dans la phrase parlée. Stimuli tirés des stimuli du [BOSS](https://sites.google.com/site/bosstimuli/) (Brodier et al., 2010).\n</notranslate>\n\n\nLa phrase parlée fait référence à un ou plusieurs objets. Par exemple, une pomme (l'objet cible) peut être représentée tandis que la phrase parlée \"au petit déjeuner, la fille a mangé une pomme\" est reproduite. Dans ce cas, la cible correspond à la phrase en totalité. La phrase peut également faire référence indirectement à un objet montré. Par exemple, une pomme (encore l'objet cible) peut être représentée tandis que la phrase parlée \"au petit déjeuner, la fille a mangé une banane\" est reproduite. Dans ce cas, la cible correspond à la phrase sémantiquement, car une banane et une pomme sont tous deux des fruits qu'une fille peut manger au petit déjeuner.\n\nPendant l'expérience, la position des yeux est enregistrée, et la proportion de fixations sur les objets cibles et non cibles est mesurée dans le temps. La constatation typique est alors que les yeux sont attirés par les objets cibles ; c'est-à-dire que les participants regardent principalement les objets qui sont directement ou indirectement mentionnés dans la phrase parlée. Et plus la référence est directe, plus cet effet est fort.\n\nFormalisons cela maintenant. Notre expérience aura la conception suivante :\n\n- Un facteur (correspondance cible) avec deux niveaux (complet ou sémantique), varié entre les sujets. Dans la condition de correspondance complète, l'objet cible est directement mentionné dans la phrase. Dans la condition de correspondance sémantique, l'objet cible est sémantiquement lié à un objet qui est mentionné dans la phrase.\n- Nous avons 16 phrases parlées et seize objets cibles. Chaque phrase et chaque objet cible est montré deux fois : une fois dans la condition de correspondance complète, et une fois dans la condition de correspondance sémantique.\n- Nous avons 16 x 3 = 48 objets distracteurs, dont chacun (comme les cibles) est montré deux fois.\n- Chaque essai commence par un point de fixation pendant 1 s, suivi de la présentation des stimuli, suivi 1 s plus tard par l'apparition de la phrase parlée. L'essai se termine 5 s plus tard.\n\n\n## Le tutoriel\n\n\n### Étape 1 : Télécharger et démarrer OpenSesame\n\nOpenSesame est disponible pour Windows, Linux, Mac OS (expérimental) et Android (runtime uniquement). Ce tutoriel est écrit pour OpenSesame 3.2.X *Kafkaesque Koffka*. Pour pouvoir utiliser PyGaze, vous devez télécharger la version Python 2.7 (qui est celle par défaut). Vous pouvez télécharger OpenSesame ici :\n\n- %link:download%"
  },
  "(If you start OpenSesame for the first time, you will see a Welcome tab. Dismiss this tab.) When you start OpenSesame, you will be given a choice of template experiments, and (if any) a list of recently opened experiments (see %FigStartUp). Click on 'Default Template' to start with an almost empty experiment.\n\n\n<notranslate>\nfigure:\n id: FigStartUp\n source: start-up.png\n caption: |\n  The OpenSesame window on start-up.\n</notranslate>\n\n\n### Step 2: Build the main structure of the experiment\n\nFor now, build the following main structure for your experiment (see also %FigMainStructure):\n\n1. We start with an instructions screen. This will be a `sketchpad`.\n2. Next, we run one block of trials. This will be a single `sequence`, corresponding to a single trial, inside a single `loop`, corresponding to a block of trials. You can leave the trial sequence empty for now!\n3. Finally, we end with a goodbye screen.\n\nWe also need to change the foreground color of the experiment to black, and the background color to white. This is because we will use images that have a white background, and we don't want these images to stand out!\n\nAnd don't forget to give your experiment a sensible name, and to save it!\n\n\n<notranslate>\nfigure:\n id: FigMainStructure\n source: main-structure.png\n caption: |\n  The main structure of the experiment.\n</notranslate>\n\n\n### Step 3: Import files into the file pool\n\nFor this experiment we need stimuli: sound files for the spoken sentences, and image files for the objects. Download these from the link below, extract the `zip` file, and place the stimuli in the file pool of your experiment (see also %FigFilePool).\n\n- %static:attachments/visual-world/stimuli.zip%\n\n<notranslate>\nfigure:\n id: FigFilePool\n source: file-pool.png\n caption: |\n  The file pool of your experiment after all stimuli have been added.\n</notranslate>\n\n\n### Step 4: Define experimental variables in the block_loop\n\nThe *block_loop* is where we define the experimental variables, by entering them into a table, where each row corresponds to a trial, and each column corresponds to an experimental variable.\n\nFor now, we define only the Full Match condition, in which the target object is directly mentioned in the spoken sentence. (We will add the Semantic Match condition as part of the Extra Assignments.)\n\nWe need the following variables. First, simply add columns to the loop table, without giving the rows any content.\n\n- `pic1` — the name of the first picture (e.g. 'apple.jpg')\n- `pic2` — the name of the second picture\n- `pic3` — the name of the third picture\n- `pic4` — the name of the fourth picture\n- `pos1` — the position of the first picture (e.g. 'topleft')\n- `pos2` — the position of the first picture\n- `pos3` — the position of the first picture\n- `pos4` — the position of the first picture\n- `sound` — the name of a sound file that contains a spoken sentence (e.g. 'apple.ogg').\n\nThe target object will always correspond to `pic1`. We have the following target objects; that is, for the following objects, we have sound files that refer to them. Simply copy-paste the following list into the `pic1` column of the table:\n\n~~~\napple.jpg\narmchair.jpg\nbanana.jpg\nbear.jpg\ncard.jpg\ncello.jpg\nchicken.jpg\ncookie.jpg\ncroissant.jpg\ndice.jpg\negg.jpg\nguitar.jpg\nkeyboard.jpg\nmouse.jpg\nsofa.jpg\nwolf.jpg\n~~~\n\nAnd do the same for the sound files:\n\n~~~\napple.ogg\narmchair.ogg\nbanana.ogg\nbear.ogg\ncard.ogg\ncello.ogg\nchicken.ogg\ncookie.ogg\ncroissant.ogg\ndice.ogg\negg.ogg\nguitar.ogg\nkeyboard.ogg\nmouse.ogg\nsofa.ogg\nwolf.ogg\n~~~\n\nThe rest of the pictures are distractors. Copy-paste the following list into the `pic2`, `pic3`, and `pic4` columns, in such a way that each column has exactly 16 rows. (If you accidentally make the table longer than 16 rows, simply select the extraneous rows, right-click and delete them.)": {
    "fr": "(Si vous démarrez OpenSesame pour la première fois, vous verrez un onglet de bienvenue. Fermez cet onglet.) Lorsque vous démarrez OpenSesame, on vous proposera des expériences modèles et (le cas échéant) une liste des expériences récemment ouvertes (voir %FigStartUp). Cliquez sur \"Modèle par défaut\" pour commencer avec une expérience presque vide.\n\n<notranslate>\nfigure :\n id : FigStartUp\n source : start-up.png\n légende : |\n  La fenêtre OpenSesame au démarrage.\n</notranslate>\n\n### Étape 2 : Construire la structure principale de l'expérience\n\nPour l'instant, construisez la structure principale suivante pour votre expérience (voir également %FigMainStructure) :\n\n1. On commence par un écran d'instructions. Ce sera un `sketchpad`.\n2. Ensuite, nous exécutons un bloc d'essais. Ce sera une seule `sequence`, correspondant à un seul essai, à l'intérieur d'une seule `loop`, correspondant à un bloc d'essais. Vous pouvez laisser la séquence d'essais vide pour l'instant !\n3. Enfin, nous terminons avec un écran d'au revoir.\n\nNous devons également changer la couleur de premier plan de l'expérience en noir et la couleur d'arrière-plan en blanc. Cela est dû au fait que nous utiliserons des images ayant un fond blanc, et nous ne voulons pas que ces images ressortent !\n\nEt n'oubliez pas de donner un nom judicieux à votre expérience et de la sauvegarder !\n\n<notranslate>\nfigure :\n id : FigMainStructure\n source : main-structure.png\n légende : |\n  La structure principale de l'expérience.\n</notranslate>\n\n### Étape 3 : Importer des fichiers dans la file d'attente\n\nPour cette expérience, nous avons besoin de stimuli : des fichiers sonores pour les phrases prononcées et des fichiers d'images pour les objets. Téléchargez ceux-ci à partir du lien ci-dessous, extrayez le fichier `zip` et placez les stimuli dans le pool de fichiers de votre expérience (voir également %FigFilePool).\n\n- %static : attachments/visual-world/stimuli.zip %\n\n<notranslate>\nfigure :\n id : FigFilePool\n source : file-pool.png\n légende : |\n  La file d'attente de votre expérience après l'ajout de tous les stimuli.\n</notranslate>\n\n### Étape 4 : Définir les variables expérimentales dans la block_loop\n\nLa *block_loop* est l'endroit où nous définissons les variables expérimentales, en les saisissant dans un tableau, où chaque ligne correspond à un essai, et chaque colonne correspond à une variable expérimentale.\n\nPour l'instant, nous définissons uniquement la condition de Correspondance Complète, dans laquelle l'objet cible est directement mentionné dans la phrase prononcée. (Nous ajouterons la condition de Correspondance Sémantique dans le cadre des Missions Supplémentaires.)\n\nNous aurons besoin des variables suivantes. Tout d'abord, ajoutez simplement des colonnes au tableau de boucle, sans donner de contenu aux lignes.\n\n- `pic1` — le nom de la première image (par exemple, 'apple.jpg')\n- `pic2` — le nom de la deuxième image\n- `pic3` — le nom de la troisième image\n- `pic4` — le nom de la quatrième image\n- `pos1` — la position de la première image (par exemple, 'topleft')\n- `pos2` — la position de la première image\n- `pos3` — la position de la première image\n- `pos4` — la position de la première image\n- `sound` — le nom d'un fichier sonore contenant une phrase prononcée (par exemple, 'apple.ogg').\n\nL'objet cible correspondra toujours à `pic1`. Nous avons les objets cibles suivants ; c'est-à-dire que pour les objets suivants, nous avons des fichiers sonores qui s'y réfèrent. Copiez-collez simplement la liste suivante dans la colonne `pic1` du tableau :\n\n~~~\napple.jpg\narmchair.jpg\nbanana.jpg\nbear.jpg\ncard.jpg\ncello.jpg\nchicken.jpg\ncookie.jpg\ncroissant.jpg\ndice.jpg\negg.jpg\nguitar.jpg\nkeyboard.jpg\nmouse.jpg\nsofa.jpg\nwolf.jpg\n~~~\n\nEt faites de même pour les fichiers sonores :\n\n~~~\napple.ogg\narmchair.ogg\nbanana.ogg\nbear.ogg\ncard.ogg\ncello.ogg\nchicken.ogg\ncookie.ogg\ncroissant.ogg\ndice.ogg\negg.ogg\nguitar.ogg\nkeyboard.ogg\nmouse.ogg\nsofa.ogg\nwolf.ogg\n~~~\n\nLe reste des images sont des distracteurs. Copiez-collez la liste suivante dans les colonnes `pic2`, `pic3` et `pic4`, de manière à ce que chaque colonne ait exactement 16 lignes. (Si vous allongez accidentellement le tableau sur plus de 16 lignes, sélectionnez simplement les lignes superflues, faites un clic droit et supprimez-les.)"
  },
  "~~~\nbasketball01.jpg\nbasketballhoop02.jpg\nbathtub.jpg\nbattery02b.jpg\nbattleaxe.jpg\nbattleship.jpg\nbeachpaddle01a.jpg\nbelt03b.jpg\nbookshelf.jpg\nbottlecap.jpg\nbowl01.jpg\nboxingglove02a.jpg\nboxtruck.jpg\nbracelet01.jpg\nbrainmodel.jpg\nbrick.jpg\nbulldozer.jpg\nbumpercar.jpg\nbust.jpg\nbutton01.jpg\ncactus.jpg\ncalculator01.jpg\ncalendar.jpg\ncamera01b.jpg\ncd.jpg\nceilingfan02.jpg\ncellphone.jpg\nmitten04.jpg\nmonument.jpg\nmoon.jpg\nmotorboat02.jpg\nmotoroilbottle03b.jpg\nmrpotatohead.jpg\nnailclipper03b.jpg\nneedlenosepliers03a.jpg\nnightstand.jpg\nnintendods.jpg\nnoparkingsign.jpg\noven.jpg\npacifier02a.jpg\npaintcan01.jpg\npants.jpg\npaperairplane.jpg\npaperclip02.jpg\nparkfountain.jpg\npatioumbrella.jpg\npencilsharpener03b.jpg\npeppermill01a.jpg\n~~~\n\nNow we need to specify the positions. Simply set:\n\n- `pos1` to 'topleft'\n- `pos2` to 'topright'\n- `pos3` to 'bottomleft'\n- `pos4` to 'bottomright'\n\nYour loop table should now look like %FigLoopTable.\n\n<notranslate>\nfigure:\n id: FigLoopTable\n source: loop-table.png\n caption: |\n  The `loop` table after all experimental variables have been defined.\n</notranslate>\n\n\n### Step 5: Apply advanced loop operations\n\nAlthough you have now defined all experimental variables, the `loop` table is not finished yet! Let's see what's wrong:\n\n\n__Positions__\n\n`pos1` is always the top left, meaning that `pic1` (the target object) is always presented at the top left of the display! (Assuming that we will implement our trial sequence such that these positions are used in that way.) And the same is true for `pos2`, `pos3`, and `pos4`.\n\nWe can fix this by horizontal shuffling of the `pos[x] columns`. That is, for each row, we randomly swap the values of these rows, such that this:\n\n~~~\npos1        pos2         pos3        pos4\ntopleft     topright     bottomleft  bottomright\ntopleft     topright     bottomleft  bottomright\n…\n~~~\n\nBecomes (say) this:\n\n~~~\npos1        pos2         pos3        pos4\nbottomleft  topleft      topright    bottomright\ntopright    bottomright  topright    bottomleft\n…\n~~~\n\nTo do this, view the script of *block_loop*, and add the following line of code to the very end of the script:\n\n~~~\nshuffle_horiz pos1 pos2 pos3 pos4\n~~~\n\nAnd click 'Apply and close'. If you now click on 'Preview', you will get a preview of what your loop table might look like if the experiment were actually run. And you will see that the `pos[x]` columns are horizontally shuffled, meaning that the pictures will be shown at random positions!\n\n\n__Distractors__\n\nThe distactor pictures are always linked to the same target object. For example, 'basketball01.jpg' always occurs together with the target 'apple.jpg'. But this is not what we want! Rather, we want the pairing between distactors and targets to be random, and different for all participants. (Except if by chance an identical pairing occurs for two participants.)\n\nWe can fix this by vertical shuffling of the `pic2`, `pic3`, and `pic4` columns. That is, the order of each of these columns should be shuffled independently. To do this, view the script again, and add the following lines to the very end of the script:\n\n~~~\nshuffle pic2\nshuffle pic3\nshuffle pic4\n~~~\n\nAnd click 'Apply and close'. If you now click on 'Preview', you will see that the `loop` table is properly randomized!\n\nFor more information about advanced loop operations, see:\n\n- %link:manual/structure/loop%\n\n\n<div class='info-box' markdown='1'>\n\n__Question__\n\nAt this point, you may wonder why we do not also need to horizontally shuffle the `pic2`, `pic3`, and `pic4` columns. But we don't! Do you know why not?\n\n</div>\n\n\n### Step 6: Create the trial sequence\n\nAs shown in %FigParadigm, our trial sequence is simple, and consists of:\n\n- central fixation dot (a `sketchpad`)\n- After 1000 ms: stimulus display (another `sketchpad`)\n- After 1000 ms: start sound playback (a `sampler`) while stimulus displays remains on screen\n- After 5000 ms: trial end": {
    "fr": "~~~\nbasketball01.jpg\nbasketballhoop02.jpg\nbaignoire.jpg\nbattery02b.jpg\nhache.jpg\ncuirassé.jpg\nraquetteplage01a.jpg\nbelt03b.jpg\netagere.jpg\ncapsule.jpg\nbol01.jpg\ngantdebox02a.jpg\ncamionnette.jpg\nbracelet01.jpg\nmodèlecervel.jpg\nbrique.jpg\nbulldozer.jpg\n_autotamponneuse.jpg\nbuste.jpg\nbouton01.jpg\ncactus.jpg\ncalculatrice01.jpg\ncalendrier.jpg\nappareilphoto01b.jpg\ncd.jpg\nventilateur_plafond02.jpg\ntéléphone portable.jpg\nmoufle04.jpg\nmonument.jpg\nlune.jpg\nmotortour02.jpg\nflaconhuilemoteur03b.jpg\nmrpatatetête.jpg\ncoupe-ongles03b.jpg\npincenef03a.jpg\ntabledenuit.jpg\nnintendods.jpg\npanneauinterdictionstationner.jpg\nfour.jpg\ntétines02a.jpg\npotdepeinture01.jpg\npantalon.jpg\navion_papier.jpg\ntrombone02.jpg\nfontaine_publique.jpg\n_parasolterrasse.jpg\ntaille-crayon03b.jpg\nmoulinàpoivre01a.jpg\n~~~\n\nMaintenant, nous devons spécifier les positions. Il suffit de définir :\n\n- `pos1` comme 'enhautagauche'\n- `pos2` comme 'enhautadroite'\n- `pos3` comme 'enbasagauche'\n- `pos4` comme 'enbasadroite'\n\nVotre tableau de boucle devrait maintenant ressembler à %FigLoopTable.\n\n<notranslate>\nfigure:\n id: FigLoopTable\n source: loop-table.png\n caption: |\n  Le tableau `loop` après que toutes les variables expérimentales ont été définies.\n</notranslate>\n\n### Étape 5 : Appliquer des opérations de boucle avancées\n\nBien que vous ayez défini toutes les variables expérimentales, le tableau `loop` n'est pas encore terminé ! Voyons ce qui ne va pas :\n\n__Positions__\n\n`pos1` est toujours en haut à gauche, ce qui signifie que `pic1` (l'objet cible) est toujours présenté en haut à gauche de l'écran ! (En supposant que nous allons mettre en œuvre notre séquence de test de telle sorte que ces positions sont utilisées de cette manière.) Et il en va de même pour `pos2`, `pos3` et `pos4`.\n\nNous pouvons résoudre cela en mélangeant horizontalement les colonnes `pos[x]`. C'est-à-dire que pour chaque rangée, nous échangeons aléatoirement les valeurs de ces rangées, de sorte que cela :\n\n~~~\npos1        pos2         pos3        pos4\nenhautagauche     enhautadroite     enbasagauche  enbasadroite\nenhautagauche     enhautadroite     enbasagauche  enbasadroite\n…\n~~~\n\nDevient (par exemple) ceci :\n\n~~~\npos1        pos2         pos3        pos4\nenbasagauche  enhautagauche      enhautadroite    enbasadroite\nenhautadroite    enbasadroite  enhautadroite    enbasagauche\n…\n~~~\n\nPour ce faire, affichez le script de *block_loop*, et ajoutez la ligne de code suivante à la toute fin du script :\n\n~~~\nshuffle_horiz pos1 pos2 pos3 pos4\n~~~\n\nEt cliquez sur « Appliquer et fermer ». Si vous cliquez maintenant sur « Aperçu », vous obtiendrez un aperçu de ce que votre tableau de boucle pourrait ressembler si l'expérience était réellement menée. Et vous verrez que les colonnes `pos[x]` sont mélangées horizontalement, ce qui signifie que les images seront présentées dans des positions aléatoires !\n\n__Distracteurs__\n\nLes images distractives sont toujours liées au même objet cible. Par exemple, « basketball01.jpg » se produit toujours avec la cible « apple.jpg ». Mais ce n'est pas ce que nous voulons ! Nous voulons plutôt que la liaison entre les distracteurs et les cibles soit aléatoire et différente pour tous les participants. (Sauf si par hasard une liaison identique se produit pour deux participants.)\n\nNous pouvons résoudre cela en mélangeant verticalement les colonnes `pic2`, `pic3` et `pic4`. Autrement dit, l'ordre de chacune de ces colonnes doit être mélangé indépendamment. Pour ce faire, affichez à nouveau le script et ajoutez les lignes suivantes à la toute fin du script :\n\n~~~\nshuffle pic2\nshuffle pic3\nshuffle pic4\n~~~\n\nEt cliquez sur « Appliquer et fermer ». Si vous cliquez maintenant sur « Aperçu », vous verrez que le tableau `loop` est correctement randomisé !\n\nPour plus d'informations sur les opérations de boucle avancées, voir :\n\n- %link:manual/structure/loop%\n\n<div class='info-box' markdown='1'>\n\n__Question__\n\nÀ ce stade, vous pouvez vous demander pourquoi nous ne devons pas également mélanger horizontalement les colonnes « pic2 », « pic3 » et « pic4 ». Mais nous ne le faisons pas ! Savez-vous pourquoi ?\n\n</div>\n\n### Étape 6 : Créez la séquence d'essai\n\nComme le montre %FigParadigm, notre séquence d'essai est simple et se compose de :\n\n- Point de fixation central (un « sketchpad »)\n- Après 1000 ms : Affichage du stimulus (un autre « sketchpad »)\n- Après 1000 ms : Lancement de la lecture du son (un « sampler ») pendant que l'affichage du stimulus reste à l'écran\n- Après 5000 ms : Fin de l'essai"
  },
  "For now, the trial sequence is therefore purely sequential, and we could implement it using only a `sequence`, as we've done in other tutorials. However, as one of the Extra Assignments, we want to analyze eye position *during* the trial sequence; in other words, later we'll want to do two things in parallel, and therefore we need a `coroutines` item. (Even if for now we won't do anything that requires this.)\n\nSo we want to have the following structure:\n\n- *trial_sequence* should contain a `coroutines` item (let's call it *trial_coroutines*) followed by a `logger` item.\n- *trial_coroutines* should have a duration of 7000 ms, and contain three items:\n  - A `sketchpad` for the fixation dot (let's call it *fixation_dot*) that is shown after 0 ms\n\t- A `sketchpad` for the stimulus display (let's call it *objects*) that is shown after 1000 ms\n\t- A `sampler` for the sound (let's call it *spoken_sentence*) that is shown after 2000 ms\n\n\nThe structure of your experiment should now look as in %FigCoroutinesStructure.\n\n\n<notranslate>\nfigure:\n id: FigCoroutinesStructure\n source: coroutines-structure.png\n caption: |\n  The experiment structure after defining the trial sequence.\n</notranslate>\n\n\n### Step 7: Define the visual stimuli\n\n__fixation_dot__\n\nThe *fixation_dot* is easily defined: simply draw a central fixation dot on it.\n\nNote that you don't need to specify the duration of the `sketchpad`, as you would normally need to do; this is because the item is part of *trial_coroutines*, and the timing is specified by the start and end time indicated there.\n\n\n__objects__\n\nTo define the *objects*, first create a prototype display, an example of what a display *might* look like on a particular trial. More specifically, draw a central fixation dot, and draw an arbitrary images in each of the four quadrants, as shown in %FigObjectsPrototype.\n\nAlso give each of the four objects a name: `pic1`, `pic2`, `pic3`, and `pic4`. We will use these names in the Extra Assignments to perform a regions-of-interest (ROI) analysis.\n\n\n<notranslate>\nfigure:\n id: FigObjectsPrototype\n source: objects-prototype.png\n caption: |\n  A prototype display with an arbitrary object in each of the four quadrants.\n</notranslate>\n\n\nOf course, we don't want to show the same objects over and over again. Rather, we want the `pic[x]` variables to specify which objects are shown, and the `pos[x]` variables to specify where these objects are shown. Let's start with the first object: the object in top-left, which in my example is an apple.\n\nView the script and find the line that corresponds to the first object. In my example, this is the following line:\n name=pic1\n~~~ .python\ndraw image center=1 file=\"apple.jpg\" scale=1 show_if=always x=-256 y=-192 z_index=0\n~~~\n\nNow change `file=\"apple.jpg\"` to `file=[pic1]`. This will make sure that the target picture as specified in the `pic1` variable is shown, rather than always the same apple.\n\nSo how can we use `pos1`, which has values like 'topleft', 'bottomright', etc., to specify the X and Y coordinates of the image? To do so, we make use of the fact that we can embed Python expressions in OpenSesame script, by using the `[=python_expression]` notation:\n\n- Change `x=-256` to `x=\"[=-256 if 'left' in var.pos1 else 256]\"`\n- Change `y=-192` to `y=\"[=-192 if 'top' in var.pos1 else 192]\"`\n\nAnd do the same for the other images, until the script looks this:": {
    "fr": "Pour l'instant, la séquence d'essai est donc purement séquentielle, et nous pourrions la mettre en œuvre en utilisant uniquement une `sequence`, comme nous l'avons fait dans d'autres tutoriels. Cependant, dans l'un des devoirs supplémentaires, nous voulons analyser la position des yeux *pendant* la séquence d'essai ; en d'autres termes, plus tard, nous voudrons faire deux choses en parallèle, et donc nous avons besoin d'un élément `coroutines`. (Même si pour l'instant nous ne ferons rien qui nécessite cela.)\n\nNous voulons donc avoir la structure suivante :\n\n- *trial_sequence* doit contenir un élément `coroutines` (appelons-le *trial_coroutines*) suivi d'un élément `logger`.\n- *trial_coroutines* doit avoir une durée de 7000 ms et contenir trois éléments :\n  - Un `sketchpad` pour le point de fixation (appelons-le *fixation_dot*) qui est montré après 0 ms\n\t- Un `sketchpad` pour l'affichage des stimuli (appelons-le *objects*) qui est montré après 1000 ms\n\t- Un `sampler` pour le son (appelons-le *spoken_sentence*) qui est montré après 2000 ms\n\nLa structure de votre expérience devrait maintenant ressembler à celle de %FigCoroutinesStructure.\n\n<notranslate>\nfigure:\n id: FigCoroutinesStructure\n source: coroutines-structure.png\n caption: |\n  La structure de l'expérience après avoir défini la séquence d'essai.\n</notranslate>\n\n### Étape 7 : Définir les stimuli visuels\n\n__fixation_dot__\n\nLe *fixation_dot* est facile à définir : il suffit de dessiner un point de fixation central sur celui-ci.\n\nNotez que vous n'avez pas besoin de spécifier la durée du `sketchpad`, comme vous devriez normalement le faire ; cela est dû au fait que l'élément fait partie de *trial_coroutines*, et le timing est spécifié par le temps de début et de fin indiqué là-bas.\n\n__objects__\n\nPour définir les *objects*, créez d'abord un prototype d'affichage, un exemple de ce à quoi un affichage *pourrait* ressembler lors d'un essai particulier. Plus précisément, dessinez un point de fixation central et dessinez une image arbitraire dans chacun des quatre quadrants, comme le montre %FigObjectsPrototype.\n\nDonnez également à chacun des quatre objets un nom : `pic1`, `pic2`, `pic3` et `pic4`. Nous utiliserons ces noms dans les devoirs supplémentaires pour effectuer une analyse des régions d'intérêt (ROI).\n\n<notranslate>\nfigure:\n id: FigObjectsPrototype\n source: objects-prototype.png\n caption: |\n  Un prototype d'affichage avec un objet arbitraire dans chacun des quatre quadrants.\n</notranslate>\n\nBien sûr, nous ne voulons pas montrer les mêmes objets encore et encore. Plutôt, nous voulons que les variables `pic[x]` spécifient quels objets sont montrés, et que les variables `pos[x]` spécifient où ces objets sont montrés. Commençons par le premier objet : l'objet en haut à gauche, qui dans mon exemple est une pomme.\n\nConsultez le script et trouvez la ligne qui correspond au premier objet. Dans mon exemple, il s'agit de la ligne suivante :\n name=pic1\n~~~ .python\ndraw image center=1 file=\"apple.jpg\" scale=1 show_if=always x=-256 y=-192 z_index=0\n~~~\n\nChangez maintenant `file=\"apple.jpg\"` en `file=[pic1]`. Cela permettra de montrer l'image cible telle que spécifiée dans la variable `pic1`, plutôt que toujours la même pomme.\n\nAlors, comment pouvons-nous utiliser `pos1`, qui a des valeurs comme 'topleft', 'bottomright', etc., pour spécifier les coordonnées X et Y de l'image ? Pour ce faire, nous profitons du fait que nous pouvons intégrer des expressions Python dans le script OpenSesame, en utilisant la notation `[=python_expression]` :\n\n- Changez `x=-256` en `x=\"[=-256 if 'left' in var.pos1 else 256]\"` \n- Changez `y=-192` en `y=\"[=-192 if 'top' in var.pos1 else 192]\"` \n\nEt faites la même chose pour les autres images, jusqu'à ce que le script ressemble à ceci :"
  },
  "~~~ .python\ndraw fixdot color=black show_if=always style=default x=0 y=0 z_index=0\ndraw image center=1 file=\"[pic1]\" scale=1 show_if=always x=\"[=-256 if 'left' in var.pos1 else 256]\" y=\"[=-192 if 'top' in var.pos1 else 192]\" z_index=0\ndraw image center=1 file=\"[pic2]\" scale=1 show_if=always x=\"[=-256 if 'left' in var.pos2 else 256]\" y=\"[=-192 if 'top' in var.pos2 else 192]\" z_index=0\ndraw image center=1 file=\"[pic3]\" scale=1 show_if=always x=\"[=-256 if 'left' in var.pos3 else 256]\" y=\"[=-192 if 'top' in var.pos3 else 192]\" z_index=0\ndraw image center=1 file=\"[pic4]\" scale=1 show_if=always x=\"[=-256 if 'left' in var.pos4 else 256]\" y=\"[=-192 if 'top' in var.pos4 else 192]\" z_index=0\n~~~\n\n\n<div class='info-box' markdown='1'>\n\n__Try it yourself: the `if` expression__\n\nIf you're not familiar with the Python `if` *expression*, which is slightly different from the traditional `if` *statement*, open the debug window, and enter the following line:\n\n~~~ .python\nprint('This is shown if True' if True else 'This is shown if False')\n~~~\n\nWhat do you see? Now change `if True else` into `if False else` and run the line again. What do you see now? Do you get the logic?\n\n</div>\n\n\n### Step 8: Define the sound\n\nDefining the sound is easy: simply open the *spoken_sentence* item, and enter '[sound]' in the 'Sound file' box, indicating that the variable `sound` specifies the sound file.\n\n\n### Step 9: Add basic eye tracking\n\nEye tracking is done with the [PyGaze](%url:manual/eyetracking/pygaze%) plug-ins, which are installed by default in OpenSesame. The general procedure is as follows:\n\n- At the start of the experiment, the eye tracker is *initialized and calibrated* with the `pygaze_init` item. This is also where you indicate what eye tracker you wan to use. During, it's convenient to select the Advanced Dummy eye tracker, which allows you to simulate eye movements with the mouse.\n- Before each trial, a *drift-correction* procedure is performed with the `pygaze_drift_correct` item. During drift correction, a single point is shown on the screen and the participant looks at it. This allows the eye tracker to see how much drift error there is in the eye-position measurement. How this error is treated depends on your eye tracker and settings:\n  - The drift error is either used for a single-point recalibration\n  - Or a simple check is performed to see if the drift error does not exceed a certain maximum error, giving the possibility to recalibrate if the maximum error is exceeded.\n- Next, still before each trial, the eye-tracker is told to start collecting data with the `pygaze_start_recording` item. You can specify a status message to indicate the start of each trial. It's convenient to include a trial number in this status message (e.g. 'start_trial [count_trial_sequence]').\n- At the end of each trial, data is sent to the eye-tracker log file with the `pygaze_log` item. It's convenient to enable the 'Automatically detect and log all variables' option.\n- Finally, at the very end of each trial, the eye tracker is told to stop recording with the `pygaze_stop_recording` item.\n\nThe structure of your experiment should now look as in %FigEyeTrackingStructure.\n\n\n<notranslate>\nfigure:\n id: FigEyeTrackingStructure\n source: eye-tracking-structure.png\n caption: |\n  The structure of the experiment after adding PyGaze items for eye tracking.\n</notranslate>\n\n\n### Step 10: Define instructions and goodbye screen\n\nWe now have a working experiment! But we haven't added any content to the *instructions* and *goodbye* items yet. So before you run the experiment, open these items and add some text.\n\n### Step 11: Run the experiment!\n\nCongratulations—you have implemented a visual-world paradigm! It's now time to give your experiment a quick test run by clicking on the orange play button (shortcut: `Ctrl+Shift+W`).\n\n\n## Extra assignments\n\n### Extra 1: Define the Semantic Match condition": {
    "fr": "~~~ .python\ndraw fixdot color=noir show_if=toujours style=default x=0 y=0 z_index=0\ndraw image center=1 file=\"[pic1]\" scale=1 show_if=toujours x=\"[=-256 if 'left' in var.pos1 else 256]\" y=\"[=-192 if 'top' in var.pos1 else 192]\" z_index=0\ndraw image center=1 file=\"[pic2]\" scale=1 show_if=toujours x=\"[=-256 if 'left' in var.pos2 else 256]\" y=\"[=-192 if 'top' in var.pos2 else 192]\" z_index=0\ndraw image center=1 file=\"[pic3]\" scale=1 show_if=toujours x=\"[=-256 if 'left' in var.pos3 else 256]\" y=\"[=-192 if 'top' in var.pos3 else 192]\" z_index=0\ndraw image center=1 file=\"[pic4]\" scale=1 show_if=toujours x=\"[=-256 if 'left' in var.pos4 else 256]\" y=\"[=-192 if 'top' in var.pos4 else 192]\" z_index=0\n~~~\n\n\n<div class='info-box' markdown='1'>\n\n__Essayez-le vous-même : l'expression `if`__\n\nSi vous n'êtes pas familier avec l'expression `if` en Python, qui est légèrement différente de l'instruction `if` traditionnelle, ouvrez la fenêtre de débogage et saisissez la ligne suivante :\n\n~~~ .python\nprint('Ceci est affiché si True' if True else 'Ceci est affiché si False')\n~~~\n\nQue voyez-vous ? Changez maintenant `if True else` en `if False else` et exécutez à nouveau la ligne. Que voyez-vous maintenant ? Vous comprenez la logique ?\n\n</div>\n\n\n### Étape 8 : Définir le son\n\nDéfinir le son est facile : ouvrez simplement l'élément *spoken_sentence* et entrez '[sound]' dans la case 'Sound file', indiquant que la variable `sound` spécifie le fichier son.\n\n\n### Étape 9 : Ajouter un suivi oculaire de base\n\nLe suivi oculaire est effectué avec les plug-ins [PyGaze](%url:manual/eyetracking/pygaze%), qui sont installés par défaut dans OpenSesame. La procédure générale est la suivante :\n\n- Au début de l'expérience, l'eye tracker est *initialisé et calibré* avec l'élément `pygaze_init`. C'est également là que vous indiquez quel eye tracker vous souhaitez utiliser. Pendant, il est pratique de sélectionner le eye tracker Advanced Dummy, qui vous permet de simuler des mouvements oculaires avec la souris.\n- Avant chaque essai, une procédure de *correction de dérive* est effectuée avec l'élément `pygaze_drift_correct`. Pendant la correction de dérive, un seul point est affiché à l'écran et le participant le regarde. Cela permet au eye tracker de voir combien d'erreur de dérive il y a dans la mesure de la position des yeux. La façon dont cette erreur est traitée dépend de votre eye tracker et de vos paramètres :\n  - L'erreur de dérive est soit utilisée pour un recalibrage en un seul point.\n  - Ou un simple contrôle est effectué pour voir si l'erreur de dérive ne dépasse pas une certaine erreur maximale, donnant la possibilité de recalibrer si l'erreur maximale est dépassée.\n- Ensuite, toujours avant chaque essai, on demande au eye-tracker de commencer à collecter des données avec l'élément `pygaze_start_recording`. Vous pouvez spécifier un message d'état pour indiquer le début de chaque essai. Il est pratique d'intégrer un numéro d'essai dans ce message d'état (par exemple 'start_trial [count_trial_sequence]').\n- À la fin de chaque essai, les données sont envoyées au fichier journal de l'eye-tracker avec l'élément `pygaze_log`. Il est pratique d'activer l'option 'Automatically detect and log all variables'.\n- Enfin, à la toute fin de chaque essai, on demande au eye tracker d'arrêter l'enregistrement avec l'élément `pygaze_stop_recording`.\n\nLa structure de votre expérience doit maintenant être similaire à celle de %FigEyeTrackingStructure.\n\n\n<notranslate>\nfigure:\n id: FigEyeTrackingStructure\n source: eye-tracking-structure.png\n caption: |\n  La structure de l'expérience après avoir ajouté des éléments PyGaze pour le suivi oculaire.\n</notranslate>\n\n\n### Étape 10 : Définir les instructions et l'écran d'au revoir\n\nNous avons maintenant une expérience fonctionnelle ! Mais nous n'avons pas encore ajouté de contenu aux éléments *instructions* et *goodbye*. Donc, avant de lancer l'expérience, ouvrez ces éléments et ajoutez du texte.\n\n### Étape 11 : Lancer l'expérience !\n\nFélicitations - vous avez mis en œuvre un paradigme de monde visuel ! Il est maintenant temps de tester rapidement votre expérience en cliquant sur le bouton de lecture orange (raccourci : `Ctrl+Shift+W`).\n\n\n## Travaux supplémentaires\n\n### Supplément 1 : Définir la condition de correspondance sémantique"
  },
  "So far, we have only implemented the Full Match condition, in which the target object (e.g. 'apple') is explicitly mentioned in the spoken sentence (e.g. 'at breakfast, the girl ate an apple').\n\nNow, also implement the Semantic Match condition, in which each target (e.g. 'apple') is paired with a semantically related spoken sentence (e.g. 'at breakfast, the girl at a banana'). The stimuli have been created such that there is one semantically related spoken sentence for each target object.\n\nIn every other way, the Semantic Match condition should be identical to the Full Match condition.\n\nAnd don't forget to create a variable that indicates the condition!\n\n\n### Extra 2: Use Python constants to define coordinates\n\nRight now, the coordinates of the objects have been hard-coded in the *objects* script, in the sense that the coordinates have been typed directly into the script:\n\n~~~ .python\nx=\"[=-256 if 'left' in var.pos1 else 256]\"\n~~~\n\nIt's more elegant to define the coordinates (`XLEFT`, `XRIGHT`, `YTOP`, and `YBOTTOM`) as constants in an `inline_script` at the start of the experiment, and then refer to these constants in the *objects* script.\n\n\n<div class='info-box' markdown='1'>\n\n__Constants in Python__\n\nIn computer science, a *constant* is a variable with a value that you cannot change. In Python, you can always change variables, so constants don't strictly speaking exist in the language. However, if you have a variable that you treat as though it were a constant (i.e. you define it once and never change its value), you typically indicate this by writing the variable name in `ALL_CAPS`.\n\nSuch naming conventions are described in Python's PEP-8 style guidelines:\n\n- <https://www.python.org/dev/peps/pep-0008/>\n\n</div>\n\n\n### Extra 3: Analyze eye position online (challenging!)\n\nIn *trial_coroutines*, you can indicate the name of a generator function (see below for an explanation of generators). Let's enter the name `roi_analysis` here, and also create an `inline_script` at the start of the experiment in which we define this function.\n\nHere's a partly implemented `roi_analysis()` function. Can you finish the TODO list?\n\n~~~ .python\ndef roi_analysis():\n\n\t# sample_nr will be used to create a different variable name for each\n\t# 500 ms sample\n\tsample_nr = 0\n\t# This first yield indicates that the generator has finished preparing\n\tyield\n\t# Retrieve the canvas of the objects sketchpad. We need to do this after\n\t# the yield statement that signals the end of preparation, because that we\n\t# are sure that the canvas object has been constructed (which also happens)\n\t# during preparation.\n\tcanvas = items['objects'].canvas\n\twhile True:\n\t\t# We only want to analyze a gaze sample once very 500 ms. This is so\n\t\t# that we don't end up with too many columns in the log file. If it's\n\t\t# not time to analyze a gaze sample, simply yield and continue.\n\t\tif not clock.once_in_a_while(ms=500):\n\t\t\tyield # so that other items in the coroutines can run\n\t\t\tcontinue\n\t\t#\n\t\t# TODO:\n\t\t#\n\t\t# - Get an eye-position coordinate from the eye tracker\n\t\t#   (Hint: Use eyetracker.sample())\n\t\t# - Check which sketchpad elements are at this coordinate (if any)\n\t\t#   (Hint: use canvas.elements_at())\n\t\t# - If pic1 (the target object) is among these elements set\n\t\t#   var.on_target_[sample_nr] to 1, else to 0\n\t\t#   (Hint: use var.set())\n~~~\n\nSee also:\n\n- %link:manual/structure/coroutines%\n\n<div class='info-box' markdown='1'>\n\n__Generator functions in Python__\n\nIn Python, a *generator* function is a function with a `yield` statement. A `yield` statement is similar to a `return` statement, in that it stops a function. However, whereas `return` stops a function permanently, `yield` merely suspends a function—and the function can later resume from the `yield` point onward.\n\n</div>\n\n## Download the experiment\n\nYou can download the full experiment from here:\n\n- <https://osf.io/z27rt/>\n\n\n## References": {
    "fr": "Jusqu'à présent, nous n'avons mis en œuvre que la condition de correspondance complète, dans laquelle l'objet cible (par exemple, 'pomme') est explicitement mentionné dans la phrase parlée (par exemple, 'au petit déjeuner, la fille a mangé une pomme').\n\nMaintenant, mettez également en œuvre la condition de correspondance sémantique, dans laquelle chaque cible (par exemple, 'pomme') est associée à une phrase parlée sémantiquement liée (par exemple, 'au petit déjeuner, la fille a mangé une banane'). Les stimuli ont été créés de manière à ce qu'il y ait une phrase parlée sémantiquement liée pour chaque objet cible.\n\nDe toutes les autres manières, la condition de correspondance sémantique doit être identique à la condition de correspondance complète.\n\nEt n'oubliez pas de créer une variable qui indique la condition !\n\n### Extra 2 : Utiliser des constantes Python pour définir les coordonnées\n\nActuellement, les coordonnées des objets ont été codées en dur dans le script *objects*, en ce sens que les coordonnées ont été directement saisies dans le script :\n\n~~~ .python\nx=\"[=-256 si 'left' dans var.pos1 else 256]\"\n~~~\n\nIl est plus élégant de définir les coordonnées (`XLEFT`, `XRIGHT`, `YTOP` et `YBOTTOM`) en tant que constantes dans un `inline_script` au début de l'expérience, puis de se référer à ces constantes dans le script *objects*.\n\n<div class='info-box' markdown='1'>\n\n__Constantes en Python__\n\nEn informatique, une *constante* est une variable dont la valeur ne peut pas être modifiée. En Python, vous pouvez toujours modifier des variables, donc les constantes n'existent pas strictement parlant dans le langage. Cependant, si vous avez une variable que vous traitez comme si c'était une constante (c'est-à-dire que vous la définissez une fois et ne changez jamais sa valeur), vous l'indiquez généralement en écrivant le nom de la variable en `MAJUSCULES`.\n\nCes conventions de nommage sont décrites dans les directives de style PEP-8 de Python :\n\n- <https://www.python.org/dev/peps/pep-0008/>\n\n</div>\n\n### Extra 3 : Analyser en ligne la position des yeux (difficile !)\n\nDans *trial_coroutines*, vous pouvez indiquer le nom d'une fonction de générateur (voir ci-dessous pour une explication des générateurs). Entrez ici le nom `roi_analysis` et créez également un `inline_script` au début de l'expérience dans lequel nous définissons cette fonction.\n\nVoici une fonction `roi_analysis()` partiellement implémentée. Pouvez-vous terminer la liste des tâches à faire ?\n\n~~~ .python\ndef roi_analysis():\n\n\t# sample_nr sera utilisé pour créer un nom de variable différent pour chacun\n\t# échantillon de 500 ms\n\tsample_nr = 0\n\t# Ce premier rendement indique que le générateur a fini de se préparer\n\tyield\n\t# Récupérez le canevas du sketchpad des objets. Nous devons le faire après\n\t# la déclaration de rendement qui signale la fin de la préparation, parce que nous\n\t# sommes sûrs que l'objet canevas a été construit (ce qui se produit également)\n\t# lors de la préparation.\n\tcanvas = items['objects'].canvas\n\twhile True:\n\t\t# Nous ne voulons analyser un échantillon de regard que toutes les 500 ms. Cela est fait pour\n\t\t# que nous n'ayons pas trop de colonnes dans le fichier journal. Si ce n'est pas\n\t\t# le temps d'analyser un échantillon de regard, il suffit de céder et de continuer.\n\t\tif not clock.once_in_a_while(ms=500):\n\t\t\tyield # afin que d'autres éléments dans les coroutines puissent fonctionner\n\t\t\tcontinue\n\t\t#\n\t\t# TODO :\n\t\t#\n\t\t# - Obtenir une coordonnée de position des yeux du suivi des yeux\n\t\t#   (Astuce : Utiliser eyetracker.sample())\n\t\t# - Vérifier quels éléments de sketchpad sont à cette coordonnée (le cas échéant)\n\t\t#   (Astuce : utiliser canvas.elements_at())\n\t\t# - Si pic1 (l'objet cible) fait partie de ces éléments, définissez\n\t\t#   var.on_target_[sample_nr] sur 1, sinon sur 0\n\t\t#   (Astuce : utiliser var.set ())\n~~~\n\nVoir aussi :\n\n- %link:manual/structure/coroutines%\n\n<div class='info-box' markdown='1'>\n\n__Fonctions de générateur en Python__\n\nEn Python, une fonction *générateur* est une fonction avec une déclaration `yield`. Une déclaration `yield` est similaire à une déclaration `return`, en ce sens qu'elle arrête une fonction. Cependant, alors que `return` arrête une fonction de manière permanente, `yield` suspend simplement une fonction, et la fonction peut ensuite reprendre à partir du point `yield`.\n\n</div>\n\n## Télécharger l'expérience\n\nVous pouvez télécharger l'expérience complète à partir d'ici :\n\n- <https://osf.io/z27rt/>\n\n\n## Références"
  },
  "Brodeur, M. B., Dionne-Dostie, E., Montreuil, T., Lepage, M., & Op de Beeck, H. P. (2010). The Bank of Standardized Stimuli (BOSS), a new set of 480 normative photos of objects to be used as visual stimuli in cognitive research. *PloS ONE*, *5*(5), e10773. doi:10.1371/journal.pone.0010773\n{: .reference}\n\nCooper, R. M. (1974). The control of eye fixation by the meaning of spoken language: A new methodology for the real-time investigation of speech perception, memory, and language processing. *Cognitive Psychology*, *6*(1), 84–107. doi:10.1016/0010-0285(74)90005-X\n{: .reference}\n\nDalmaijer, E., Mathôt, S., & Van der Stigchel, S. (2014). PyGaze: An open-source, cross-platform toolbox for minimal-effort programming of eyetracking experiments. *Behavior Research Methods*, *46*(4), 913–921. doi:10.3758/s13428-013-0422-2\n{: .reference}\n\nHuettig, F., Rommers, J., & Meyer, A. S. (2011). Using the visual world paradigm to study language processing: A review and critical evaluation. *Acta Psychologica*, *137*(2), 151–171. doi:10.1016/j.actpsy.2010.11.003\n{: .reference}\n\nMathôt, S., Schreij, D., & Theeuwes, J. (2012). OpenSesame: An open-source, graphical experiment builder for the social sciences. *Behavior Research Methods*, *44*(2), 314–324. doi:10.3758/s13428-011-0168-7\n{: .reference}\n": {
    "fr": "Brodeur, M. B., Dionne-Dostie, E., Montreuil, T., Lepage, M., & Op de Beeck, H. P. (2010). La Banque de Stimuli Standardisés (BOSS), un nouvel ensemble de 480 photos normatives d'objets à utiliser comme stimuli visuels dans la recherche cognitive. *PloS ONE*, *5*(5), e10773. doi:10.1371/journal.pone.0010773\n{: .reference}\n\nCooper, R. M. (1974). Le contrôle de la fixation des yeux par le sens du langage parlé: Une nouvelle méthodologie pour l'étude en temps réel de la perception du discours, de la mémoire et du traitement du langage. *Cognitive Psychology*, *6*(1), 84–107. doi:10.1016/0010-0285(74)90005-X\n{: .reference}\n\nDalmaijer, E., Mathôt, S., & Van der Stigchel, S. (2014). PyGaze: Une boîte à outils open-source et multiplateforme pour la programmation d'expériences de suivi du regard avec un effort minimal. *Behavior Research Methods*, *46*(4), 913–921. doi:10.3758/s13428-013-0422-2\n{: .reference}\n\nHuettig, F., Rommers, J., & Meyer, A. S. (2011). Utilisation du paradigme du monde visuel pour étudier le traitement du langage: Un examen et une évaluation critique. *Acta Psychologica*, *137*(2), 151–171. doi:10.1016/j.actpsy.2010.11.003\n{: .reference}\n\nMathôt, S., Schreij, D., & Theeuwes, J. (2012). OpenSesame: Un constructeur d'expériences graphiques open-source pour les sciences sociales. *Behavior Research Methods*, *44*(2), 314–324. doi:10.3758/s13428-011-0168-7\n{: .reference}"
  },
  "Support": {
    "fr": "Assistance"
  },
  "\n## Free support\n\nWe offer free support through the community forum at <https://forum.cogsci.nl/>.\n\n\n## Professional (paid) support\n\nFor professional (paid) support, visit <https://professional.cogsci.nl/>.\n": {
    "fr": "## Support gratuit\n\nNous offrons un support gratuit via le forum communautaire à l'adresse <https://forum.cogsci.nl/>.\n\n## Support professionnel (payant)\n\nPour un support professionnel (payant), visitez <https://professional.cogsci.nl/>."
  },
  "Siena 2018 workshop (Day 1)": {
    "fr": "Atelier Siena 2018 (Jour 1)"
  },
  "<notranslate>[TOC]</notranslate>\n\n\n## About the workshop\n\nThis OpenSesame workshop will take place at the University of Siena on February 22 and 23, 2018. This booklet corresponds to day 1.\n\nThe workshop consisted of two main parts. In the first part, corresponding to the Tutorial below, we created a complete experiment together. In the second part, corresponding to the Extra assignments below, the workshop participants improved this experiment by themselves, based on a few suggestions.\n\nYou can download the full experiment, including the solutions of the extra assignments here:\n\n- <http://osf.io/jw7dr>\n\n- For Day 2, see: <http://osdoc.cogsci.nl/3.2/siena2018day2>\n\n\n## The tutorial\n\n<notranslate>\nfigure:\n id: FigMeowingCapybara\n source: meowing-capybara.png\n caption: |\n  Don't be fooled by meowing capybaras! ([Source][capybara_photo])\n</notranslate>\n\n<notranslate>[TOC]</notranslate>\n\nWe will create a simple animal-filled multisensory integration task, in which participants see a picture of a dog, cat, or capybara. A meow or a bark is played while the picture is shown. The participant reports whether a dog or a cat is shown, by pressing the right or the left key. No response should be given when a capybara is shown: those are catch trials.\n\nTo make things more fun, we will design the experiment so that you can run it on [OSWeb](http://osweb.cogsci.nl/), an online runtime for OpenSesame experiments (which is still a work in progress, but it works for basic experiments).\n\nWe make two simple predictions:\n\n- Participants should be faster to identify dogs when a barking sound is played, and faster to identify cats when a meowing sound is played. In other words, we expect a multisensory congruency effect.\n- When participants see a capybara, they are more likely to report seeing a dog when they hear a bark, and more likely to report seeing a cat when they hear a meow. In other words, false alarms are biased by the sound.\n\n\n### Step 1: Download and start OpenSesame\n\nOpenSesame is available for Windows, Linux, Mac OS, and Android (runtime only). This tutorial is written for OpenSesame 3.1.X, and you can use either the version based on Python 2.7 (default) or Python 3.5. You can download OpenSesame from here:\n\n- %link:download%\n\nWhen you start OpenSesame, you will be given a choice of template experiments, and (if any) a list of recently opened experiments (see %FigStartUp).\n\n<notranslate>\nfigure:\n id: FigStartUp\n source: start-up.png\n caption: |\n  The OpenSesame window on start-up.\n</notranslate>\n\nThe *Extended template* provides a good starting point for creating Android-based experiments. However, in this tutorial we will create the entire experiment from scratch. Therefore, we will continue with the 'default template', which is already loaded when OpenSesame is launched (%FigDefaultTemplate). Therefore, simply close the 'Get started!' and (if shown) 'Welcome!' tabs.\n\n<notranslate>\nfigure:\n id: FigDefaultTemplate\n source: default-template.png\n caption: |\n  The structure of the 'Default template' as seen in the overview area.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Background box 1: Basics**\n\nOpenSesame experiments are collections of *items*. An item is a small chunk of functionality that, for example, can be used to present visual stimuli (the SKETCHPAD item) or to record key presses (the KEYBOARD_RESPONSE item). Items have a type and a name. For example, you might have two items of the type KEYBOARD_RESPONSE with the names *t1_response* and *t2_response*. To make the distinction between item types and item names clear, we will use THIS_STYLE for types, and *this style* for names.\n\nTo give structure to your experiment, two types of items are especially important: the LOOP and the SEQUENCE. Understanding how you can combine LOOPs and SEQUENCEs to build experiments is perhaps the trickiest part of working with OpenSesame, so let's get that out of the way first.": {
    "fr": "## À propos de l'atelier\n\nCet atelier OpenSesame aura lieu à l'Université de Sienne les 22 et 23 février 2018. Ce livret correspond au jour 1.\n\nL'atelier se composait de deux parties principales. Dans la première partie, correspondant au didacticiel ci-dessous, nous avons créé une expérience complète ensemble. Dans la deuxième partie, correspondant aux missions supplémentaires ci-dessous, les participants à l'atelier ont amélioré cette expérience par eux-mêmes, en fonction de quelques suggestions.\n\nVous pouvez télécharger l'expérience complète, y compris les solutions des missions supplémentaires ici :\n\n- <http://osf.io/jw7dr>\n\n- Pour le jour 2, voir : <http://osdoc.cogsci.nl/3.2/siena2018day2>\n\n\n## Le tutoriel\n\n<notranslate>\nfigure:\n id: FigMeowingCapybara\n source: meowing-capybara.png\n caption: |\n  Ne vous laissez pas berner par les capybaras qui miaulent ! ([Source][capybara_photo])\n</notranslate>\n\n<notranslate>[TOC]</notranslate>\n\nNous allons créer une tâche simple d'intégration multisensorielle remplie d'animaux, dans laquelle les participants voient une image d'un chien, d'un chat ou d'un capybara. Un miaulement ou un aboiement est joué pendant que l'image est montrée. Le participant signale si un chien ou un chat est montré, en appuyant sur la touche droite ou gauche. Aucune réponse ne doit être donnée lorsqu'un capybara est montré : il s'agit de tests de capture.\n\nPour rendre les choses plus amusantes, nous concevrons l'expérience de manière à ce que vous puissiez l'exécuter sur [OSWeb](http://osweb.cogsci.nl/), un runtime en ligne pour les expériences OpenSesame (qui est encore en cours de développement, mais fonctionne pour des expériences de base).\n\nNous faisons deux prédictions simples :\n\n- Les participants devraient être plus rapides pour identifier les chiens lorsqu'un son d'aboiement est joué, et plus rapides pour identifier les chats lorsqu'un son de miaulement est joué. En d'autres termes, nous nous attendons à un effet de congruence multisensorielle.\n- Lorsque les participants voient un capybara, ils sont plus susceptibles de signaler avoir vu un chien lorsqu'ils entendent un aboiement, et plus susceptibles de signaler avoir vu un chat lorsqu'ils entendent un miaulement. En d'autres termes, les fausses alertes sont biaisées par le son.\n\n### Étape 1 : Téléchargez et démarrez OpenSesame\n\nOpenSesame est disponible pour Windows, Linux, Mac OS et Android (runtime uniquement). Ce tutoriel est écrit pour OpenSesame 3.1.X, et vous pouvez utiliser la version basée sur Python 2.7 (par défaut) ou Python 3.5. Vous pouvez télécharger OpenSesame ici :\n\n- %link:download%\n\nLorsque vous démarrez OpenSesame, on vous propose des expériences modèles, et (le cas échéant) une liste d'expériences récemment ouvertes (voir %FigStartUp).\n\n<notranslate>\nfigure:\n id: FigStartUp\n source: start-up.png\n caption: |\n  La fenêtre OpenSesame au démarrage.\n</notranslate>\n\nLe modèle *Extended* fournit un bon point de départ pour créer des expériences basées sur Android. Cependant, dans ce tutoriel, nous créerons l'ensemble de l'expérience à partir de zéro. Par conséquent, nous continuerons avec le modèle 'par défaut', qui est déjà chargé lorsque OpenSesame est lancé (%FigDefaultTemplate). Fermez simplement les onglets 'Get started!' et (si affiché) 'Welcome!'.\n\n<notranslate>\nfigure:\n id: FigDefaultTemplate\n source: default-template.png\n caption: |\n  La structure du modèle \"par défaut\" telle qu'elle apparaît dans la zone de présentation.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Boîte d'information 1 : Bases**\n\nLes expériences OpenSesame sont des collections d'éléments. Un élément est un petit morceau de fonctionnalité qui, par exemple, peut être utilisé pour présenter des stimuli visuels (l'élément SKETCHPAD) ou pour enregistrer les touches de clavier (l'élément KEYBOARD_RESPONSE). Les éléments ont un type et un nom. Par exemple, vous pouvez avoir deux éléments de type KEYBOARD_RESPONSE avec les noms *t1_response* et *t2_response*. Pour bien distinguer les types d'éléments et les noms d'éléments, nous utiliserons CE_STYLE pour les types et *ce style* pour les noms.\n\nPour donner de la structure à votre expérience, deux types d'éléments sont particulièrement importants : la LOOP et la SEQUENCE. Comprendre comment vous pouvez combiner LOOPs et SEQUENCEs pour construire des expériences est peut-être la partie la plus délicate du travail avec OpenSesame, alors commençons par là."
  },
  "A LOOP is where, in most cases, you define your independent variables. In a LOOP you can create a table in which each column corresponds to a variable, and each row corresponds to a single run of the 'item to run'. To make this more concrete, let's consider the following *block_loop* (unrelated to this tutorial):\n\n<notranslate>\nfigure:\n id: FigLoopTable\n source: loop-table.png\n caption: |\n  An example of variables defined in a loop table. (This example is not related to the experiment created in this tutorial.)\n</notranslate>\n\nThis *block_loop* will execute *trial_sequence* four times. Once while `soa` is 100 and `target` is 'F', once while `soa` is 100 and `target` is 'H', etc. The order in which the rows are walked through is random by default, but can also be set to sequential in the top-right of the tab.\n\nA SEQUENCE consists of a series of items that are executed one after another. A prototypical SEQUENCE is the *trial_sequence*, which corresponds to a single trial. For example, a basic *trial_sequence* might consist of a SKETCHPAD, to present a stimulus, a KEYBOARD_RESPONSE, to collect a response, and a LOGGER, to write the trial information to the log file.\n\n<notranslate>\nfigure:\n id: FigExampleSequence\n source: example-sequence.png\n caption: |\n  An example of a SEQUENCE item used as a trial sequence. (This example is not related to the experiment created in this tutorial.)\n</notranslate>\n\nYou can combine LOOPs and SEQUENCEs in a hierarchical way, to create trial blocks, and practice and experimental phases. For example, the *trial_sequence* is called by the *block_loop*. Together, these correspond to a single block of trials. One level up, the *block_sequence* is called by the *practice_loop*. Together, these correspond to the practice phase of the experiment.\n\n</div>\n\n\n### Step 2: Add a block_loop and trial_sequence\n\nThe default template starts with three items: A NOTEPAD called *getting_started*, a SKETCHPAD called *welcome*, and a SEQUENCE called *experiment*. We don't need *getting_started* and *welcome*, so let's remove these right away. To do so, right-click on these items and select 'Delete'. Don't remove *experiment*, because it is the entry for the experiment (i.e. the first item that is called when the experiment is started).\n\nOur experiment will have a very simple structure. At the top of the hierarchy is a LOOP, which we will call *block_loop*. The *block_loop* is the place where we will define our independent variables (see also Background box 1). To add a LOOP to your experiment, drag the LOOP icon from the item toolbar onto the *experiment* item in the overview area.\n\nA LOOP item needs another item to run; usually, and in this case as well, this is a SEQUENCE. Drag the SEQUENCE item from the item toolbar onto the *new_loop* item in the overview area. OpenSesame will ask whether you want to insert the SEQUENCE into or after the LOOP. Select 'Insert into new_loop'.\n\nBy default, items have names such as *new_sequence*, *new_loop*, *new_sequence_2*, etc. These names are not very informative, and it is good practice to rename them. Item names must consist of alphanumeric characters and/ or underscores. To rename an item, double-click on the item in the overview area. Rename *new_sequence* to *trial_sequence* to indicate that it will correspond to a single trial. Rename *new_loop* to *block_loop* to indicate that will correspond to a block of trials.\n\nThe overview area of our experiment now looks as in %FigStep3.\n\n<notranslate>\nfigure:\n id: FigStep3\n source: step3.png\n caption: |\n  The overview area at the end of Step 2.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Background box 3: Unused items**\n\n__Tip__ — Deleted items are still available in the Unused Items bin, until you select 'Permanently delete unused items' in the Unused Items tab. You can re-add deleted items to your experiment by dragging them out of the Unused Items bin into a SEQUENCE or LOOP.\n\n</div>": {
    "fr": "Une LOOP est l'endroit où, dans la plupart des cas, vous définissez vos variables indépendantes. Dans une LOOP, vous pouvez créer un tableau dans lequel chaque colonne correspond à une variable et chaque ligne correspond à une seule exécution de l'élément à exécuter. Pour rendre cela plus concret, considérons le *block_loop* suivant (sans rapport avec ce tutoriel) :\n\n<notranslate>\nfigure :\n id : FigLoopTable\n source : loop-table.png\n légende : |\n  Exemple de variables définies dans un tableau de boucle. (Cet exemple n'est pas lié à l'expérience créée dans ce tutoriel. )\n</notranslate>\n\nCe *block_loop* exécutera *trial_sequence* quatre fois. Une fois quand `soa` est 100 et `target` est 'F', une fois quand `soa` est 100 et `target` est 'H', etc. L'ordre dans lequel les lignes sont parcourues est aléatoire par défaut, mais peut aussi être réglé en séquence dans le coin supérieur droit de l'onglet.\n\nUne SEQUENCE se compose d'une série d'éléments qui sont exécutés les uns après les autres. Une SEQUENCE prototypique est le *trial_sequence*, qui correspond à un seul essai. Par exemple, une *trial_sequence* basique pourrait consister en un SKETCHPAD, pour présenter un stimulus, un KEYBOARD_RESPONSE, pour recueillir une réponse, et un LOGGER, pour écrire les informations de l'essai dans le fichier journal.\n\n<notranslate>\nfigure:\n id: FigExampleSequence\n source: example-sequence.png\n caption: |\n  Un exemple d'un élément SEQUENCE utilisé comme séquence d'essai. (Cet exemple n'est pas lié à l'expérience créée dans ce tutoriel. )\n</notranslate>\n\nVous pouvez combiner les LOOPs et les SEQUENCEs de manière hiérarchique, pour créer des blocs d'essais et des phases de pratique et expérimentales. Par exemple, le *trial_sequence* est appelé par le *block_loop*. Ensemble, ceux-ci correspondent à un seul bloc d'essais. Un niveau plus haut, le *block_sequence* est appelé par le *practice_loop*. Ensemble, il s'agit de la phase de pratique de l'expérience.\n\n</div>\n\n\n### Étape 2 : Ajouter un block_loop et trial_sequence\n\nLe modèle par défaut commence avec trois éléments : un NOTEPAD appelé *getting_started*, un SKETCHPAD appelé *welcome* et une SEQUENCE appelée *experiment*. Nous n'avons pas besoin de *getting_started* et de *welcome*, alors supprimons-les tout de suite. Pour ce faire, faites un clic droit sur ces éléments et sélectionnez \"Supprimer\". Ne supprimez pas *experiment*, car il s'agit de l'entrée pour l'expérience (c'est-à-dire du premier élément appelé lorsque l'expérience est lancée).\n\nNotre expérience aura une structure très simple. Au sommet de la hiérarchie se trouve une LOOP, que nous appellerons *block_loop*. La *block_loop* est l'endroit où nous définirons nos variables indépendantes (voir également l'encadré 1 sur les antécédents). Pour ajouter une LOOP à votre expérience, faites glisser l'icône LOOP de la barre d'outils des éléments sur l'élément *experiment* de la zone d'aperçu.\n\nUn élément LOOP a besoin d'un autre élément pour fonctionner ; généralement, et dans ce cas également, il s'agit d'une SEQUENCE. Faites glisser l'élément SEQUENCE de la barre d'outils des éléments sur l'élément *new_loop* de la zone d'aperçu. OpenSesame vous demandera si vous souhaitez insérer la SEQUENCE dans ou après la LOOP. Sélectionnez \"Insérer dans new_loop\".\n\nPar défaut, les éléments ont des noms tels que *new_sequence*, *new_loop*, *new_sequence_2*, etc. Ces noms ne sont pas très informatifs et il est recommandé de les renommer. Les noms des éléments doivent être composés de caractères alphanumériques et/ou de traits de soulignement. Pour renommer un élément, double-cliquez dessus dans la zone d'aperçu. Renommez *new_sequence* en *trial_sequence* pour indiquer qu'il correspondra à un seul essai. Renommez *new_loop* en *block_loop* pour indiquer qu'il correspondra à un bloc d'essais.\n\nLa zone d'aperçu de notre expérience ressemble maintenant à celle de %FigStep3.\n\n<notranslate>\nfigure :\n id : FigStep3\n source : step3.png\n légende : |\n  La zone d'aperçu à la fin de l'étape 2.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Encadré 3 : Éléments inutilisés**\n\n__Tip__ — Les éléments supprimés sont toujours disponibles dans la corbeille des éléments inutilisés, jusqu'à ce que vous sélectionnez \"Supprimer définitivement les éléments inutilisés\" dans l'onglet des éléments inutilisés. Vous pouvez réajouter les éléments supprimés à votre expérience en les faisant glisser hors de la corbeille des éléments inutilisés dans une SEQUENCE ou une LOOP.\n\n</div>"
  },
  "### Step 3: Import images and sound files\n\nFor this experiment, we will use images of cats, dogs, and capybaras. We will also use sound samples of meows and barks. You can download all the required files from here:\n\n- %static:attachments/cats-dogs-capybaras/stimuli.zip%\n\nDownload `stimuli.zip` and extract it somewhere (to your desktop, for example). Next, in OpenSesame, click on the 'Show file pool' button in the main toolbar (or: Menu →View → Show file pool). This will show the file pool, by default on the right side of the window. The easiest way to add the stimuli to the file pool is by dragging them from the desktop (or wherever you have extracted the files to) into the file pool. Alternatively, you can click on the '+' button in the file pool and add files using the file-selection dialog that appears. The file pool will automatically be saved with your experiment.\n\nAfter you have added all stimuli, your file pool looks as in %FigStep4.\n\n<notranslate>\nfigure:\n id: FigStep4\n source: step4.png\n caption: |\n  The file pool at the end of Step 3.\n</notranslate>\n\n### Step 4: Define the experimental variables in the block_loop\n\nConceptually, our experiment has a fully crossed 3×2 design: We have three types of visual stimuli (cats, dogs, and capybaras) which occur in combination with two types of auditory stimuli (meows and barks). However, we have five exemplars for each stimulus type: five meow sounds, five capybara pictures, etc. From a technical point of view, it therefore makes sense to treat our experiment as a 5×5×3×2 design, in which picture number and sound number are factors with five levels.\n\nOpenSesame is very good at generating full-factorial designs. First, open *block_loop* by clicking on it in the overview area. Next, click on the Full-Factorial Design button. This will open a wizard for generating full-factorial designs, which works in a straightforward way: Every column corresponds to an experimental variable (i.e. a factor). The first row is the name of the variable, the rows below contain all possible values (i.e. levels). In our case, we can specify our 5×5×3×2 design as shown in %FigLoopWizard.\n\n<notranslate>\nfigure:\n id: FigLoopWizard\n source: loop-wizard.png\n caption: |\n  The loop wizard generates full-factorial designs.\n</notranslate>\n\nAfter clicking 'Ok', you will see that there is now a LOOP table with four rows, one for each experimental variable. There are 150 cycles (=5×5×3×2), which means that we have 150 unique trials. Your LOOP table now looks as in %FigStep5.\n\n<notranslate>\nfigure:\n id: FigStep5\n source: step5.png\n caption: |\n  The LOOP table at the end of Step 4.\n</notranslate>\n\n### Step 5: Add items to the trial sequence\n\nOpen *trial_sequence*, which is still empty. It's time to add some items! Our basic *trial_sequence* is:\n\n1. A SKETCHPAD to display a central fixation dot for 500 ms\n2. A SAMPLER to play an animal sound\n3. A SKETCHPAD to display an animal picture\n4. A KEYBOARD_RESPONSE to collect a response\n5. A LOGGER to write the data to file\n\nTo add these items, simply drag them one by one from the item toolbar into the *trial_sequence*. If you accidentally drop items in the wrong place, you can simply re-order them by dragging and dropping. Once all items are in the correct order, give each of them a sensible name. The overview area now looks as in %FigStep6.\n\n<notranslate>\nfigure:\n id: FigStep6\n source: step6.png\n caption: |\n  The overview area at the end of Step 5.\n</notranslate>\n\n### Step 6: Define the central fixation dot\n\nClick on *fixation_dot* in the overview area. This opens a basic drawing board that you can use to design your visual stimuli. To draw a central fixation dot, first click on the crosshair icon, and then click on the center of the display, i.e. at position (0, 0).\n\nWe also need to specify for how long the fixation dot is visible. To do so, change the duration from 'keypress' to 495 ms, in order to specify a 500 ms duration. (See Background box 4 for an explanation.)": {
    "fr": "### Étape 3 : Importer des images et des fichiers audio\n\nPour cette expérience, nous utiliserons des images de chats, de chiens et de capybaras. Nous utiliserons également des échantillons sonores de miaulements et d'aboiements. Vous pouvez télécharger tous les fichiers nécessaires ici :\n\n- %static:attachments/cats-dogs-capybaras/stimuli.zip%\n\nTéléchargez `stimuli.zip` et extrayez-le quelque part (sur votre bureau, par exemple). Ensuite, dans OpenSesame, cliquez sur le bouton \"Afficher le répertoire de fichiers\" dans la barre d'outils principale (ou : Menu → Vue → Afficher le répertoire de fichiers). Cela affichera la banque de fichiers, par défaut sur le côté droit de la fenêtre. La manière la plus simple d'ajouter les stimuli à la banque de fichiers est de les faire glisser depuis le bureau (ou l'endroit où vous avez extrait les fichiers) dans la banque de fichiers. Sinon, vous pouvez cliquer sur le bouton '+' dans la banque de fichiers et ajouter des fichiers à l'aide de la boîte de dialogue de sélection de fichiers qui apparaît. La banque de fichiers sera automatiquement enregistrée avec votre expérience.\n\nUne fois que vous avez ajouté tous les stimuli, votre banque de fichiers ressemble à celle de %FigStep4.\n\n<notranslate>\nfigure:\n id: FigStep4\n source: step4.png\n caption: |\n  La banque de fichiers à la fin de l'étape 3.\n</notranslate>\n\n### Étape 4 : Définir les variables expérimentales dans le block_loop\n\nConceptuellement, notre expérience a un plan croisé complet 3x2 : nous avons trois types de stimuli visuels (chats, chiens et capybaras) qui se produisent en combinaison avec deux types de stimuli auditifs (miaulements et aboiements). Cependant, nous avons cinq exemplaires pour chaque type de stimulus : cinq sons de miaulement, cinq images de capybara, etc. D'un point de vue technique, il est donc logique de traiter notre expérience comme un plan 5×5×3×2, dans lequel le numéro de l'image et le numéro du son sont des facteurs avec cinq niveaux.\n\nOpenSesame est très bon pour générer des plans factoriels complets. Tout d'abord, ouvrez le *block_loop* en cliquant dessus dans la zone d'aperçu. Ensuite, cliquez sur le bouton Design factoriel complet. Cela ouvrira un assistant pour générer des plans factoriels complets, qui fonctionne de manière simple : chaque colonne correspond à une variable expérimentale (c'est-à-dire un facteur). La première ligne est le nom de la variable, les lignes ci-dessous contiennent toutes les valeurs possibles (c'est-à-dire les niveaux). Dans notre cas, nous pouvons spécifier notre plan 5×5×3×2 comme indiqué dans %FigLoopWizard.\n\n<notranslate>\nfigure:\n id: FigLoopWizard\n source: loop-wizard.png\n caption: |\n  L'assistant de boucle génère des plans factoriels complets.\n</notranslate>\n\nAprès avoir cliqué sur \"Ok\", vous verrez qu'il y a maintenant une table LOOP avec quatre lignes, une pour chaque variable expérimentale. Il y a 150 cycles (=5×5×3×2), ce qui signifie que nous avons 150 essais uniques. Votre table LOOP ressemble maintenant à celle de %FigStep5.\n\n<notranslate>\nfigure:\n id: FigStep5\n source: step5.png\n caption: |\n  La table LOOP à la fin de l'étape 4.\n</notranslate>\n\n### Étape 5 : Ajouter des éléments à la séquence d'essais\n\nOuvrez *trial_sequence*, qui est encore vide. Il est temps d'ajouter des éléments ! Notre *trial_sequence* de base est :\n\n1. Un SKETCHPAD pour afficher un point de fixation central pendant 500 ms\n2. Un SAMPLER pour jouer un son d'animal\n3. Un SKETCHPAD pour afficher une image d'animal\n4. Une KEYBOARD_RESPONSE pour recueillir une réponse\n5. Un LOGGER pour enregistrer les données dans un fichier\n\nPour ajouter ces éléments, faites-les simplement glisser un par un depuis la barre d'outils des éléments vers la *trial_sequence*. Si vous déposez accidentellement des éléments au mauvais endroit, vous pouvez simplement les réorganiser en les faisant glisser et en les déposant. Une fois que tous les éléments sont dans le bon ordre, donnez à chacun d'entre eux un nom pertinent. La zone d'aperçu ressemble maintenant à celle de %FigStep6.\n\n<notranslate>\nfigure:\n id: FigStep6\n source: step6.png\n caption: |\n  La zone d'aperçu à la fin de l'étape 5.\n</notranslate>\n\n### Étape 6 : Définir le point de fixation central\n\nCliquez sur *fixation_dot* dans la zone d'aperçu. Cela ouvre un tableau de dessin de base que vous pouvez utiliser pour concevoir vos stimuli visuels. Pour dessiner un point de fixation central, cliquez d'abord sur l'icône en forme de croix, puis cliquez au centre de l'affichage, c'est-à-dire à la position (0, 0).\n\nNous devons également préciser combien de temps le point de fixation est visible. Pour ce faire, changez la durée de \"keypress\" à 495 ms, afin de spécifier une durée de 500 ms. (Voir Background box 4 pour une explication.)"
  },
  "The *fixation_dot* item now looks as in %FigStep7.\n\n<notranslate>\nfigure:\n id: FigStep7\n source: step7.png\n caption: |\n  The *fixation_dot* item at the end of Step 6.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Background box 4: Selecting the correct duration**\n\nWhy specify a duration of 495 if we want a duration of 500 ms? The reason for this is that the actual display-presentation duration is always rounded up to a value that is compatible with your monitor's refresh rate. This may sound complicated, but for most purposes the following rules of thumb are sufficient:\n\n1. Choose a duration that is possible given your monitor's refresh rate. For example, if your monitor's refresh rate is 60 Hz, this means that every frame lasts 16.7 ms (=1000 ms/60 Hz). Therefore, on a 60 Hz monitor, you should always select a duration that is a multiple of 16.7 ms, such as 16.7, 33.3, 50, 100, etc.\n2. In the duration field of the SKETCHPAD specify a duration that is a few milliseconds shorter than what you're aiming for. So if you want to present a SKETCHPAD for 50 ms, choose a duration of 45. If you want to present a SKETCHPAD for 1000 ms, choose a duration of 995. Etcetera.\n\nFor a detailed discussion of experimental timing, see:\n\n- %link:timing%\n\n</div>\n\n### Step 7: Define the animal sound\n\nOpen *animal_sound*. The SAMPLER item provides a number of options, the most important being the sound file that should be played. Click on the browse button to open the file-pool selection dialog, and select one of the sound files, such as `bark1.ogg`.\n\nOf course, we don't want to play the same sound over-and-over again! Instead, we want to select a sound based on the variables `sound` and `sound_nr` that we have defined in the *block_loop* (Step 5). To do this, simply replace the part of the string that you want to have depend on a variable by the name of that variable between square brackets. More specifically, 'bark1.ogg' becomes '[sound][sound_nr].ogg', because we want to replace 'bark' by the value of the variable `sound` and '1' by the value of `sound_nr`.\n\nWe also need to change the duration of the SAMPLER. By default, the duration is 'sound', which means that the experiment will pause while the sound is playing. Change the duration to 0. This does not mean that the sound will be played for only 0 ms, but that the experiment will advance right away to the next item, while the sound continues to play in the background. The item *animal_sound* now looks as shown in %FigStep8.\n\n<notranslate>\nfigure:\n id: FigStep8\n source: step8.png\n caption: |\n  The item *animal_sound* at the end of Step 7.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Background box 5: Variables**\n\nFor more information about using variables, see:\n\n- %link:manual/variables%\n\n</div>\n\n### Step 8: Define the animal picture\n\nOpen *animal_picture*. Select the image tool by clicking on the button with the landscape-like icon. Click on the center (0, 0) of the display. In the File Pool dialog that appears, select `capybara1.png`. The capybara's sideways glance will now lazily stare at you from the center of the display. But of course, we don't always want to show the same capybara. Instead, we want to have the image depend on the variables `animal` and `pic_nr` that we have defined in the *block_loop* (Step 5).\n\nWe can use essentially the same trick as we did for *animal_sound*, although things work slightly differently for images. First, right-click on the capybara and select 'Edit script'. This allows you to edit the following line of OpenSesame script that corresponds to the capybara picture:\n\n\tdraw image center=1 file=\"capybara1.png\" scale=1 show_if=always x=0 y=0 z_index=0\n\nNow change the name of image file from 'capybara.png' to '[animal][pic_nr].png':\n\n\tdraw image center=1 file=\"[animal][pic_nr].png\" scale=1 show_if=always x=0 y=0 z_index=0": {
    "fr": "L'élément *fixation_dot* apparaît maintenant comme dans %FigStep7.\n\n<notranslate>\nfigure:\n id: FigStep7\n source: step7.png\n caption: |\n  L'élément *fixation_dot* à la fin de l'étape 6.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Encadré 4 : Choisir la bonne durée**\n\nPourquoi spécifier une durée de 495 si nous voulons une durée de 500 ms ? La raison est que la durée réelle de présentation de l'affichage est toujours arrondie à une valeur compatible avec le taux de rafraîchissement de votre écran. Cela peut sembler compliqué, mais pour la plupart des applications, les règles de base suivantes sont suffisantes :\n\n1. Choisissez une durée possible compte tenu du taux de rafraîchissement de votre écran. Par exemple, si le taux de rafraîchissement de votre écran est de 60 Hz, cela signifie que chaque image dure 16,7 ms (=1000 ms/60 Hz). Par conséquent, sur un écran de 60 Hz, vous devez toujours choisir une durée qui est un multiple de 16,7 ms, comme 16,7, 33,3, 50, 100, etc.\n2. Dans le champ de durée du SKETCHPAD, spécifiez une durée de quelques millisecondes plus courte que celle que vous visez. Ainsi, si vous voulez présenter un SKETCHPAD pendant 50 ms, choisissez une durée de 45. Si vous voulez présenter un SKETCHPAD pendant 1000 ms, choisissez une durée de 995. Etc.\n\nPour une discussion détaillée sur le chronométrage expérimental, voir :\n\n- %link:timing%\n\n</div>\n\n### Étape 7 : Définir le son de l'animal\n\nOuvrez *animal_sound*. L'élément SAMPLER offre un certain nombre d'options, la plus importante étant le fichier son à jouer. Cliquez sur le bouton de navigation pour ouvrir le sélecteur de fichiers et sélectionnez l'un des fichiers audio, comme `bark1.ogg`.\n\nBien évidemment, nous ne voulons pas jouer le même son encore et encore! Au lieu de cela, nous souhaitons sélectionner un son en fonction des variables `sound` et `sound_nr` que nous avons définies dans le *block_loop* (étape 5). Pour ce faire, remplacez simplement la partie de la chaîne de caractères que vous souhaitez dépendre d'une variable par le nom de cette variable entre crochets. Plus précisément, 'bark1.ogg' devient '[sound][sound_nr].ogg', car nous voulons remplacer 'bark' par la valeur de la variable `sound` et '1' par la valeur de `sound_nr`.\n\nNous devons également changer la durée de l'échantillonneur. Par défaut, la durée est 'sound', ce qui signifie que l'expérience est en pause pendant la lecture du son. Modifiez la durée pour 0. Cela ne signifie pas que le son sera joué pendant seulement 0 ms, mais que l'expérience passera immédiatement à l'élément suivant, pendant que le son continue de jouer en arrière-plan. L'élément *animal_sound* ressemble maintenant à celui présenté dans %FigStep8.\n\n<notranslate>\nfigure:\n id: FigStep8\n source: step8.png\n caption: |\n  L'élément *animal_sound* à la fin de l'étape 7.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Encadré 5 : Variables**\n\nPour plus d'informations sur l'utilisation des variables, consultez :\n\n- %link:manual/variables%\n\n</div>\n\n### Étape 8 : Définir l'image de l'animal\n\nOuvrez *animal_picture*. Sélectionnez l'outil image en cliquant sur le bouton avec l'icône de paysage. Cliquez au centre (0, 0) de l'affichage. Dans la boîte de dialogue File Pool qui apparaît, sélectionnez `capybara1.png`. Le regard de côté du capybara vous observera paresseusement au centre de l'affichage. Mais bien sûr, nous ne voulons pas toujours montrer le même capybara. Au lieu de cela, nous voulons que l'image dépende des variables `animal` et `pic_nr` que nous avons définies dans le *block_loop* (étape 5).\n\nNous pouvons utiliser essentiellement la même astuce que pour *animal_sound*, bien que les choses fonctionnement légèrement différemment pour les images. Tout d'abord, faites un clic droit sur le capybara et sélectionnez \"Modifier le script\". Cela vous permet de modifier la ligne de script OpenSesame correspondant à l'image du capybara :\n\n\tdraw image center=1 file=\"capybara1.png\" scale=1 show_if=always x=0 y=0 z_index=0\n\nMaintenant, changez le nom du fichier image de 'capybara.png' en '[animal][pic_nr].png' :\n\n\tdraw image center=1 file=\"[animal][pic_nr].png\" scale=1 show_if=always x=0 y=0 z_index=0"
  },
  "Click on 'Ok' to apply the change. The capybara is now gone, replaced by a placeholder image, and OpenSesame tells you that one object is not shown, because it is defined using variables. Don't worry, it will be shown during the experiment!\n\nTo remind the participant of the task, also add two response circles, one marked 'dog' on the left side of the screen, and one marked 'cat' on the right side. I'm sure you will able to figure out how to do this with the SKETCHPAD drawing tools. My rendition is shown in %FigStep9. Note that these response circles are purely visual, and we still need to explicitly define the response criteria (see Step 10).\n\nFinally, set 'Duration' field to '0'. This does not mean that the picture is presented for only 0 ms, but that the experiment will advance to the next item (*response*) right away. Since *response* waits for a response, but doesn't change what's on the screen, the target will remain visible until a response has been given.\n\n<notranslate>\nfigure:\n id: FigStep9\n source: step9.png\n caption: |\n  The *animal_picture* SKETCHPAD at the end of Step 8.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Background box 6: Image formats**\n\n__Tip__ -- OpenSesame can handle a wide variety of image formats. However, some (non-standard) `.bmp` formats are known to cause trouble. If you find that a `.bmp` image is not shown, you may want to consider using a different format, such as `.png`. You can convert images easily with free tools such as [GIMP].\n\n</div>\n\n\n### Step 9: Define the response\n\nOpen the *response* item. This is a KEYBOARD_RESPONSE item, which collects a single key press. There are a few options:\n\n- __Correct response__ — let's skip this for now; we'll get back to it in Step 10.\n- __Allowed responses__ is a semicolon-separated list of keys that are accepted. Let's set this to *left;right* to indicate that only the left and right arrow keys are accepted. (The *escape* key pauses the experiment, and is always accepted!)\n- __Timeout__ indicates a duration after which the response will be set to 'None', and the experiment will continue. A timeout is important in our experiment, because participants need to have the opportunity to *not* respond when they see a capybara. So let's set the timeout to 2000.\n- __Flush pending keypresses__ indicates that we should only accept new key presses. This is best left enabled (it is by default).\n\n<notranslate>\nfigure:\n id: FigStep10\n source: step10.png\n caption: |\n  The *response* KEYBOARD_RESPONSE at the end of Step 9.\n</notranslate>\n\n\n### Step 10: Define the correct response\n\nSo far, we haven't defined the correct response for each trial. This is done by defining a `correct_response` variable. You can do this either by creating a `correct_response` column in a LOOP table (here the *block_loop*) and entering the correct responses manually, or by specifying the correct response in a Python INLINE_SCRIPT item—which is what we will do here.\n\nFirst, drag an INLINE_SCRIPT item from the item toolbar and insert it at the top of the *trial_sequence*. (Don't forget to give it a sensible name!) You now see a text editor with two tabs: a *Run* tab, and a *Prepare* tab. You can enter Python code in both tabs, but this code is executed during different phases of the experiment. The *Prepare* phase is executed first whenever a SEQUENCE is executed; this gives all items in the SEQUENCE a chance to perform time consuming operations that could otherwise slow down the experiment at time-sensitive moments. Next, the *Run* phase is executed; this is where the action happens, such as showing a display, collecting a response, etc.\n\nFor more information, see:\n\n- %link:prepare-run%\n\nDefining a correct response is a clear example of something that should be done in the *Prepare* phase. The following script will do the trick:": {
    "fr": "Cliquez sur \"Ok\" pour appliquer le changement. Le capybara a disparu, remplacé par une image fantôme, et OpenSesame vous indique qu'un objet n'est pas affiché car il est défini à l'aide de variables. Ne vous inquiétez pas, il sera affiché pendant l'expérience !\n\nPour rappeler la tâche au participant, ajoutez également deux cercles de réponse, un marqué \"chien\" sur le côté gauche de l'écran et un marqué \"chat\" sur le côté droit. Je suis sûr que vous saurez comment faire cela avec les outils de dessin SKETCHPAD. Ma version est présentée dans %FigStep9. Notez que ces cercles de réponse sont purement visuels, et que nous devons encore définir explicitement les critères de réponse (voir l'étape 10).\n\nEnfin, définissez le champ 'Durée' sur '0'. Cela ne signifie pas que l'image est présentée pendant seulement 0 ms, mais que l'expérience passera à l'objet suivant (*réponse*) immédiatement. Comme *réponse* attend une réponse, mais ne change pas ce qui est affiché à l'écran, la cible restera visible jusqu'à ce qu'une réponse soit donnée.\n\n<notranslate>\nfigure:\n id: FigStep9\n source : step9.png\n caption : |\n  Le SKETCHPAD *animal_picture* à la fin de l'étape 8.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Background box 6: Formats d'image**\n\n__Astuce__ -- OpenSesame peut gérer une grande variété de formats d'image. Cependant, certains formats `.bmp` (non standard) sont connus pour poser problème. Si vous constatez qu'une image `.bmp` n'est pas affichée, vous pouvez envisager d'utiliser un format différent, tel que `.png`. Vous pouvez convertir les images facilement avec des outils gratuits tels que [GIMP].\n\n</div>\n\n\n### Étape 9: Définir la réponse\n\nOuvrez l'objet *réponse*. Il s'agit d'un élément KEYBOARD_RESPONSE, qui collecte une seule pression de touche. Il y a quelques options :\n\n- __Réponse correcte__ — passons cela pour le moment ; nous y reviendrons à l'étape 10.\n- __Réponses autorisées__ est une liste de touches séparées par des points-virgules qui sont acceptées. Définissons-le sur *left;right* pour indiquer que seules les touches fléchées gauche et droite sont acceptées. (La touche *escape* met en pause l'expérience et est toujours acceptée !)\n- __Durée__ indique une durée après laquelle la réponse sera définie sur 'Aucune', et l'expérience se poursuivra. Une durée est importante dans notre expérience, car les participants doivent avoir la possibilité de *ne pas* répondre lorsqu'ils voient un capybara. Fixons donc la durée à 2000.\n- __Vider les pressions de touches en attente__ indique que nous devrions seulement accepter les nouvelles pressions de touches. Il est préférable de le laisser activé (il l'est par défaut).\n\n<notranslate>\nfigure:\n id: FigStep10\n source: step10.png\n caption: |\n  Le KEYBOARD_RESPONSE *réponse* à la fin de l'étape 9.\n</notranslate>\n\n\n### Étape 10: Définir la réponse correcte\n\nJusqu'à présent, nous n'avons pas défini de réponse correcte pour chaque essai. Cela se fait en définissant une variable `correct_response`. Vous pouvez le faire soit en créant une colonne `correct_response` dans un tableau LOOP (ici le *block_loop*) et en entrant les réponses correctes manuellement, soit en spécifiant la réponse correcte dans un élément INLINE_SCRIPT Python, ce que nous ferons ici.\n\nTout d'abord, faites glisser un élément INLINE_SCRIPT depuis la barre d'outils des éléments et insérez-le au début de la *trial_sequence*. (N'oubliez pas de lui donner un nom significatif!) Vous voyez maintenant un éditeur de texte avec deux onglets : un onglet *Run* et un onglet *Prepare*. Vous pouvez saisir du code Python dans les deux onglets, mais ce code est exécuté à différentes phases de l'expérience. La phase *Prepare* est exécutée en premier chaque fois qu'une SEQUENCE est exécutée ; cela donne à tous les éléments de la SEQUENCE la possibilité d'effectuer des opérations longues qui pourraient sinon ralentir l'expérience lors de moments sensibles au temps. Ensuite, la phase *Run* est exécutée ; c'est là que se passe l'action, comme montrer un affichage, collecter une réponse, etc.\n\nPour plus d'informations, voir :\n\n- %link:prepare-run %\n\nDéfinir une réponse correcte est un exemple clair de quelque chose qui doit être fait lors de la phase *Prepare*. Le script suivant fera l'affaire:"
  },
  "~~~ .python\nif var.animal == 'dog':\n\tvar.correct_response = 'left'\nelif var.animal == 'cat':\n\tvar.correct_response = 'right'\nelif var.animal == 'capybara':\n\tvar.correct_response = None # A timeout is coded as None!\nelse:\n\traise ValueError('Invalid animal: %s' % var.animal)\n~~~\n\nThis code is almost plain English, but a few pointers may be useful:\n\n- In Python script, experimental variables are not referred to using square brackets (`[my_variable]`), as they are elsewhere in OpenSesame, but as properties of the `var` object (i.e. `var.my_variable`).\n- We also consider the possibility that the animal is neither a dog, a cat, nor a capybara. Of course this should never happen, but by taking this possibility into account, we protect ourselves against typos and other bugs. This is called 'defensive programming'.\n\n\n### Step 11: Define the logger\n\nWe don't need to configure the LOGGER, because its default settings are fine; but let's take a look at it anyway. Click on *logger* in the overview area to open it. You see that the option 'Log all variables (recommended)' is selected. This means that OpenSesame logs everything, which is fine.\n\n<div class='info-box' markdown='1'>\n\n**Background box 8: Always check your data!**\n\n__The one tip to rule them all__ — Always triple-check whether all necessary variables are logged in your experiment! The best way to check this is to run the experiment and investigate the resulting log files.\n\n</div>\n\n### Step 12: Add per-trial feedback\n\nIt is good practice to inform the participant of whether the response was correct or not. To avoid disrupting the flow of the experiment, this type of immediate feedback should be as unobtrusive as possible. Here, we will do this by briefly showing a green fixation dot after a correct response, and a red fixation dot after an incorrect response.\n\nFirst, add two new SKETCHPADs to the end of the *trial_sequence*. Rename the first one to *feedback_correct* and the second one to *feedback_incorrect*. Of course, we want to run only one of these items on any given trial, depending on whether or not the response was correct. To do this, we can make use of the built-in variable `correct`, which has the value 0 after an incorrect response, and 1 after a correct response. (Provided that we have defined `correct_response`, which we did in Step 11.) To tell the *trial_sequence* that the *feedback_correct* item should be called only when the response is correct, we use the following run-if statement:\n\n\t[correct] = 1\n\nThe square brackets around `correct` indicate that this is the name of a variable, and not simply the string 'correct'. Analogously, we use the following run-if statement for the *feedback_incorrect* item:\n\n\t[correct] = 0\n\nWe still need to give content to the *feedback_correct* and *feedback_incorrect* items. To do this, simply open the items and draw a green or red fixation dot in the center. Also, don't forget to change the durations from 'keypress' to some brief interval, such as 195.\n\nThe *trial_sequence* now looks as shown in %FigStep13.\n\n<notranslate>\nfigure:\n id: FigStep13\n source: step13.png\n caption: |\n  The *trial_sequence* at the end of Step 12.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Background box 9: Conditional statements**\n\nFor more information about conditional 'if' statements, see:\n\n- %link:manual/variables%\n\n</div>\n\n### Step 13: Add instructions and goodbye screens\n\nA good experiment always start with an instruction screen, and ends by thanking the participant for his or her time. The easiest way to do this in OpenSesame is with `form_text_display` items.\n\nDrag two `form_text_display`s into the main *experiment* SEQUENCE. One should be at the very start, and renamed to *form_instructions*. The other should be at the very end, and renamed to *form_finished*. Now simply add some appropriate text to these forms, for example as shown in %FigStep14.": {
    "fr": "~~~ .python\nif var.animal == 'chien':\n\tvar.correct_response = 'gauche'\nelif var.animal == 'chat':\n\tvar.correct_response = 'droite'\nelif var.animal == 'capybara':\n\tvar.correct_response = None # Un délai d'attente est codé comme None!\nelse:\n\traise ValueError('Animal invalide : %s' % var.animal)\n~~~\n\nCe code est presque en anglais courant, mais quelques conseils peuvent être utiles :\n\n- Dans le script Python, les variables expérimentales ne sont pas référencées en utilisant des crochets (`[my_variable]`), comme elles le sont ailleurs dans OpenSesame, mais en tant que propriétés de l'objet `var` (c'est-à-dire `var.my_variable`).\n- Nous prenons également en compte la possibilité que l'animal ne soit ni un chien, ni un chat, ni un capybara. Bien sûr, cela ne devrait jamais arriver, mais en prenant cette possibilité en compte, nous nous protégeons contre les fautes de frappe et autres bugs. Ceci s'appelle la 'programmation défensive'.\n\n\n### Étape 11 : Définir le logger\n\nNous n'avons pas besoin de configurer le LOGGER, car ses paramètres par défaut conviennent; mais regardons-le quand même. Cliquez sur *logger* dans la zone d'aperçu pour l'ouvrir. Vous voyez que l'option 'Log all variables (recommended)' est sélectionnée. Cela signifie qu'OpenSesame enregistre tout, ce qui est bien.\n\n<div class='info-box' markdown='1'>\n\n**Encadré 8 : Vérifiez toujours vos données !**\n\n__Le conseil ultime__ — Vérifiez toujours et encore si toutes les variables nécessaires sont enregistrées dans votre expérience! Le meilleur moyen de vérifier cela est de lancer l'expérience et d'examiner les fichiers journaux résultants.\n\n</div>\n\n### Étape 12: Ajouter un retour d'information par essai\n\nIl est bon de prévenir le participant si la réponse était correcte ou non. Pour éviter de perturber le déroulement de l'expérience, ce type de retour d'information immédiat doit être aussi discret que possible. Ici, nous le ferons en affichant brièvement un point de fixation vert après une réponse correcte, et un point de fixation rouge après une réponse incorrecte.\n\nTout d'abord, ajoutez deux nouveaux SKETCHPADs à la fin de la *trial_sequence*. Renommez le premier en *feedback_correct* et le second en *feedback_incorrect*. Bien sûr, nous voulons exécuter seulement l'un de ces éléments lors d'un essai donné, en fonction de la réponse correcte ou non. Pour ce faire, nous pouvons utiliser la variable intégrée `correct`, qui a la valeur 0 après une réponse incorrecte et 1 après une réponse correcte. (À condition d'avoir défini `correct_response`, ce que nous avons fait à l'étape 11.) Pour indiquer à la *trial_sequence* que l'élément *feedback_correct* doit être appelé uniquement lorsque la réponse est correcte, nous utilisons la déclaration run-if suivante :\n\n\t[correct] = 1\n\nLes crochets autour de `correct` indiquent qu'il s'agit du nom d'une variable, et non simplement de la chaîne 'correct'. De manière analogue, nous utilisons la déclaration run-if suivante pour l'élément *feedback_incorrect*:\n\n\t[correct] = 0\n\nIl nous reste à donner du contenu aux éléments *feedback_correct* et *feedback_incorrect*. Pour ce faire, ouvrez simplement les éléments et dessinez un point de fixation vert ou rouge au centre. N'oubliez pas non plus de changer les durées de 'keypress' à un intervalle court, comme 195.\n\nLe *trial_sequence* apparaît maintenant comme indiqué dans %FigStep13.\n\n<notranslate>\nfigure:\n id: FigStep13\n source: step13.png\n caption: |\n  Le *trial_sequence* à la fin de l'étape 12.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Encadré 9 : Déclarations conditionnelles**\n\nPour plus d'informations sur les déclarations conditionnelles 'if', voir :\n\n- %link:manual/variables%\n\n</div>\n\n### Étape 13: Ajouter des instructions et des écrans d'au revoir\n\nUne bonne expérience commence toujours par un écran d'instructions et se termine en remerciant le participant pour le temps qu'il a consacré. La manière la plus simple de le faire dans OpenSesame est avec des éléments `form_text_display`.\n\nFaites glisser deux `form_text_display`s dans la SEQUENCE principale *experiment*. L'un doit être au tout début, et renommé en *form_instructions*. L'autre doit être à la toute fin, et renommé en *form_finished*. Maintenant, ajoutez simplement du texte approprié à ces formulaires, par exemple comme indiqué dans %FigStep14."
  },
  "<notranslate>\nfigure:\n id: FigStep14\n source: step14.png\n caption: |\n  The *form_instructions* item at the end of Step 13.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Background box 10: Text**\n\n__Tip__ -- Forms, and text more generally, support a subset of HTML tags to allow for text formatting (i.e. colors, boldface, etc.). This is described here:\n\n- %link:visual%\n\n</div>\n\n### Step 15: Finished!\n\nYour experiment is now finished! Click on the 'Run fullscreen' (`Control+R`) button in the main toolbar to give it a test run. You can also upload the experiment to OSWeb (<http://osweb.cogsci.nl/>) and run it online!\n\n<div class='info-box' markdown='1'>\n\n**Background box 11: Quick run**\n\n__Tip__ — A test run is executed even faster by clicking the orange 'Run in window' button, which doesn't ask you how to save the logfile (and should therefore only be used for testing purposes).\n\n</div>\n\n\n## Extra assignments\n\nThe solutions to these extra assingments can be found in the [experiment file](http://osf.io/jw7dr).\n\n### Extra 1: Add an instruction and goodbye screen\n\nTips:\n\n- SKETCHPAD and FORM_TEXT_DISPLAY can present text\n- Good instructions are brief and concrete\n\n### Extra 2: Analyze the data\n\nTips:\n\n- Run the experiment once on yourself\n- Open the data file in Excel, LibreOffice, or JASP\n\n### Extra 3: Divide the trials into multiple blocks\n\nTips:\n\n- Use a break-if statement to break the loop after (say) 15 trials: `([count_trial_sequence]+1) % 15 = 0`\n- Add a new LOOP-SEQUENCE structure above the *block_loop* to repeat a block of trials multiple times\n- Disable the 'Evaluate on first cycle' option in the *block_loop* so that the break-if statement isn't evaluated when the `count_trial_sequence` variable doesn't yet exist\n- Enable the 'Resume after break' option in the *block_loop* to randomly sample without replacement from the LOOP table\n\n### Extra 4: Add accuracy and average response time feedback after every block\n\nFirst do Extra 3!\n\nTips:\n\n- Use a FEEDBACK item to provide feedback\n- The variables `acc` and `avg_rt` contain the running accuracy and average reaction time\n\n### Extra 5: Counterbalance the response rule\n\nTips:\n\n- The variable `subject_parity` is 'even' or 'odd'\n- This requires a simple INLINE_SCRIPT\n- Make sure that the instructions match the response rule!\n\n\n## References\n\nMathôt, S., Schreij, D., & Theeuwes, J. (2012). OpenSesame: An open-source, graphical experiment builder for the social sciences. *Behavior Research Methods*, *44*(2), 314–324. doi:10.3758/s13428-011-0168-7\n{: .reference}\n\n[OpenSesame runtime for Android]: /getting-opensesame/android\n[slides]: /attachments/rovereto2014-workshop-slides.pdf\n[modulo]: http://en.wikipedia.org/wiki/Modulo_operation\n[pdf]: /rovereto2014/index.pdf\n[gimp]: http://www.gimp.org/\n[capybara_photo]: https://commons.wikimedia.org/wiki/File:Capybara_Hattiesburg_Zoo_(70909b-58)_2560x1600.jpg\n": {
    "fr": "<notranslate>\nfigure:\n id: FigStep14\n source: step14.png\n caption: |\n  L'élément *form_instructions* à la fin de l'étape 13.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Encadré 10 : Texte**\n\n__Astuce__ -- Les formulaires, et plus généralement le texte, prennent en charge un sous-ensemble de balises HTML pour permettre la mise en forme du texte (c'est-à-dire les couleurs, le gras, etc.). Ceci est décrit ici :\n\n- %link:visual%\n\n</div>\n\n### Étape 15 : Terminé !\n\nVotre expérience est maintenant terminée ! Cliquez sur le bouton \"Run fullscreen\" (`Control+R`) dans la barre d'outils principale pour faire un essai. Vous pouvez également télécharger l'expérience sur OSWeb (<http://osweb.cogsci.nl/>) et la lancer en ligne !\n\n<div class='info-box' markdown='1'>\n\n**Encadré 11 : Exécution rapide**\n\n__Astuce__ — Une exécution de test est encore plus rapide en cliquant sur le bouton orange \"Exécuter dans une fenêtre\", qui ne vous demande pas comment enregistrer le fichier de journalisation (et ne doit donc être utilisé qu'à des fins de test).\n\n</div>\n\n## Travaux pratiques supplémentaires\n\nLes solutions à ces travaux pratiques supplémentaires se trouvent dans le [fichier d'expérience](http://osf.io/jw7dr).\n\n### Supplément 1 : Ajouter un écran d'instructions et d'au revoir\n\nConseils :\n\n- SKETCHPAD et FORM_TEXT_DISPLAY peuvent présenter du texte\n- De bonnes instructions sont brèves et concrètes\n\n### Supplément 2 : Analyser les données\n\nConseils :\n\n- Lancez l'expérience une fois sur vous-même\n- Ouvrez le fichier de données dans Excel, LibreOffice ou JASP\n\n### Supplément 3 : Diviser les essais en plusieurs blocs\n\nConseils :\n\n- Utilisez une instruction break-if pour interrompre la boucle après (disons) 15 essais : `([count_trial_sequence]+1) % 15 = 0`\n- Ajoutez une nouvelle structure LOOP-SEQUENCE au-dessus de la *boucle de bloc* pour répéter un bloc d'essais plusieurs fois\n- Désactivez l'option \"Évaluer lors du premier cycle\" dans la *boucle de bloc* pour que l'instruction break-if ne soit pas évaluée lorsque la variable `count_trial_sequence` n'existe pas encore\n- Activez l'option \"Reprendre après la pause\" dans la *boucle de bloc* pour échantillonner aléatoirement sans remplacement à partir de la table LOOP\n\n### Supplément 4 : Ajouter des commentaires sur la précision et le temps de réponse moyen après chaque bloc\n\nFaites d'abord Supplément 3 !\n\nConseils :\n\n- Utilisez un élément FEEDBACK pour fournir des commentaires\n- Les variables `acc` et `avg_rt` contiennent la précision et le temps de réaction moyen en cours\n\n### Supplément 5 : Contrebalancer la règle de réponse\n\nConseils :\n\n- La variable `subject_parity` est \"pair\" ou \"impair\"\n- Cela nécessite un simple INLINE_SCRIPT\n- Assurez-vous que les instructions correspondent à la règle de réponse !\n\n## Références\n\nMathôt, S., Schreij, D., & Theeuwes, J. (2012). OpenSesame: An open-source, graphical experiment builder for the social sciences. *Behavior Research Methods*, *44*(2), 314-324. doi:10.3758/s13428-011-0168-7\n{: .reference}\n\n[OpenSesame runtime for Android]: /getting-opensesame/android\n[diapositives]: /attachments/rovereto2014-workshop-slides.pdf\n[modulo]: http://fr.wikipedia.org/wiki/Op%C3%A9ration_modulo\n[pdf]: /rovereto2014/index.pdf\n[gimp]: http://www.gimp.org/\n[capybara_photo]: https://commons.wikimedia.org/wiki/File:Capybara_Hattiesburg_Zoo_(70909b-58)_2560x1600.jpg"
  },
  "Meet the team!": {
    "fr": "Rencontrez l'équipe !"
  },
  "OpenSesame is mainly developed by a loose collection of individuals. But anyone is welcome to contribute!\n\n<notranslate>[TOC]</notranslate>\n\n## [Sebastiaan Mathôt](http://www.cogsci.nl/smathot)\n\nProject manager and lead developer\n\n*Laboratoire de Psychologie Cognitive, CNRS, Aix-Marseille Université*\n\n<notranslate>\nfigure:\n source: sebastiaan.png\n id: FigSebastiaan\n caption: Sebastiaan Mathôt\n</notranslate>\n\nI'm currently a Marie Curie fellow at the Laboratoire de Pyschologie Cognitive in Marseille, France. My research focuses on the relationship between the pupillary light response, visual attention, and eye movements.\n\nOpenSesame is the more pragmatic side of my job. With this project, we aim to provide experimental psychologists and neuroscientists with a free (in every sense of the word) high quality experiment builder.\n\n---\n\n## Daniel Schreij\n\nDeveloper\n\n*Freelance software developer*\n\n<notranslate>\nfigure:\n source: daniel.png\n id: FigDaniel\n caption: Daniel Schreij\n</notranslate>\n\nI have always had a fond interest in both technology and human cognition and therefore chose to study Artificial Intelligence (AI), which is a combination of these two disciplines. During this time, I was mainly drawn towards the\n'what makes people tick' side of AI and this led me to follow a master's degree in Cognitive Science. In this area, I also graduated for my PhD Degree at the VU University of Amsterdam, where I have worked as a post-doc. During all this time, I never lost my interest in computer science and kept up to speed with the latest developments in this area. Currently, I am working as a freelance software developer for scientific projects.\n\nI am happy I can assist Sebastiaan in the development of OpenSesame, as it allows to keep doing more pragmatic tasks like software development\nwhich I really like, and at the same time lets me stay involved in psychological research.\n\n---\n\n## [Lotje van der Linden](http://www.cogsci.nl/lvanderlinden)\n\nDocumentation and support\n\n*Laboratoire de Psychologie Cognitive, Aix-Marseille Université*\n\n<notranslate>\nfigure:\n source: lotje.png\n id: FigLotje\n caption: Lotje van der Linden\n</notranslate>\n\nI'm working as a PhD student under supervision of Françoise Vitu at the Laboratoire Psychologie Cognitive at the Aix-Marseille Université. My PhD project is on whether affordances (possibilities for action in our environment) influence how and where we move our eyes.\n\nI really enjoy being involved in the OpenSesame project, from helping with the documentation to offering support on the forum. So don't hesitate to post any questions!\n\n---\n\n## [Edwin Dalmaijer](http://www.pygaze.org/esdalmaijer/)\n\nDeveloper\n\n*Department of Experimental Psychology, Utrecht University*\n\n<notranslate>\nfigure:\n source: edwin.png\n id: FigEdwin\n caption: Edwin Dalmaijer\n</notranslate>\n\nI'm currently a PhD student at Oxford University. For the OpenSesame project, I maintain the affiliated PyGaze package for eye tracking.\n\nBeing young and idealistic, I am a strong believer in the principles of open source software. I think OpenSesame is a great example of how open source software can really be a good alternative to - often very expensive - commercial software.\n\n---\n\n## Eduard Ort\n\nDocumentation and support\n\n*Department of Cognitive Psychology, Vrije Universiteit Amsterdam*\n\n<notranslate>\nfigure:\n source: eduard.png\n id: FigEduard\n caption: Eduard Ort\n</notranslate>\n\nI am a PhD candidate with Chris Olivers and Johannes Fahrenfort. Together we investigate what kinds of representations can serve as attentional templates in visual search.\n\nHaving used Opensesame during my studies, I am very happy to have gotten the chance to contribute to its development. I think many researchers could benefit from using this powerful and easy-to-use tool.\n\n---\n\n## Joshua Snell\n\nDocumentation and support\n\n*Laboratoire de Psychologie Cognitive, CNRS, Aix-Marseille Université*": {
    "fr": "OpenSesame est principalement développé par un groupe informel d'individus. Mais tout le monde est le bienvenu pour contribuer !\n\n<notranslate>[TOC]</notranslate>\n\n## [Sebastiaan Mathôt](http://www.cogsci.nl/smathot)\n\nChef de projet et développeur principal\n\n*Laboratoire de Psychologie Cognitive, CNRS, Aix-Marseille Université*\n\n<notranslate>\nfigure:\n source: sebastiaan.png\n id: FigSebastiaan\n caption: Sebastiaan Mathôt\n</notranslate>\n\nJe suis actuellement boursier Marie Curie au Laboratoire de Psychologie Cognitive de Marseille, en France. Mes recherches portent sur la relation entre la réponse pupillaire à la lumière, l'attention visuelle et les mouvements oculaires.\n\nOpenSesame est l'aspect plus pragmatique de mon travail. Avec ce projet, nous souhaitons fournir aux psychologues expérimentaux et aux neuroscientifiques un générateur d'expériences de haute qualité et gratuit (dans tous les sens du terme).\n\n---\n\n## Daniel Schreij\n\nDéveloppeur\n\n*Développeur de logiciels indépendant*\n\n<notranslate>\nfigure:\n source: daniel.png\n id: FigDaniel\n caption: Daniel Schreij\n</notranslate>\n\nJ'ai toujours été intéressé par la technologie et la cognition humaine, c'est pourquoi j'ai choisi d'étudier l'intelligence artificielle (IA), qui est une combinaison de ces deux disciplines. Pendant cette période, j'ai été principalement attiré par le côté \"ce qui fait agir les gens\" de l'IA, ce qui m'a conduit à suivre une maîtrise en sciences cognitives. Dans ce domaine, j'ai également obtenu mon doctorat à la VU University d'Amsterdam, où j'ai travaillé en tant que post-doctorant. Pendant tout ce temps, je n'ai jamais perdu mon intérêt pour l'informatique et j'ai suivi de près les derniers développements dans ce domaine. Actuellement, je travaille en tant que développeur de logiciels indépendant pour des projets scientifiques.\n\nJe suis ravi de pouvoir aider Sebastiaan dans le développement d'OpenSesame, car cela me permet de continuer à effectuer des tâches plus pragmatiques comme le développement de logiciels, que j'aime beaucoup, et en même temps, de rester impliqué dans la recherche en psychologie.\n\n---\n\n## [Lotje van der Linden](http://www.cogsci.nl/lvanderlinden)\n\nDocumentation et support\n\n*Laboratoire de Psychologie Cognitive, Aix-Marseille Université*\n\n<notranslate>\nfigure:\n source: lotje.png\n id: FigLotje\n caption: Lotje van der Linden\n</notranslate>\n\nJe travaille en tant que doctorante sous la supervision de Françoise Vitu au Laboratoire de Psychologie Cognitive de l'Aix-Marseille Université. Mon projet de doctorat porte sur la façon dont les affordances (possibilités d'action dans notre environnement) influencent les mouvements et les endroits où nous dirigeons nos yeux.\n\nJe suis vraiment ravie de participer au projet OpenSesame, de contribuer à la documentation et d'offrir un soutien sur le forum. N'hésitez donc pas à poser vos questions !\n\n---\n\n## [Edwin Dalmaijer](http://www.pygaze.org/esdalmaijer/)\n\nDéveloppeur\n\n*Département de Psychologie Expérimentale, Université d'Utrecht*\n\n<notranslate>\nfigure:\n source: edwin.png\n id: FigEdwin\n caption: Edwin Dalmaijer\n</notranslate>\n\nJe suis actuellement doctorant à l'Université d'Oxford. Pour le projet OpenSesame, je m'occupe du package PyGaze lié à l'oculométrie.\n\nJeune et idéaliste, je crois fermement aux principes des logiciels libres. Je pense qu'OpenSesame est un excellent exemple de la manière dont les logiciels libres peuvent constituer une véritable alternative aux logiciels commerciaux, souvent très coûteux.\n\n---\n\n## Eduard Ort\n\nDocumentation et support\n\n*Département de Psychologie Cognitive, Vrije Universiteit Amsterdam*\n\n<notranslate>\nfigure:\n source: eduard.png\n id: FigEduard\n caption: Eduard Ort\n</notranslate>\n\nJe suis doctorant avec Chris Olivers et Johannes Fahrenfort. Ensemble, nous étudions les types de représentations pouvant servir de modèles attentionnels dans la recherche visuelle.\n\nAyant utilisé OpenSesame pendant mes études, je suis très heureux d'avoir eu l'occasion de contribuer à son développement. Je pense que de nombreux chercheurs pourraient bénéficier de l'utilisation de cet outil puissant et facile à utiliser.\n\n---\n\n## Joshua Snell\n\nDocumentation et support\n\n*Laboratoire de Psychologie Cognitive, CNRS, Aix-Marseille Université*"
  },
  "<notranslate>\nfigure:\n source: joshua.png\n id: FigJoshua\n caption: Joshua Snell\n</notranslate>\n\nHi! I am a PhD student at the Laboratoire de Psychologie Cognitive in Marseille. I'll happily contribute in finding a solution to any problem you might encounter while realizing your experiment (or whatever it is you want to use OpenSesame for - heck, you could even make games with it!)\n": {
    "fr": "<notranslate>\nfigure:\n source: joshua.png\n id: FigJoshua\n caption: Joshua Snell\n</notranslate>\n\nSalut ! Je suis doctorant au Laboratoire de Psychologie Cognitive à Marseille. Je serai ravi de contribuer à trouver une solution à tout problème que vous pourriez rencontrer lors de la réalisation de votre expérience (ou pour toute autre utilisation d'OpenSesame - vous pourriez même créer des jeux avec !)"
  },
  "Publications": {
    "fr": "Publications"
  },
  "<notranslate>[TOC]</notranslate>\n\n\n## How to cite OpenSesame\n\nIf you have used OpenSesame, we would appreciate it if you cite us:\n\n- Mathôt, S., Schreij, D., & Theeuwes, J. (2012). OpenSesame: An open-source, graphical experiment builder for the social sciences. *Behavior Research Methods*, *44*(2), 314-324. doi:10.3758/s13428-011-0168-7\n\nIf you make extensive use of modules such as SciPy/ NumPy, [PsychoPy][psycho], or [Expyriment][xpyriment], please also cite the respective authors.\n\n\n## OpenSesame in publications, theses, conference proceedings, etc.\n\n<div class=\"reference\" markdown=\"1\">\n\n### 2017 / in press / online first\n\nAdamou, E., & Shen, X. R. (2017). There are no language switching costs when codeswitching is frequent. *International Journal of Bilingualism*. doi:10.1177/1367006917709094\n\nAlbonico, A., & Barton, J. J. S. (2017). Face perception in pure alexia: Complementary contributions of the left fusiform gyrus to facial identity and facial speech processing. *Cortex*. doi:10.1016/j.cortex.2017.08.029\n\nBahn, D., Vesker, M., García Alanis, J. C., Schwarzer, G., & Kauschke, C. (2017). Age-dependent positivity-bias in children’s processing of emotion terms. *Frontiers in Psychology*, 8. doi:10.3389/fpsyg.2017.01268\n\nBonny, J. W., Lindberg, J. C., & Pacampara, M. C. (2017). Hip hop dance experience linked to sociocognitive ability. *PLoS ONE*, *12*(2), e0169947. doi:journal.pone.0169947\n\nBernstein, E. E., Heeren, A., & McNally, R. J. (2017). Unpacking Rumination and Executive Control: A Network Perspective. *Clinical Psychological Science*. doi:10.1177/2167702617702717\n\nÇakır, M. P., Çakır, N. A., Ayaz, H., & Lee, F. J. (2016). Behavioral and neural effects of game-based learning on improving computational fluency with numbers. *Zeitschrift Für Psychologie*, *224*(4), 297–302. doi:10.1027/2151-2604/a000267\n\nĆirić, M., & Đurđević, D. F. (2017). Procene konkretnosti reči zavise od stimulusnog konteksta. *Primenjena Psihologija*, *10*(3), 375–400. doi:10.19090/pp.2017.3.375-400\n\nDeclerck, M., Snell, J., & Grainger, J. (2017). On the role of language membership information during word recognition in bilinguals: Evidence from flanker-language congruency effects. *Psychonomic Bulletin & Review*. doi:10.3758/s13423-017-1374-9\n\nFairhurst, M. T., & Deroy, O. (2017). Testing the shared spatial representation of magnitude of auditory and visual intensity. *Journal of Experimental Psychology: Human Perception and Performance*, *43*(3), 629–637. doi:10.1037/xhp0000332\n\nFairchild, S., & Papafragou, A. (2017). Flexible expectations of speaker informativeness shape pragmatic inference. *University of Pennsylvania Working Papers in Linguistics*, 23(1), 7.\n\nFerrand, L., Méot, A., Spinelli, E., New, B., Pallier, C., Bonin, P., Dufau, S., Mathôt, S., & Grainger, J. (2017). MEGALEX: A megastudy of visual and auditory word recogntion. *Behavior Research Methods*. doi:10.3758/s13428-017-0943-1\n\nFido, D., Santo, M. G. E., Bloxsom, C. A. J., Gregson, M., & Sumich, A. L. (2017). Electrophysiological study of the violence inhibition mechanism in relation to callous-unemotional and aggressive traits. *Personality and Individual Differences*, *118*, 44–49. doi:10.1016/j.paid.2017.01.049\n\nFormoso, J., Barreyro, J.P, Jacubovich, S., & Injoque-Ricle, I. (2017). Possible associations between subitizing, estimation\nand visuospatial working memory (VSWM) in\nchildren. *The Spanish Journal of Psychology*. doi:10.1017/sjp.2017.23\n\nGaraizar, P., & Vadillo, M. A. (2017). Metronome LKM: An open source virtual keyboard driver to measure experiment software latencies. *Behavior Research Methods*. doi:10.3758/s13428-017-0958-7\n\nGarza, R., Heredia, R. R., & Cieślicka, A. B. (2017). An eye tracking examination of men’s attractiveness by conceptive risk women. *Evolutionary Psychology*, *15*(1). doi:10.1177/1474704917690741": {
    "fr": "<notranslate>[TOC]</notranslate>\n\n\n## Comment citer OpenSesame\n\nSi vous avez utilisé OpenSesame, nous vous serions reconnaissants de nous citer :\n\n- Mathôt, S., Schreij, D., & Theeuwes, J. (2012). OpenSesame : Un logiciel libre, concepteur d'expériences graphiques pour les sciences sociales. *Behavior Research Methods*, *44*(2), 314-324. doi:10.3758/s13428-011-0168-7\n\nSi vous utilisez largement des modules tels que SciPy/ NumPy, [PsychoPy][psycho], ou [Expyriment][xpyriment], veuillez également citer les auteurs respectifs.\n\n\n## OpenSesame dans les publications, thèses, actes de conférence, etc.\n\n<div class=\"reference\" markdown=\"1\">\n\n### 2017 / sous presse / en ligne en premier\n\nAdamou, E., & Shen, X. R. (2017). Il n'y a pas de coûts de commutation de langue lorsque la commutation de code est fréquente. *International Journal of Bilingualism*. doi:10.1177/1367006917709094\n\nAlbonico, A., & Barton, J. J. S. (2017). Perception du visage dans l'alexie pure : Contributions complémentaires du gyrus fusiforme gauche à l'identité faciale et au traitement de la parole faciale. *Cortex*. doi:10.1016/j.cortex.2017.08.029\n\nBahn, D., Vesker, M., García Alanis, J. C., Schwarzer, G., & Kauschke, C. (2017). Tendance à l'optimisme liée à l'âge dans le traitement des termes émotionnels chez les enfants. *Frontiers in Psychology*, 8. doi:10.3389/fpsyg.2017.01268\n\nBonny, J. W., Lindberg, J. C., & Pacampara, M. C. (2017). L'expérience de la danse hip hop liée à la capacité sociocognitive. *PLoS ONE*, *12*(2), e0169947. doi:journal.pone.0169947\n\nBernstein, E. E., Heeren, A., & McNally, R. J. (2017). Déballer les rumination et contrôle exécutif: Une perspective de réseau. *Clinical Psychological Science*. doi:10.1177/2167702617702717\n\nÇakır, M. P., Çakır, N. A., Ayaz, H., & Lee, F. J. (2016). Effets comportementaux et neuronaux de l'apprentissage basé sur les jeux pour améliorer la fluidité computationnelle avec les chiffres. *Zeitschrift Für Psychologie*, *224*(4), 297–302. doi:10.1027/2151-2604/a000267\n\nĆirić, M., & Đurđević, D. F. (2017). Les estimations de la concrétude des mots dépendent du contexte des stimuli. *Primenjena Psihologija*, *10*(3), 375–400. doi:10.19090/pp.2017.3.375-400\n\nDeclerck, M., Snell, J., & Grainger, J. (2017). The role of language membership information during word recognition in bilinguals: Evidence from flanker-language congruency effects. *Psychonomic Bulletin & Review*. doi:10.3758/s13423-017-1374-9\n\nFairhurst, M. T., & Deroy, O. (2017). Testing the shared spatial representation of magnitude of auditory and visual intensity. *Journal of Experimental Psychology: Human Perception and Performance*, *43*(3), 629–637. doi:10.1037/xhp0000332\n\nFairchild, S., & Papafragou, A. (2017). Les attentes flexibles de l'information du locuteur façonnent l'inférence pragmatique. *University of Pennsylvania Working Papers in Linguistics*, 23(1), 7.\n\nFerrand, L., Méot, A., Spinelli, E., New, B., Pallier, C., Bonin, P., Dufau, S., Mathôt, S., & Grainger, J. (2017). MEGALEX : Une mégétude de la reconnaissance visuelle et auditive des mots . *Behavior Research Methods*. doi:10.3758/s13428-017-0943-1\n\nFido, D., Santo, M. G. E., Bloxsom, C. A. J., Gregson, M., & Sumich, A. L. (2017). Étude électrophysiologique du mécanisme d'inhibition de la violence en relation avec les caractères apathiques-agressifs. *Personality and Individual Differences*, *118*, 44–49. doi:10.1016/j.paid.2017.01.049\n\nFormoso, J., Barreyro, J.P, Jacubovich, S., & Injoque-Ricle, I. (2017). Associations possibles entre la subitisation, l'estimation et la mémoire de travail spatiale visuelle (VSWM) chez les enfants. *The Spanish Journal of Psychology*. doi:10.1017/sjp.2017.23\n\nGaraizar, P., & Vadillo, M. A. (2017). Metronome LKM : Un pilote de clavier virtuel open source pour mesurer les temps de latence des logiciels expérimentaux. *Behavior Research Methods*. doi:10.3758/s13428-017-0958-7\n\nGarza, R., Heredia, R. R., & Cieślicka, A. B. (2017). Un examen oculomoteur de l'attrait des hommes pour les femmes à risque de conception. *Evolutionary Psychology*, *15*(1). doi:10.1177/1474704917690741"
  },
  "Leiden University 2022 workshop": {
    "fr": "Atelier de l'Université de Leiden 2022"
  },
  "\n<notranslate>[TOC]</notranslate>\n\n\n## Practical information\n\n- Host: Leiden University\n- Location and dates: \n    - Day 1: Oct 10, FSW, Living Lab 1B01, 13:00 - 17:00\n    - Day 2: Oct 17, FSW, Living Lab 1B01, 13:00 - 17:00\n- Presenters: Lotje van der Linden and Sebastiaan Mathôt\n\n\n## Description\n\nIn this two-day, hands-on workshop, you will learn how to implement psychological experiments with the open-source software OpenSesame. You will learn:\n\n- How to build and run experiments with OpenSesame\n- The limitations and advantages of online and laboratory-based experiments.\n- How to parse data collected with OpenSesame\n\nAn important aspect of the workshop will also be to help you with things that are directly relevant for your own research. Therefore, each day ends with a Q&A session. To make the most out the workshop, think in advance about what you would like to use OpenSesame for, and what you need help with during this workshop!\n\nIf you want to work on your own computer, please install OpenSesame before the workshop. You can download OpenSesame for free from <https://osdoc.cogsci.nl/>. No prior experience with OpenSesame, Python, or JavaScript is required.\n\nWe're looking forward to meeting you all!\n\n— Lotje and Sebastiaan\n\n\n### Day 1 (Oct 10): Introduction\n\nFirst presentation:\n\n- %static:attachments/leiden2022/1_Intro_OpenSesame_slides.pdf%\n\nSecond presentation:\n\n- %static:attachments/leiden2022/2_Cats_dogs_slides.pdf%\n\nSchedule:\n\n- 13:00 – 14:30: __Introduction to OpenSesame__. A general introduction to the software OpenSesame, followed by a hands-on tutorial in which you will learn the basic concepts of the software.\n- Break\n- 15:00 – 16:00: __Introduction to OpenSesame (continued)__. Continue with extra assignments to improve the experiment.\n- 16:00 – 17:00: __Q&A__ What kind of experiment would you like to build for your own research? And what kind of help do you need from us?\n\n\n### Day 2 (Oct 17): Online experiments + data analysis\n\nSlides:\n\n- *Will be made available here*\n\nExperiment file (in case you didn't create your own during day 1):\n\n- %static:attachments/leiden2022/capybaras.osexp%\n\nParticipant data files (experiment 2 from [this paper](https://doi.org/10.16910/jemr.5.2.4)):\n\n- %static:attachments/leiden2022/jemr2012exp2.zip%\n\nSchedule:\n\n- 13:00 – 14:00: __Building an online experiment.__ We will start this session with a general introduction to online experiments. This is followed by a hands-on tutorial in which we will modify the experiment that we created during Day 1 so that it is suitable for running online.\n- 14:00 – 15:00: __Using <https://mindprobe.eu> (a JATOS server) to run experiments online.__ In this session, you will learn how to use mindprobe.eu, which is a free server that runs the open-source software JATOS, to actually run an experiment online. A mindprobe.eu account will be provided to each participant.\n- Break\n- 15:30 – 16:00: __Getting data ready for analysis.__ We will start this session with a general explanation of how data is structured, both for experiments that are conducted online and for experiments that are conducted in a traditional laboratory set-up. Next, we will see how to transform this data into a format that lends itself to statistical analysis. Specifically, we will learn how data from multiple participants can be merged into a single `.csv` spreadsheet; we will also learn how data from an online experiment can be downloaded from JATOS and converted to a `.csv` spreadsheet.\n- 16:00 – 17:00: __Q&A__ What kind of experiment would you like to build for your own research? And what kind of help do you need from us?\n": {
    "fr": "\n<notranslate>[TOC]</notranslate>\n\n\n## Informations pratiques\n\n- Hôte : Université de Leiden\n- Lieu et dates :\n    - Jour 1 : 10 octobre, FSW, Living Lab 1B01, 13h00 - 17h00\n    - Jour 2 : 17 octobre, FSW, Living Lab 1B01, 13h00 - 17h00\n- Conférenciers : Lotje van der Linden et Sebastiaan Mathôt\n\n\n## Description\n\nDans cet atelier pratique de deux jours, vous apprendrez à mettre en œuvre des expériences de psychologie avec le logiciel open-source OpenSesame. Vous apprendrez:\n\n- Comment construire et exécuter des expériences avec OpenSesame\n- Les limites et avantages des expériences en ligne et en laboratoire.\n- Comment analyser les données collectées avec OpenSesame\n\nUn aspect important de l'atelier sera également de vous aider avec des choses qui sont directement pertinentes pour votre propre recherche. Par conséquent, chaque jour se termine par une séance de questions-réponses. Pour tirer le meilleur parti de l'atelier, réfléchissez à l'avance à ce que vous aimeriez utiliser OpenSesame pour, et à ce dont vous avez besoin d'aide lors de cet atelier !\n\nSi vous souhaitez travailler sur votre propre ordinateur, veuillez installer OpenSesame avant l'atelier. Vous pouvez télécharger OpenSesame gratuitement sur <https://osdoc.cogsci.nl/>. Aucune expérience préalable avec OpenSesame, Python ou JavaScript n'est requise.\n\nNous avons hâte de vous rencontrer tous !\n\n— Lotje et Sebastiaan\n\n### Jour 1 (10 octobre) : Introduction\n\nPremière présentation :\n\n- %static:attachments/leiden2022/1_Intro_OpenSesame_slides.pdf%\n\nDeuxième présentation :\n\n- %static:attachments/leiden2022/2_Cats_dogs_slides.pdf%\n\nProgramme :\n\n- 13h00 – 14h30 : __Introduction à OpenSesame__. Une introduction générale au logiciel OpenSesame, suivie d'un tutoriel pratique dans lequel vous apprendrez les concepts de base du logiciel.\n- Pause\n- 15h00 – 16h00 : __Introduction à OpenSesame (suite)__. Poursuivre avec des exercices supplémentaires pour améliorer l'expérience.\n- 16h00 – 17h00 : __Questions-réponses__ Quel type d'expérience aimeriez-vous créer pour vos propres recherches ? Et de quelle aide avez-vous besoin de notre part ?\n\n### Jour 2 (17 octobre) : Expériences en ligne et analyse de données\n\nDiapositives :\n\n- *Seront mises à disposition ici*\n\nFichier d'expérience (au cas où vous n'en auriez pas créé un lors du Jour1) :\n\n- %static:attachments/leiden2022/capybaras.osexp%\n\nFichiers de données des participants (expérience 2 de [cet article](https://doi.org/10.16910/jemr.5.2.4)) :\n\n- %static:attachments/leiden2022/jemr2012exp2.zip%\n\nProgramme :\n\n- 13h00 – 14h00 : __Création d'une expérience en ligne.__ Nous commencerons cette séance par une introduction générale aux expériences en ligne. Cela sera suivi d'un tutoriel pratique dans lequel nous modifierons l'expérience que nous avons créée lors du Jour 1 afin qu'elle soit adaptée à une utilisation en ligne.\n- 14h00 – 15h00 :__Utilisation de <https://mindprobe.eu> (serveur JATOS) pour exécuter des expériences en ligne.__ Dans cette session, vous apprendrez comment utiliser mindprobe.eu, qui est un serveur gratuit qui exécute le logiciel open-source JATOS, pour effectivement réaliser une expérience en ligne. Un compte mindprobe.eu sera fourni à chaque participant.\n- Pause\n- 15h30 – 16h00 : __Préparation des données pour l'analyse.__ Nous commencerons cette session par une explication générale de la manière dont les données sont structurées, à la fois pour les expériences menées en ligne et pour les expériences menées dans un laboratoire traditionnel. Ensuite, nous verrons comment transformer ces données en un format qui se prête à l'analyse statistique. Plus précisément, nous apprendrons comment les données de plusieurs participants peuvent être fusionnées en une seule feuille de calcul `.csv` ; nous apprendrons également comment les données d'une expérience en ligne peuvent être téléchargées depuis JATOS et converties en une feuille de calcul `.csv`.\n- 16h00 – 17h00 : __Questions-réponses__ Quel type d'expérience aimeriez-vous créer pour vos propres recherches ? Et de quelle aide avez-vous besoin de notre part ?"
  },
  "OpenSesame and PyGaze at ECEM 2017": {
    "fr": "OpenSesame et PyGaze à ECEM 2017"
  },
  "OpenSesame and PyGaze will have a demonstration booth at the European Conference on Eye Movements (ECEM), which will be held in Wuppertal, Germany between August 20 and 24, 2017.\n\n- <http://ecem2017.uni-wuppertal.de/>\n\nDrop by and say hi!\n": {
    "fr": "OpenSesame et PyGaze auront un stand de démonstration lors de la Conférence européenne sur les mouvements oculaires (ECEM), qui se tiendra à Wuppertal, en Allemagne, entre le 20 et le 24 août 2017.\n\n- <http://ecem2017.uni-wuppertal.de/>\n\nPassez nous voir et dites bonjour !"
  },
  "Page not found": {
    "fr": "Page non trouvée"
  },
  "The page that you requested does not exist. Perhaps it has moved.\n\nTo find the information you're looking for:\n\n- Browse the menu at the top of this page; or\n- Search using the search bar at the left of this page.\n": {
    "fr": "La page que vous avez demandée n'existe pas. Peut-être qu'elle a été déplacée.\n\nPour trouver l'information que vous recherchez :\n\n- Parcourez le menu en haut de cette page ; ou\n- Recherchez en utilisant la barre de recherche à gauche de cette page."
  },
  "Donate": {
    "fr": "Faire un don"
  },
  "With a donation you help to keep OpenSesame an active and innovative project! We use donations, among other things, to:\n\n- Reimburse volunteers for their expenses\n- Buy equipment for testing and development\n- Pay for webhosting\n\nGood software needs aware developers! Helps us stay sharp and buy us a coffee!\n\n<div class='cogsci-coffee'>\n<a href=\"https://www.buymeacoffee.com/cogsci\">\n<img style=\"max-width:192px; margin-top: -8px;\" src=\"https://img.buymeacoffee.com/button-api/?text=Buy us a coffee!&emoji=&slug=cogsci&button_colour=FFDD00&font_colour=000000&font_family=Cookie&outline_colour=000000&coffee_colour=ffffff\">\n</a>\n</div>\n\n\nFor large donations or sponsorship contracts, please contact: <s.mathot@cogsci.nl>\n": {
    "fr": "Avec un don, vous contribuez à maintenir OpenSesame comme un projet actif et innovant ! Nous utilisons les dons, entre autres, pour :\n\n- Rembourser les frais des bénévoles\n- Acheter du matériel pour les tests et le développement\n- Payer l'hébergement web\n\nUn bon logiciel nécessite des développeurs conscients ! Aidez-nous à rester à la pointe en nous offrant un café !\n\n<div class='cogsci-coffee'>\n<a href=\"https://www.buymeacoffee.com/cogsci\">\n<img style=\"max-width:192px; margin-top: -8px;\" src=\"https://img.buymeacoffee.com/button-api/?text=Offrez-nous un café !&emoji=&slug=cogsci&button_colour=FFDD00&font_colour=000000&font_family=Cookie&outline_colour=000000&coffee_colour=ffffff\">\n</a>\n</div>\n\n\nPour les gros dons ou les contrats de partenariat, veuillez contacter : <s.mathot@cogsci.nl>"
  },
  "ESCoP 2019 workshop": {
    "fr": "Atelier ESCoP 2019"
  },
  "\n<notranslate>[TOC]</notranslate>\n\n\n## About the workshop\n\nThis OpenSesame workshop will take place as a pre-conference event before *ESCoP 2019*. Participation is free and no registration is required.\n\nThe workshop consists of two main parts. In the first part, corresponding to the Basic Steps below, we created a complete experiment together. In the second part, corresponding to the Extra Assignments below, you will finish and improve the experiment by yourself.\n\n\n### When?\n\n- September 25th, 2019\n- 14:30 - 16:10\n\n### Where?\n\n- Conference Center \"La Pirámide de Arona\" (Mare Nostrum Resort)\n- Av. las Américas\n- 38650 Arona\n- Santa Cruz de Tenerife\n- Spain\n\n### More info\n\n- Conference site: <https://escop2019.webs.ull.es/>\n\n\n## The basic steps\n\n\n<notranslate>\nfigure:\n id: FigWCST\n source: wcst.png\n caption: |\n  The Wisconsin Card Sorting Test (WCST) is a neuropsychological test of executive functions.\n</notranslate>\n\n\nYou will implement the Wisconsin Card Sorting Test (WCST) and learn how you can run this test online with OSWeb.\n\nIn the WCST, participants see four stimulus cards, which differ on three dimensions: color (red, green, blue, yellow), shape (circle, star, triangle, cross), and number of shapes (one, two, three, or four). Participants also see a single response card, which also has a color, shape, and number.\n\nThe participant's task is to match the response card to the correct stimulus card, based on a specific dimension (e.g. color), or *matching rule*. The participant initially doesn't know on which dimension to match, and his or her task is to figure out the matching rule through trial and error.\n\nTo make things more difficult, the matching rule changes after every five correct responses. Therefore, the participant needs to flexibly update their matching rule.\n\n\n### Step 1: Download and start OpenSesame\n\nOpenSesame is available for Windows, Linux, and Mac OS. This tutorial is written for OpenSesame 3.2, and you can use either the version based on Python 2.7 (default) or Python 3.6. You can download OpenSesame from here:\n\n- %link:download%\n\n\nWhen you start OpenSesame, you will be given a choice of template experiments, and (if any) a list of recently opened experiments (see %FigStartUp).\n\n<notranslate>\nfigure:\n id: FigStartUp\n source: start-up.png\n caption: |\n  The OpenSesame window on start-up.\n</notranslate>\n\nThe *Extended template* provides a good starting point for creating many experiments that use a block-trial structure. However, in this tutorial we will create the entire experiment from scratch, and we will use the 'default template', which is already loaded when OpenSesame is launched (%FigDefaultTemplate). Therefore, simply close the 'Get started!' and (if shown) 'Welcome!' tabs.\n\n<notranslate>\nfigure:\n id: FigDefaultTemplate\n source: default-template.png\n caption: |\n  The structure of the 'Default template' as seen in the overview area.\n</notranslate>\n\n\n### Step 2: Add a block_loop and trial_sequence\n\nThe default template starts with three items: A NOTEPAD called *getting_started*, a SKETCHPAD called *welcome*, and a SEQUENCE called *experiment*. We don't need *getting_started* and *welcome*, so let's remove these right away. To do so, right-click on these items and select 'Delete'. Don't remove *experiment*, because it is the entry for the experiment (i.e. the first item that is called when the experiment is started).\n\nOur experiment will have a very simple structure. At the top of the hierarchy is a LOOP, which we will call *block_loop*. The *block_loop* is the place where we will define our independent variables. To add a LOOP to your experiment, drag the LOOP icon from the item toolbar onto the *experiment* item in the overview area.": {
    "fr": "## À propos de l'atelier\n\nCet atelier OpenSesame se déroulera lors d'un événement pré-conférence avant *ESCoP 2019*. La participation est gratuite et aucune inscription n'est requise.\n\nL'atelier se compose de deux parties principales. Dans la première partie, correspondant aux étapes de base ci-dessous, nous créons ensemble une expérience complète. Dans la deuxième partie, correspondant aux missions supplémentaires ci-dessous, vous terminez et améliorez l'expérience par vous-même.\n\n\n\n\n### Quand ?\n\n- 25 septembre 2019\n- 14h30 - 16h10\n\n### Où ?\n\n- Centre de conférence \"La Pirámide de Arona\" (Mare Nostrum Resort)\n- Av. las Américas\n- 38650 Arona\n- Santa Cruz de Tenerife\n- Espagne\n\n### Plus d'informations\n\n- Site de la conférence : <https://escop2019.webs.ull.es/>\n\n\n\n## Les étapes de base\n\n\n\nVous allez mettre en œuvre le Wisconsin Card Sorting Test (WCST) et apprendre comment vous pouvez exécuter ce test en ligne avec OSWeb.\n\nDans le WCST, les participants voient quatre cartes stimulus, qui diffèrent sur trois dimensions : couleur (rouge, vert, bleu, jaune), forme (cercle, étoile, triangle, croix) et nombre de formes (un, deux, trois ou quatre). Les participants voient aussi une seule carte de réponse, qui a également une couleur, une forme et un nombre.\n\nLa tâche du participant est de faire correspondre la carte de réponse à la bonne carte stimulus, en fonction d'une dimension spécifique (par exemple, la couleur) ou d'une *règle de correspondance*. Le participant ne sait pas initialement sur quelle dimension faire correspondre, et sa tâche est de découvrir la règle de correspondance par essais et erreurs.\n\nPour compliquer les choses, la règle de correspondance change après chaque cinq réponses correctes. Par conséquent, le participant doit mettre à jour de manière flexible sa règle de correspondance.\n\n\n### Étape 1: Téléchargez et démarrez OpenSesame\n\nOpenSesame est disponible pour Windows, Linux et Mac OS. Ce tutoriel est écrit pour OpenSesame 3.2 et vous pouvez utiliser la version basée sur Python 2.7 (par défaut) ou Python 3.6. Vous pouvez télécharger OpenSesame à partir d'ici:\n\n- %link:download%\n\nLorsque vous démarrez OpenSesame, il vous est proposé de choisir des expériences modèles et (le cas échéant) une liste des expériences ouvertes récemment (voir %FigStartUp).\n\nLe modèle *Extended template* offre un bon point de départ pour créer de nombreuses expériences qui utilisent une structure de blocs-essais. Cependant, dans ce tutoriel, nous créerons toute l'expérience à partir de zéro et nous utiliserons le 'modèle par défaut', qui est déjà chargé au lancement d'OpenSesame (%FigDefaultTemplate). Fermez simplement les onglets 'Get started!' et (si affiché) 'Welcome!'.\n\nNous allons maintenant ajouter un LOOP et une SEQUENCE à notre expérience. Le modèle par défaut démarre avec trois éléments : un NOTEPAD appelé *getting_started*, un SKETCHPAD appelé *welcome*, et une SEQUENCE appelée *experiment*. Nous n'avons pas besoin de *getting_started* et de *welcome*, alors supprimons-les tout de suite. Pour ce faire, faites un clic droit sur ces éléments et sélectionnez 'Supprimer'. Ne supprimez pas *experiment*, car c'est l'entrée de l'expérience (c'est-à-dire le premier élément qui est appelé lorsque l'expérience est lancée).\n\nNotre expérience aura une structure très simple. En haut de la hiérarchie se trouve un LOOP, que nous appellerons *block_loop*. Le *block_loop* est l'endroit où nous définirons nos variables indépendantes. Pour ajouter un LOOP à votre expérience, faites glisser l'icône LOOP de la barre d'outils des éléments sur l'élément *experiment* dans la zone de vue d'ensemble."
  },
  "A LOOP item needs another item to run; usually, and in this case as well, this is a SEQUENCE. Drag the SEQUENCE item from the item toolbar onto the *new_loop* item in the overview area. OpenSesame will ask whether you want to insert the SEQUENCE into or after the LOOP. Select 'Insert into new_loop'.\n\nBy default, items have names such as *new_sequence*, *new_loop*, *new_sequence_2*, etc. These names are not very informative, and it is good practice to rename them. Item names must consist of alphanumeric characters and/ or underscores. To rename an item, double-click on the item in the overview area. Rename *new_sequence* to *trial_sequence* to indicate that it will correspond to a single trial. Rename *new_loop* to *block_loop* to indicate that will correspond to a block of trials.\n\nFinally, click on 'New experiment'to open the General Properties tab. Click on the title of the experiment, and rename it to 'Wisconsin Card Sorting Test'.\n\nThe overview area of our experiment now looks as in %FigBasicStructure.\n\n<notranslate>\nfigure:\n id: FigBasicStructure\n source: basic-structure.png\n caption: |\n  The overview area at the end of Step 2.\n</notranslate>\n\n\n### Step 3: Import images and sound files\n\nFor this experiment, we will use images for the playing cards. You can download these from here:\n\n- %static:attachments/wisconsin-card-sorting-test/stimuli.zip%\n\nDownload `stimuli.zip` and extract it somewhere (to your desktop, for example). Next, in OpenSesame, click on the 'Show file pool' button in the main toolbar (or: Menu →View → Show file pool). This will show the file pool, by default on the right side of the window. The easiest way to add the stimuli to the file pool is by dragging them from the desktop (or wherever you have extracted the files to) into the file pool. Alternatively, you can click on the '+' button in the file pool and add files using the file-selection dialog that appears. The file pool will automatically be saved with your experiment.\n\nAfter you have added all stimuli, your file pool looks as in %FigFilePool.\n\n<notranslate>\nfigure:\n id: FigFilePool\n source: file-pool.png\n caption: |\n  The file pool containing the stimuli.\n</notranslate>\n\n\n### Step 4: Create a static card display\n\nTo start with, we'll create a display with four stimulus cards and one response card. However, which cards are shown will not, for now, depend on variables; that is, we will create a *static* display.\n\nDrag a a SKETCHPAD into *trial_sequence*, and rename it to *card_display*. Use the image tool to draw four cards in a horizontal row somewhere near the top of the display; these will be the stimulus cards. Draw a single card near the bottom of the display; this will be the response card. Also add some text to indicate to the participant what he or she has to do, namely press `a`, `b`, `c`, or `d` to indicate which of the stimulus cards matches the response card. The exact text, layout, and cards are up to you! Tips: you can use the *scale* option to adjust the size of the cards; you can change the background color in the General Properties tab, which you can open by clicking on the top-level item of the experiment.\n\nFor me, the result looks like this:\n\n\n<notranslate>\nfigure:\n id: FigStaticCards\n source: static-cards.png\n caption: |\n  A SKETCHPAD with statically defined cards.\n</notranslate>\n\n\n### Step 5: Make the response card variable\n\nRight now we're always showing the same response card (in the example above a single blue triangle). But of course we want to show a different response card on every trial. To do so, we first need to define the variables that determine which response card we will show. We will do this in the *block_loop*.": {
    "fr": "Un élément LOOP a besoin d'un autre élément pour fonctionner; généralement, et dans ce cas aussi, il s'agit d'une SEQUENCE. Faites glisser l'élément SEQUENCE depuis la barre d'outils des éléments sur l'élément *new_loop* dans la zone d'aperçu. OpenSesame vous demandera si vous souhaitez insérer la SEQUENCE dans ou après la LOOP. Sélectionnez \"Insérer dans new_loop\".\n\nPar défaut, les éléments ont des noms tels que *new_sequence*, *new_loop*, *new_sequence_2*, etc. Ces noms ne sont pas très informatifs et il est recommandé de les renommer. Les noms des éléments doivent être composés de caractères alphanumériques et/ou de traits de soulignement. Pour renommer un élément, double-cliquez sur l'élément dans la zone d'aperçu. Renommez *new_sequence* en *trial_sequence* pour indiquer qu'il correspondra à un essai unique. Renommez *new_loop* en *block_loop* pour indiquer qu'il correspondra à un bloc d'essais.\n\nEnfin, cliquez sur \"New experiment\" pour ouvrir l'onglet Propriétés générales. Cliquez sur le titre de l'expérience et renommez-le \"Wisconsin Card Sorting Test\".\n\nLa zone d'aperçu de notre expérience ressemble maintenant à %FigBasicStructure.\n\n<notranslate>\nfigure :\n id: FigBasicStructure\n source: basic-structure.png\n caption: |\n  La zone d'aperçu à la fin de l'étape 2.\n</notranslate>\n\n\n### Étape 3 : Importer des images et des fichiers audio\n\nPour cette expérience, nous utiliserons des images pour les cartes à jouer. Vous pouvez les télécharger ici :\n\n- %static:attachments/wisconsin-card-sorting-test/stimuli.zip%\n\nTéléchargez `stimuli.zip` et extrayez-le quelque part (sur votre bureau, par exemple). Ensuite, dans OpenSesame, cliquez sur le bouton \"Afficher le pool de fichiers\" dans la barre d'outils principale (ou : Menu → Affichage → Afficher le pool de fichiers). Cela affichera le pool de fichiers, par défaut sur le côté droit de la fenêtre. La manière la plus simple d'ajouter les stimuli au pool de fichiers est de les faire glisser depuis le bureau (ou depuis l'endroit où vous avez extrait les fichiers) dans le pool de fichiers. Sinon, vous pouvez cliquer sur le bouton \"+\" dans le pool de fichiers et ajouter des fichiers en utilisant la boîte de dialogue de sélection de fichiers qui apparaît. Le pool de fichiers est automatiquement sauvegardé avec votre expérience.\n\nAprès avoir ajouté tous les stimuli, votre pool de fichiers ressemble à %FigFilePool.\n\n<notranslate>\nfigure :\n id: FigFilePool\n source: file-pool.png\n caption: |\n  Le pool de fichiers contenant les stimuli.\n</notranslate>\n\n\n### Étape 4 : Créer un affichage de carte statique\n\nPour commencer, nous créerons un affichage avec quatre cartes stimuli et une carte réponse. Cependant, les cartes affichées ne dépendront pas, pour l'instant, des variables ; autrement dit, nous créerons un affichage *statique*.\n\nFaites glisser un SKETCHPAD dans *trial_sequence* et renommez-le en *card_display*. Utilisez l'outil d'image pour dessiner quatre cartes dans une rangée horizontale près du haut de l'affichage; ce seront les cartes stimuli. Dessinez une seule carte près du bas de l'affichage; ce sera la carte réponse. Ajoutez également du texte pour indiquer au participant ce qu'il doit faire, à savoir appuyer sur `a`, `b`, `c` ou `d` pour indiquer laquelle des cartes stimuli correspond à la carte réponse. Le texte exact, la disposition et les cartes dépendent de vous ! Astuces : vous pouvez utiliser l'option *scale* pour ajuster la taille des cartes ; vous pouvez changer la couleur d'arrière-plan dans l'onglet Propriétés générales, que vous pouvez ouvrir en cliquant sur l'élément de niveau supérieur de l'expérience.\n\nPour moi, le résultat ressemble à ça:\n\n<notranslate>\nfigure:\n id: FigStaticCards\n source: static-cards.png\n caption: |\n  Un SKETCHPAD avec des cartes définies statiquement.\n</notranslate>\n\n\n### Étape 5 : Rendre la carte de réponse variable\n\nPour l'instant, nous montrons toujours la même carte réponse (dans l'exemple ci-dessus, un seul triangle bleu). Mais bien sûr, nous voulons montrer une carte réponse différente à chaque essai. Pour ce faire, nous devons d'abord définir les variables qui déterminent quelle carte réponse nous allons montrer. Nous le ferons dans le *block_loop*."
  },
  "Open the *block_loop*. The LOOP table is now empty. To determine the color, shape, and number of the response card, we could manually create three columns (`response_color`, `response_shape`, and `response_number`) and 64 rows for all possible combinations of colors, shapes, and numbers. But that would be a lot of work. Instead, we will use the full-factorial-design wizard, which you can open by clicking on the 'Full-factorial design' button. (A full-factorial design is a design in which all possible combinations of variable levels occur.) In this wizard, you create one column for each of the three variables, and in the cells below enter the possible values for that variable (see %FigDesignWizard).\n\n\n<notranslate>\nfigure:\n id: FigDesignWizard\n source: design-wizard.png\n caption: |\n  The full-factorial-design wizard allows you to easily generate large LOOP tables that correspond to full-factorial designs.\n</notranslate>\n\n\nNext, click the OK button. The *block_loop* now contains all 64 combinations of colors, numbers, and shapes (see %FigLoopTable1).\n\n\n<notranslate>\nfigure:\n id: FigLoopTable1\n source: loop-table-1.png\n caption: |\n  The *block_loop* at the end of step 5.\n</notranslate>\n\n\nNow return to the *card_display*. Every item in OpenSesame is defined through a script. This script is generated automatically by the user interface. Sometimes it can be convenient (or even necessary) to edit this script directly. The most common reason for editing an item's script is to add variables to the script, which is also what we will do now!\n\nTo view the script, click on the 'View' button and select 'View script'. (The view button is the middle button at the top right of the item controls.) This will open a script editor.\n\nThe script for *card_display* mostly consists of `draw` commands, which define each of the five cards, and also the various text elements. Locate the line that corresponds to the response card. You can find it by looking at the Y coordinate, which should be positive (i.e. at the lower part of the display), or by looking at the name of the image file.\n\n```\ndraw image center=1 file=\"1-blue-triangle.png\" scale=0.5 show_if=always x=0 y=192 z_index=0\n```\n\nRight now, in my example, the image file for the response card is always `\"1-blue-triangle.png\"`. But of course we don't always want to show a single blue triangle. Instead, we want to have the image file depend on the variables that we have defined in the *block_loop*. To do so, replace the number by `[response_number]`, the color by `[response_color]`, and the shape by `[response_shape]`: (The square brackets indicate that these refer to names of variables.)\n\n\n```\ndraw image center=1 file=\"[response_number]-[response_color]-[response_shape].png\" scale=0.5 show_if=always x=0 y=192 z_index=0\n```\n\nClick on Apply to accept the changes to the script. The response card has now been replaced by a question-mark icon. This is because OpenSesame doesn't know how to show a preview of an image that has been defined using variables. But don't worry: the image will be shown when you run the experiment!\n\n\n### Step 6: Make the stimulus cards variable\n\nThe stimulus cards should be more-or-less randomly selected, but each color, shape, and number should occur only once; that is, there should never be two red cards or two cards with triangles. (If there were, the matching procedure would become ambiguous.) To accomplish this, we can use *horizontal shuffling*, which is a powerful but unusual feature of the LOOP item.\n\n- %link:loop%\n\nFirst, open the *block_loop* and create 12 (!) new columns to define the stimulus cards: `color1`, for the color of the first card, `color2`, `color3`, `color4`, and `shape1` … `shape4`, and `number1` … `number4`. Each column has the same value on every row (see %FigLoopTable2).\n\n\n<notranslate>\nfigure:\n id: FigLoopTable2\n source: loop-table-2.png\n caption: |\n  The *block_loop* during step 6.\n</notranslate>": {
    "fr": "Ouvrez la *block_loop*. La table LOOP est maintenant vide. Pour déterminer la couleur, la forme et le nombre de la carte réponse, nous pourrions créer manuellement trois colonnes (`response_color`, `response_shape` et `response_number`) et 64 lignes pour toutes les combinaisons possibles de couleurs, formes et nombres. Mais cela représenterait beaucoup de travail. Au lieu de cela, nous allons utiliser l'assistant de planification factorielle complète, que vous pouvez ouvrir en cliquant sur le bouton \"Plan factoriel complet\". (Un plan factoriel complet est un plan dans lequel toutes les combinaisons possibles de niveaux de variables se produisent.) Dans cet assistant, vous créez une colonne pour chacune des trois variables, et dans les cellules du dessous, entrez les valeurs possibles pour cette variable (voir %FigDesignWizard).\n\n<notranslate>\nfigure:\n id: FigDesignWizard\n source: design-wizard.png\n caption: |\n  L'assistant de planification factorielle complète vous permet de générer facilement de grandes tables LOOP correspondant à des plans factoriels complets.\n</notranslate>\n\nEnsuite, cliquez sur le bouton OK. La *block_loop* contient maintenant les 64 combinaisons de couleurs, nombres et formes (voir %FigLoopTable1).\n\n<notranslate>\nfigure:\n id: FigLoopTable1\n source: loop-table-1.png\n caption: |\n  La *block_loop* à la fin de l'étape 5.\n</notranslate>\n\nRevenez maintenant à la *card_display*. Chaque élément dans OpenSesame est défini par un script. Ce script est généré automatiquement par l'interface utilisateur. Parfois, il peut être pratique (ou même nécessaire) d'éditer directement ce script. La raison la plus courante pour éditer le script d'un élément est d'ajouter des variables au script, ce que nous allons faire maintenant !\n\nPour afficher le script, cliquez sur le bouton \"Afficher\" et sélectionnez \"Afficher le script\". (Le bouton Afficher se trouve au milieu des commandes d'élément en haut à droite.) Cela ouvrira un éditeur de script.\n\nLe script pour *card_display* se compose principalement de commandes `draw`, qui définissent chacune des cinq cartes, ainsi que des différents éléments de texte. Repérez la ligne correspondant à la carte réponse. Vous pouvez la trouver en regardant l'axe des Y, qui doit être positif (c'est-à-dire dans la partie inférieure de l'affichage) ou en regardant le nom du fichier image.\n\n```\ndraw image center=1 file=\"1-blue-triangle.png\" scale=0.5 show_if=always x=0 y=192 z_index=0\n```\n\nEn ce moment, dans mon exemple, le fichier image pour la carte réponse est toujours `\"1-blue-triangle.png\"`. Mais bien sûr, nous ne voulons pas toujours montrer un seul triangle bleu. Au lieu de cela, nous voulons avoir le fichier image en fonction des variables que nous avons définies dans la *block_loop*. Pour ce faire, remplacez le nombre par `[response_number]`, la couleur par `[response_color]` et la forme par `[response_shape]`: (Les crochets indiquent qu'il s'agit des noms de variables.)\n\n```\ndraw image center=1 file=\"[response_number]-[response_color]-[response_shape].png\" scale=0.5 show_if=always x=0 y=192 z_index=0\n```\n\nCliquez sur Appliquer pour accepter les modifications apportées au script. La carte réponse a maintenant été remplacée par une icône en forme de point d'interrogation. Cela est dû au fait qu'OpenSesame ne sait pas comment afficher un aperçu d'une image qui a été définie à l'aide de variables. Mais ne vous inquiétez pas : l'image sera affichée lorsque vous exécuterez l'expérience !\n\n### Étape 6: Rendre les cartes stimulus variables\n\nLes cartes stimulus doivent être sélectionnées de manière plus ou moins aléatoire, mais chaque couleur, forme et nombre ne doit apparaître qu'une seule fois ; c'est-à-dire qu'il ne devrait jamais y avoir deux cartes rouges ou deux cartes avec des triangles. (S'il y en avait, la procédure de correspondance deviendrait ambiguë.) Pour y parvenir, nous pouvons utiliser le *mélange horizontal*, qui est une fonctionnalité puissante mais inhabituelle de l'élément LOOP.\n\n- %link:loop%\n\nD'abord, ouvrez le *block_loop* et créez 12 (!) nouvelles colonnes pour définir les cartes stimulus : `color1`, pour la couleur de la première carte, `color2`, `color3`, `color4`, et `shape1` ... `shape4`, et `number1` ... `number4`. Chaque colonne a la même valeur sur chaque ligne (voir %FigLoopTable2).\n\n<notranslate>\nfigure:\n id: FigLoopTable2\n source: loop-table-2.png\n caption: |\n  La *block_loop* pendant l'étape 6.\n</notranslate>"
  },
  "\nBut we're not done yet! Right now, the first stimulus card is always a single red circle, the second two blue triangles, etc. To randomize this we tell OpenSesame to randomly swap (horizontally shuffle) the values of the four color variables, the four shape variables, and the four number variables. To do so, open the script for the *block_loop*. At the semi-last line (right before `run trial_sequence`) add the following commands:\n\n```\nshuffle_horiz color1 color2 color3 color4\nshuffle_horiz shape1 shape2 shape3 shape4\nshuffle_horiz number1 number2 number3 number4\n```\n\nClick on Apply to accept the script. To see if this has worked, click on the Preview button. This will show a preview of how the LOOP table will be randomized during the experiment. Does it look good?\n\nNow return to the *card_display* and have the image of the first stimulus card depend on the variable `color1`, `shape1`, and `number1`, and analogously for the other stimulus cards. (If you're unsure how to do this, revisit step 5.)\n\n\n### Step 7: Determine the correct response (for one matching rule)\n\n\nFor now, we're going to assume that participants always match by shape. (One of the Extra Assignments is to improve this.)\n\nRight now, the duration of *card_display* is set to 'keypress'. This means that the *card_display* is shown until a key is pressed, but it provides no control over how this key press is handled. Therefore, change the duration to 0, and insert a KEYBOARD_RESPONSE directly after the *card_display*. Rename the KEYBOARD_RESPONSE to *press_a*, and specify that the correct response is 'a' and that the allowed responses are 'a;b;c;d'.\n\n\n<notranslate>\nfigure:\n id: FigPressA\n source: press-a.png\n caption: |\n  One of the KEYBOARD_RESPONSE items defined in step 7.\n</notranslate>\n\n\nBut this is not enough! Right now there's a single response item that assumes that the correct response is always 'a'. We have not yet specified *when* the correct response is 'a', nor have we considered trials on which the correct response is 'b', 'c', or 'd'.\n\nTo accomplish this, first create three more KEYBOARD_RESPONSE items: *press_b*, *press_c*, and *press_d*. These are all the same, except for the correct response, which is defined for each of them separately and should be respectively 'b', 'c', and 'd'.\n\nFinally, in the *trial_sequence*, use Run If statements to decide under which condition each of the four KEYBOARD_RESPONSE items should be executed (thus deciding what the correct response is). For *press_a*, the condition is that `shape1` should be equal to `response_shape`. Why? Well, because that means that the shape of the first stimulus card is equal to the shape of the response card, and in that case the correct response is 'a'. This condition corresponds to the following run-if statement: `[shape1] = [response_shape]`. The run-if statements for the other KEYBOARD_RESPONSE items are analogous (see %FigTrialSequence1).\n\n\n<notranslate>\nfigure:\n id: FigTrialSequence1\n source: trial-sequence-1.png\n caption: |\n  The *trial_sequence* at the end of step 7.\n</notranslate>\n\n\n### Step 8: Give feedback to the participant\n\nOpenSesame automatically keeps track of whether a response was correct or not, by setting the variable `correct` to respectively 1 or 0. (Provided, of course, that you have specified the correct response, as we've done in step 7.) We can use this to give feedback to the participant about whether they responded correctly or not.\n\nTo do this, add two new SKETCHPADs to the *trial_sequence* and call them *correct_feedback* and *incorrect_feedback*. Then, specify which of the two should be executed using a run-if statement (see %FigTrialSequence2).\n\n\n<notranslate>\nfigure:\n id: FigTrialSequence2\n source: trial-sequence-2.png\n caption: |\n  The *trial_sequence* at the end of step 8.\n</notranslate>": {
    "fr": "Mais nous n'avons pas encore terminé! En ce moment, la première carte stimulus est toujours un seul cercle rouge, la seconde deux triangles bleus, etc. Pour randomiser cela, nous disons à OpenSesame de permuter aléatoirement (mélanger horizontalement) les valeurs des quatre variables de couleur, des quatre variables de forme et des quatre variables de nombre. Pour ce faire, ouvrez le script pour la *block_loop*. À l'avant-dernière ligne (juste avant `run trial_sequence`), ajoutez les commandes suivantes:\n\n```\nshuffle_horiz color1 color2 color3 color4\nshuffle_horiz shape1 shape2 shape3 shape4\nshuffle_horiz number1 number2 number3 number4\n```\n\nCliquez sur Appliquer pour accepter le script. Pour voir si cela a fonctionné, cliquez sur le bouton Aperçu. Cela montrera un aperçu de la manière dont le tableau LOOP sera randomisé pendant l'expérience. Est-ce que ça a l'air bien?\n\nRevenez maintenant au *card_display* et faites dépendre l'image de la première carte stimulus des variables `color1`, `shape1` et `number1`, et analogiquement pour les autres cartes stimulus. (Si vous n'êtes pas sûr de la manière de procéder, reportez-vous à l'étape 5.)\n\n\n### Étape 7: déterminer la réponse correcte (pour une règle de correspondance)\n\n\nPour l'instant, nous allons supposer que les participants correspondent toujours par forme. (L'une des missions supplémentaires consiste à améliorer cela.)\n\nEn ce moment, la durée du *card_display* est définie sur 'keypress'. Cela signifie que le *card_display* est affiché jusqu'à ce qu'une touche soit enfoncée, mais cela ne fournit aucun contrôle sur la manière dont cette pression de touche est gérée. Par conséquent, changez la durée à 0 et insérez un KEYBOARD_RESPONSE directement après le *card_display*. Renommez la KEYBOARD_RESPONSE en *press_a* et spécifiez que la réponse correcte est 'a' et que les réponses autorisées sont 'a;b;c;d'.\n\n\n<notranslate>\nfigure:\n id: FigPressA\n source: press-a.png\n légende: |\n  L'un des éléments de KEYBOARD_RESPONSE définis à l'étape 7.\n</notranslate>\n\n\nMais cela ne suffit pas! Pour l'instant, il y a un seul élément de réponse qui suppose que la réponse correcte est toujours 'a'. Nous n'avons pas encore spécifié *quand* la réponse correcte est 'a', et nous n'avons pas non plus examiné les essais pour lesquels la réponse correcte est 'b', 'c' ou 'd'.\n\nPour y parvenir, créez d'abord trois autres éléments KEYBOARD_RESPONSE: *press_b*, *press_c* et *press_d*. Ils sont tous les mêmes, sauf pour la réponse correcte, qui est définie pour chacun d'eux séparément et doit être respectivement 'b', 'c' et 'd'.\n\nEnfin, dans le *trial_sequence*, utilisez des instructions Run If pour déterminer dans quelle condition chacun des quatre éléments KEYBOARD_RESPONSE doit être exécuté (décidant ainsi quelle est la réponse correcte). Pour *press_a*, la condition est que `shape1` doit être égal à `response_shape`. Pourquoi? Eh bien, parce que cela signifie que la forme de la première carte stimulus est égale à la forme de la carte réponse, et dans ce cas, la réponse correcte est 'a'. Cette condition correspond à l'instruction run-if suivante: `[shape1] = [response_shape]`. Les instructions run-if pour les autres éléments KEYBOARD_RESPONSE sont analogues (voir %FigTrialSequence1).\n\n\n<notranslate>\nfigure:\n id: FigTrialSequence1\n source: trial-sequence-1.png\n légende: |\n  Le *trial_sequence* à la fin de l'étape 7.\n</notranslate>\n\n\n### Étape 8: donner un retour d'information au participant\n\nOpenSesame suit automatiquement si une réponse était correcte ou non, en définissant la variable `correct` à respectivement 1 ou 0. (À condition, bien sûr, que vous ayez spécifié la réponse correcte, comme nous l'avons fait à l'étape 7.) Nous pouvons utiliser cela pour donner un retour d'information au participant sur le fait qu'il a répondu correctement ou non.\n\nPour ce faire, ajoutez deux nouveaux SKETCHPADs à la *trial_sequence* et appelez-les *correct_feedback* et *incorrect_feedback*. Ensuite, spécifiez lequel des deux doit être exécuté en utilisant une instruction run-if (voir %FigTrialSequence2).\n\n\n<notranslate>\nfigure:\n id: FigTrialSequence2\n source: trial-sequence-2.png\n légende: |\n  Le *trial_sequence* à la fin de l'étape 8.\n</notranslate>"
  },
  "\nFinally, add some useful content to both SKETCHPADs. For example, for *correct_feedback* you could use a green fixation dot, and for *incorrect_feedback* you could use a red fixation dot, in both cases shown for 500 ms (i.e. setting the SKETCHPAD duration to 500). Colored dots are a nice, unobtrusive way to provide feedback.\n\n\n### Step 9: Test the experiment\n\nYou have now created a basic (but incomplete!) implementation of the Wisconsin Card Sorting Test. (You will complete the implementation as part of the Extra Assignments below.)\n\n\n<notranslate>\nfigure:\n id: FigRunButtons\n source: run-buttons.png\n caption: |\n  The main toolbar contains buttons to (from left to right): run fullscreen, run in a window, quick-run (run in a window without asking for log file or participant numebr ), abort the experiment, and run in a browser.\n</notranslate>\n\n\nTo test the experiment, click on the quick-run button (the blue double arrows) to test the experiment on the desktop (see %FigRunButtons). If the experiment runs as expected on the desktop, click on the run-in-browser button (the arrow inside a green circle) to test the experiment in a browser.\n\n\n## Extra assignments\n\n\n### Extra 1 (easy): Add a logger\n\nOpenSesame doesn't automatically log data. Instead, you need to explicitly add a `logger` item to your experiment. In a trial-based experiment, a `logger` is generally the last item of the *trial_sequence*, so that it logs all the data that was collected during the trial.\n\nRight now, our WCST doesn't log any data. Time to fix that!\n\n\n### Extra 2 (easy): Inspect the data file\n\n*Requires that you have completed Extra 1*.\n\nGive the experiment a short test run. Now inspect the log file in a program like Excel, LibreOffice Calc, or JASP. Identify the relevant variables, and think of how you could analyze the results.\n\n__Pro-tip:__ Set the repeat value of the *block_loop* to 0.1 to reduce the number of trials during testing.\n\n\n### Extra 3 (easy): Add instructions and goodbye screen\n\nA good experiment comes with clear instructions. And a polite experiment says goodbye to the participants when they are done. You can use a SKETCHPAD to do this.\n\n__Pro-tip:__ A FORM_TEXT_DISPLAY is not compatible with OSWeb, so you should not use it for instructions if you want to run your experiment online.\n\n\n### Extra 4 (medium): Set the correct response and matching rule through JavaScript\n\nTo include scripting in OSWeb, you can use the INLINE_JAVASCRIPT item, which supports JavaScript. (But it does not currently provide all the functionality that is offered by the regular Python INLINE_SCRIPT!)\n\nSo far, the matching rule is always to match by shape. To change this, add an INLINE_JAVASCRIPT item to the start of the experiment, and use the following script (in the *prepare* phase) to randomly set the variable `matching_rule` to 'shape', 'number', or 'color'.\n\n```javascript\nfunction choice(choices) {\n    // JavaScript does not have a built-in choice function, so we define it\n    // here.\n    let index = Math.floor(Math.random() * choices.length)\n    return choices[index]\n}\n\n\n// The vars object contains all experimental variables, like the var object\n// in Python inline script\nvars.matching_rule = choice(['shape', 'number', 'color'])\n```\n\nNow add another INLINE_JAVASCRIPT item to the start of the *trial_sequence*. In the *prepare* phase, add a script to set the `correct_response` variable to 'a', 'b', 'c', or 'd'. To do so, you need a series of `if` statements, that first look at the matching rule, and then look at which shape corresponds to the response shape (for the shape-matching rule) or which color corresponds to the response color (for the color-matching rule) etc.\n\nTo get started, here's part of the solution (but it needs to be completed!):\n\n```javascript\nif (vars.matching_rule === 'shape') {\n    if (vars.shape1 === vars.response_shape) vars.correct_response = 'a'\n    // Not done yet\n} // Not done yet": {
    "fr": "Enfin, ajoutez du contenu utile aux deux SKETCHPADs. Par exemple, pour *correct_feedback*, vous pouvez utiliser un point de fixation vert, et pour *incorrect_feedback*, vous pourriez utiliser un point de fixation rouge, dans les deux cas montré pendant 500 ms (c'est-à-dire régler la durée du SKETCHPAD sur 500). Les points colorés sont un moyen discret et agréable pour fournir des commentaires.\n\n### Étape 9 : Tester l'expérience\n\nVous avez maintenant créé une mise en œuvre de base (mais incomplète !) du Wisconsin Card Sorting Test. (Vous complèterez la mise en œuvre dans le cadre des missions supplémentaires ci-dessous.)\n\nPour tester l'expérience, cliquez sur le bouton quick-run (les deux flèches bleues) pour tester l'expérience sur le bureau (voir %FigRunButtons). Si l'expérience se déroule comme prévu sur le bureau, cliquez sur le bouton run-in-browser (la flèche à l'intérieur d'un cercle vert) pour tester l'expérience dans un navigateur.\n\n## Missions supplémentaires\n\n### Supplémentaire 1 (facile) : Ajouter un enregistreur\n\nOpenSesame ne journalise pas automatiquement les données. À la place, vous devez ajouter explicitement un élément `logger` à votre expérience. Dans une expérience basée sur des essais, un `logger` est généralement le dernier élément du *trial_sequence*, de sorte qu'il consigne toutes les données collectées pendant l'essai.\n\nActuellement, notre WCST ne consigne aucune donnée. Il est temps de corriger cela !\n\n### Supplémentaire 2 (facile) : Inspecter le fichier de données\n\n*Nécessite d'avoir complété le supplément 1*.\n\nDonnez à l'expérience un court test. Inspectez maintenant le fichier de journal dans un programme comme Excel, LibreOffice Calc ou JASP. Identifiez les variables pertinentes et réfléchissez à la manière d'analyser les résultats.\n\n__Astuce__: Réglez la valeur de répétition du *block_loop* sur 0,1 pour réduire le nombre d'essais lors des tests.\n\n### Supplémentaire 3 (facile) : Ajouter des instructions et un écran d'au revoir\n\nUne bonne expérience est accompagnée d'instructions claires. Et une expérience polie dit au revoir aux participants lorsqu'ils ont terminé. Vous pouvez utiliser un SKETCHPAD pour cela.\n\n__Astuce__: FORM_TEXT_DISPLAY n'est pas compatible avec OSWeb, vous ne devez donc pas l'utiliser pour les instructions si vous voulez exécuter votre expérience en ligne.\n\n### Supplémentaire 4 (moyen): Définir la réponse correcte et la règle de correspondance avec JavaScript\n\nPour inclure des scripts dans OSWeb, vous pouvez utiliser l'élément INLINE_JAVASCRIPT, qui prend en charge JavaScript. (Mais il n'offre pas actuellement toutes les fonctionnalités offertes par le INLINE_SCRIPT Python habituel !)\n\nJusqu'à présent, la règle de correspondance est toujours de faire correspondre par forme. Pour changer cela, ajoutez un élément INLINE_JAVASCRIPT au début de l'expérience et utilisez le script suivant (dans la phase *préparer*) pour définir aléatoirement la variable `matching_rule` sur 'forme', 'nombre' ou 'couleur'.\n\n```javascript\nfunction choix(choix) {\n    // JavaScript n'a pas de fonction de choix intégrée, nous la définissons\n    // ici.\n    let index = Math.floor(Math.random() * choix.length)\n    return choix[index]\n}\n\n// L'objet vars contient toutes les variables expérimentales, comme l'objet var\n// dans le script Python en ligne\nvars.matching_rule = choix(['shape', 'number', 'color'])\n```\n\nAjoutez maintenant un autre élément INLINE_JAVASCRIPT au début du *trial_sequence*. Dans la phase *préparer*, ajoutez un script pour définir la variable `correct_response` sur 'a', 'b', 'c' ou 'd'. Pour ce faire, vous avez besoin d'une série d'instructions `if` qui regardent d'abord la règle de correspondance, puis examinent quelle forme correspond à la forme de réponse (pour la règle de correspondance de forme) ou quelle couleur correspond à la couleur de réponse (pour la règle de correspondance de couleur) etc.\n\nPour commencer, voici une partie de la solution (mais elle doit être complétée !):\n\n```javascript\nif (vars.matching_rule === 'shape') {\n    if (vars.shape1 === vars.response_shape) vars.correct_response = 'a'\n    // Pas encore terminé\n} // Pas encore terminé"
  },
  "// Let's print some info to the debug window\nconsole.log('matching_rule = ' + vars.matching_rule)\nconsole.log('correct_response = ' + vars.correct_response)\n```\n\n\n### Extra 5 (difficult): Periodically change the matching rule\n\nSo far, the matching rule is randomly determined at the start of the experiment, but then remains constant throughout the experiment. In a real WCST, the matching rule changes periodically, typically after the participant has made a fixed number of correct responses.\n\nTo implement this, you need another INLINE_JAVASCRIPT. Here are some pointers to get started:\n\n- Use a counter variable that increments by 1 after a correct response, and is reset to 0 after an incorrect response.\n- When changing the matching rule, make sure that it is not (by coincidence) set to the same matching rule again.\n\n\n### Extra 6 (really difficult): Constrain the response card\n\nRight now, the response card can overlap with a stimulus card on multiple dimensions. For example, if one of the stimulus cards is a single blue circle, the response card might be two blue circles, thus overlapping on both color and shape. In a real WCST, the response card should overlap with each stimulus card on no more than a dimension.\n\nThis one is up to you. No pointers this time!\n\n\n### Extra 7 (easy): Running the experiment online with JATOS\n\nOur WCST is compatible with OSWeb, which means that you can run it in a browser. To test if this still works, you can click on the run-in-browser button in OpenSesame.\n\nHowever, to collect actual data with the experiment in a browser, you need to import the experiment into JATOS, and use JATOS to generate a link that you can distribute to your participants. This is much easier than it sounds! For more information, see:\n\n- %link:manual/osweb/workflow%\n\n\n## Solutions\n\nYou can download the full experiment, including the solutions to the extra assignments, here:\n\n- <https://osf.io/f5er2/>\n": {
    "fr": "// Affichons quelques informations dans la fenêtre de débogage\nconsole.log('règle_de_correspondance = ' + vars.matching_rule)\nconsole.log('réponse_correcte = ' + vars.correct_response)\n```\n\n\n### Supplément 5 (difficile) : Modifier périodiquement la règle de correspondance\n\nJusqu'à présent, la règle de correspondance est déterminée au hasard au début de l'expérience, mais reste constante tout au long de l'expérience. Dans un vrai WCST, la règle de correspondance change périodiquement, généralement après que le participant a fait un nombre fixe de réponses correctes.\n\nPour cela, vous avez besson d'un autre INLINE_JAVASCRIPT. Voici quelques conseils pour commencer :\n\n- Utilisez une variable de compteur qui s'incrémente de 1 après une réponse correcte et qui est réinitialisée à 0 après une réponse incorrecte.\n- Lorsque vous changez la règle de correspondance, assurez-vous qu'elle n'est pas (par hasard) réglée à nouveau sur la même règle de correspondance.\n\n\n### Supplément 6 (vraiment difficile) : Restreindre la carte de réponse\n\nActuellement, la carte de réponse peut se chevaucher avec une carte de stimulus sur plusieurs dimensions. Par exemple, si l'une des cartes de stimulus est un cercle bleu unique, la carte de réponse peut être composée de deux cercles bleus, se chevauchant ainsi sur la couleur et la forme. Dans un vrai WCST, la carte de réponse ne doit se chevaucher avec chaque carte de stimulus que sur une dimension au maximum.\n\nCelui-ci est à votre charge. Pas de conseils cette fois!\n\n\n### Supplément 7 (facile) : Exécuter l'expérience en ligne avec JATOS\n\nNotre WCST est compatible avec OSWeb, ce qui signifie que vous pouvez l'exécuter dans un navigateur. Pour tester si cela fonctionne toujours, vous pouvez cliquer sur le bouton run-in-browser dans OpenSesame.\n\nCependant, pour recueillir des données réelles avec l'expérience dans un navigateur, vous devez importer l'expérience dans JATOS et utiliser JATOS pour générer un lien que vous pouvez distribuer à vos participants. C'est beaucoup plus facile que cela en a l'air ! Pour plus d'informations, consultez :\n\n- %link:manual/osweb/workflow%\n\n## Solutions\n\nVous pouvez télécharger l'expérience complète, y compris les solutions aux tâches supplémentaires, ici :\n\n- <https://osf.io/f5er2/>"
  },
  "Geneva 2018 workshop": {
    "fr": "Atelier Genève 2018"
  },
  "<notranslate>[TOC]</notranslate>\n\n\n## About the workshop\n\nThis OpenSesame workshop will take place at the University of Geneva on March 27, 2018.\n\nThe workshop consisted of two main parts. In the first part, corresponding to the Tutorial below, we created a complete experiment together. In the second part, corresponding to the Extra assignments below, the workshop participants improved this experiment by themselves, based on a few suggestions.\n\nYou can download the full experiment, including the solutions of the extra assignments here:\n\n- <http://osf.io/jw7dr>\n\n\n## The tutorial\n\n<notranslate>\nfigure:\n id: FigMeowingCapybara\n source: meowing-capybara.png\n caption: |\n  Don't be fooled by meowing capybaras! ([Source][capybara_photo])\n</notranslate>\n\n<notranslate>[TOC]</notranslate>\n\nWe will create a simple animal-filled multisensory integration task, in which participants see a picture of a dog, cat, or capybara. A meow or a bark is played while the picture is shown. The participant reports whether a dog or a cat is shown, by pressing the right or the left key. No response should be given when a capybara is shown: those are catch trials.\n\nTo make things more fun, we will design the experiment so that you can run it on [OSWeb](http://osweb.cogsci.nl/), an online runtime for OpenSesame experiments (which is still a work in progress, but it works for basic experiments).\n\nWe make two simple predictions:\n\n- Participants should be faster to identify dogs when a barking sound is played, and faster to identify cats when a meowing sound is played. In other words, we expect a multisensory congruency effect.\n- When participants see a capybara, they are more likely to report seeing a dog when they hear a bark, and more likely to report seeing a cat when they hear a meow. In other words, false alarms are biased by the sound.\n\n\n### Step 1: Download and start OpenSesame\n\nOpenSesame is available for Windows, Linux, Mac OS, and Android (runtime only). This tutorial is written for OpenSesame 3.1.X, and you can use either the version based on Python 2.7 (default) or Python 3.5. You can download OpenSesame from here:\n\n- %link:download%\n\nWhen you start OpenSesame, you will be given a choice of template experiments, and (if any) a list of recently opened experiments (see %FigStartUp).\n\n<notranslate>\nfigure:\n id: FigStartUp\n source: start-up.png\n caption: |\n  The OpenSesame window on start-up.\n</notranslate>\n\nThe *Extended template* provides a good starting point for creating Android-based experiments. However, in this tutorial we will create the entire experiment from scratch. Therefore, we will continue with the 'default template', which is already loaded when OpenSesame is launched (%FigDefaultTemplate). Therefore, simply close the 'Get started!' and (if shown) 'Welcome!' tabs.\n\n<notranslate>\nfigure:\n id: FigDefaultTemplate\n source: default-template.png\n caption: |\n  The structure of the 'Default template' as seen in the overview area.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Background box 1: Basics**\n\nOpenSesame experiments are collections of *items*. An item is a small chunk of functionality that, for example, can be used to present visual stimuli (the SKETCHPAD item) or to record key presses (the KEYBOARD_RESPONSE item). Items have a type and a name. For example, you might have two items of the type KEYBOARD_RESPONSE with the names *t1_response* and *t2_response*. To make the distinction between item types and item names clear, we will use THIS_STYLE for types, and *this style* for names.\n\nTo give structure to your experiment, two types of items are especially important: the LOOP and the SEQUENCE. Understanding how you can combine LOOPs and SEQUENCEs to build experiments is perhaps the trickiest part of working with OpenSesame, so let's get that out of the way first.": {
    "fr": "## À propos de l'atelier\n\nCet atelier OpenSesame aura lieu à l'Université de Genève le 27 mars 2018.\n\nL'atelier se composait de deux parties principales. Dans la première partie, correspondant au tutoriel ci-dessous, nous avons créé une expérience complète ensemble. Dans la deuxième partie, correspondant aux missions supplémentaires ci-dessous, les participants de l'atelier ont amélioré cette expérience par eux-mêmes, sur la base de quelques suggestions.\n\nVous pouvez télécharger l'expérience complète, y compris les solutions des missions supplémentaires ici :\n\n- <http://osf.io/jw7dr>\n\n## Le tutoriel\n\nfigure :\n id : FigMeowingCapybara\n source : meowing-capybara.png\n légende : |\n   Ne vous laissez pas duper par les capybaras qui miaulent ! ([Source][capybara_photo])\n\nNous allons créer une simple tâche d'intégration multisensorielle remplie d'animaux, dans laquelle les participants voient une image d'un chien, d'un chat ou d'un capybara. Un miaulement ou un aboiement est joué pendant que l'image est affichée. Le participant signale si un chien ou un chat est montré, en appuyant sur la touche droite ou gauche. Aucune réponse ne doit être donnée lorsqu'un capybara est montré : il s'agit d'essais surprises.\n\nPour rendre les choses plus amusantes, nous allons concevoir l'expérience de manière à ce que vous puissiez la réaliser sur [OSWeb](http://osweb.cogsci.nl/), un environnement d'exécution en ligne pour les expériences OpenSesame (qui est encore en cours de développement, mais fonctionne pour les expériences basiques).\n\nNous faisons deux prédictions simples :\n\n- Les participants devraient être plus rapides pour identifier les chiens lorsqu'un bruit d'aboiement est joué et plus rapides pour identifier les chats lorsqu'un bruit de miaulement est joué. En d'autres termes, nous nous attendons à un effet de congruence multisensorielle.\n- Lorsque les participants voient un capybara, ils sont plus susceptibles de signaler qu'ils voient un chien lorsqu'ils entendent un aboiement et plus susceptibles de signaler qu'ils voient un chat lorsqu'ils entendent un miaulement. En d'autres termes, les fausses alertes sont biaisées par le son.\n\n### Étape 1 : Téléchargez et démarrez OpenSesame\n\nOpenSesame est disponible pour Windows, Linux, Mac OS et Android (exécution uniquement). Ce tutoriel est écrit pour OpenSesame 3.1.X, et vous pouvez utiliser la version basée sur Python 2.7 (par défaut) ou Python 3.5. Vous pouvez télécharger OpenSesame ici :\n\n- %link:download%\n\nLorsque vous démarrez OpenSesame, vous aurez le choix entre des modèles d'expériences et, le cas échéant, une liste d'expériences récemment ouvertes (voir %FigStartUp).\n\nfigure :\n id : FigStartUp\n source : start-up.png\n légende : |\n   La fenêtre OpenSesame au démarrage.\n\nLe *modèle étendu* offre un bon point de départ pour créer des expériences basées sur Android. Cependant, dans ce tutoriel, nous créerons l'ensemble de l'expérience à partir de zéro. Par conséquent, nous continuerons avec le \"modèle par défaut\", qui est déjà chargé lorsque OpenSesame est lancé (%FigDefaultTemplate). Fermez simplement les onglets \"Get started !\" et (si affiché) \"Welcome!\".\n\nfigure :\n id : FigDefaultTemplate\n source : default-template.png\n légende : |\n   La structure du modèle \"Default template\" telle qu'elle apparaît dans la zone d'aperçu.\n\n<div class='info-box' markdown='1'>\n\n**Encadré 1 : Les bases**\n\nLes expériences OpenSesame sont des collections d'éléments. Un élément est un petit morceau de fonctionnalité qui, par exemple, peut être utilisé pour présenter des stimuli visuels (l'élément SKETCHPAD) ou pour enregistrer des pressions de touches (l'élément KEYBOARD_RESPONSE). Les éléments ont un type et un nom. Par exemple, vous pouvez avoir deux éléments du type KEYBOARD_RESPONSE avec les noms *t1_response* et *t2_response*. Pour distinguer clairement les types et les noms d'éléments, nous utiliserons CE_STYLE pour les types et *ce style* pour les noms.\n\nPour donner une structure à votre expérience, deux types d'éléments sont particulièrement importants : la LOOP et la SEQUENCE. Comprendre comment combiner les LOOPs et les SEQUENCEs pour construire des expériences est peut-être la partie la plus délicate du travail avec OpenSesame, alors abordons ce sujet en premier."
  },
  "<notranslate>\nfigure:\n id: FigStep14\n source: step14.png\n caption: |\n  The *form_instructions* item at the end of Step 13.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Background box 10: Text**\n\n__Tip__ -- Forms, and text more generally, support a subset of HTML tags to allow for text formatting (i.e. colors, boldface, etc.). This is described here:\n\n- %link:visual%\n\n</div>\n\n### Step 15: Finished!\n\nYour experiment is now finished! Click on the 'Run fullscreen' (`Control+R`) button in the main toolbar to give it a test run.\n\n<div class='info-box' markdown='1'>\n\n**Background box 11: Quick run**\n\n__Tip__ — A test run is executed even faster by clicking the orange 'Run in window' button, which doesn't ask you how to save the logfile (and should therefore only be used for testing purposes).\n\n</div>\n\n\n## Extra assignments\n\nThe solutions to these extra assingments can be found in the [experiment file](http://osf.io/jw7dr).\n\n### Extra 1: Add an instruction and goodbye screen\n\nTips:\n\n- SKETCHPAD and FORM_TEXT_DISPLAY can present text\n- Good instructions are brief and concrete\n\n### Extra 2: Analyze the data and check the timing\n\nTips:\n\n- Run the experiment once on yourself\n- Open the data file in Excel, LibreOffice, or JASP\n- You can check the presentation time of `sketchpad`s through the `time_[item name]` variables.\n\n### Extra 3: Divide the trials into multiple blocks\n\nTips:\n\n- Use a break-if statement to break the loop after (say) 15 trials: `([count_trial_sequence]+1) % 15 = 0`\n- Add a new LOOP-SEQUENCE structure above the *block_loop* to repeat a block of trials multiple times\n- Disable the 'Evaluate on first cycle' option in the *block_loop* so that the break-if statement isn't evaluated when the `count_trial_sequence` variable doesn't yet exist\n- Enable the 'Resume after break' option in the *block_loop* to randomly sample without replacement from the LOOP table\n\n### Extra 4: Add accuracy and average response time feedback after every block\n\nFirst do Extra 3!\n\nTips:\n\n- Use a FEEDBACK item to provide feedback\n- The variables `acc` and `avg_rt` contain the running accuracy and average reaction time\n\n### Extra 5: Counterbalance the response rule\n\nTips:\n\n- The variable `subject_parity` is 'even' or 'odd'\n- This requires a simple INLINE_SCRIPT\n- Make sure that the instructions match the response rule!\n\n\n## References\n\nMathôt, S., Schreij, D., & Theeuwes, J. (2012). OpenSesame: An open-source, graphical experiment builder for the social sciences. *Behavior Research Methods*, *44*(2), 314–324. doi:10.3758/s13428-011-0168-7\n{: .reference}\n\n[OpenSesame runtime for Android]: /getting-opensesame/android\n[slides]: /attachments/rovereto2014-workshop-slides.pdf\n[modulo]: http://en.wikipedia.org/wiki/Modulo_operation\n[pdf]: /rovereto2014/index.pdf\n[gimp]: http://www.gimp.org/\n[capybara_photo]: https://commons.wikimedia.org/wiki/File:Capybara_Hattiesburg_Zoo_(70909b-58)_2560x1600.jpg\n": {
    "fr": "<notranslate>\nfigure:\n id: FigStep14\n source: step14.png\n caption: |\n  L'élément *form_instructions* à la fin de l'étape 13.\n</notranslate>\n\n<div class='info-box' markdown='1'>\n\n**Encadré 10 : Texte**\n\n__Astuce__ -- Les formulaires et le texte en général, supportent un sous-ensemble de balises HTML pour permettre la mise en forme du texte (c'est-à-dire les couleurs, le gras, etc.). Ceci est décrit ici :\n\n- %link:visual%\n\n</div>\n\n### Étape 15 : Terminé !\n\nVotre expérience est maintenant terminée ! Cliquez sur le bouton \"Lancer en plein écran\" (`Control+R`) dans la barre d'outils principale pour faire un essai.\n\n<div class='info-box' markdown='1'>\n\n**Encadré 11 : Lancement rapide**\n\n__Astuce__ — Un essai est exécuté encore plus rapidement en cliquant sur le bouton orange \"Lancer dans une fenêtre\", qui ne vous demande pas comment enregistrer le fichier journal (et ne doit donc être utilisé qu'à des fins de test).\n\n</div>\n\n\n## Travaux pratiques supplémentaires\n\nLes solutions de ces travaux pratiques supplémentaires se trouvent dans le [fichier d'expérience](http://osf.io/jw7dr).\n\n### Supplément 1 : Ajouter un écran d'instruction et d'au revoir\n\nConseils :\n\n- SKETCHPAD et FORM_TEXT_DISPLAY peuvent présenter du texte\n- Les bonnes instructions sont brèves et concrètes\n\n### Supplément 2 : Analyser les données et vérifier le timing\n\nConseils :\n\n- Lancez l'expérience une fois sur vous-même\n- Ouvrez le fichier de données dans Excel, LibreOffice, ou JASP\n- Vous pouvez vérifier le temps de présentation des `sketchpad`s grâce aux variables `time_[nom de l'élément]`.\n\n### Supplément 3 : Diviser les essais en plusieurs blocs\n\nConseils :\n\n- Utilisez une instruction break-if pour interrompre la boucle après (disons) 15 essais : `([count_trial_sequence]+1) % 15 = 0`\n- Ajoutez une nouvelle structure LOOP-SEQUENCE au-dessus de la *block_loop* pour répéter un bloc d'essais plusieurs fois\n- Désactivez l'option \"Évaluer lors du premier cycle\" dans la *block_loop* afin que l'instruction break-if ne soit pas évaluée lorsque la variable `count_trial_sequence` n'existe pas encore\n- Activez l'option \"Reprendre après la pause\" dans la *block_loop* pour échantillonner aléatoirement sans remplacement dans la table LOOP\n\n### Supplément 4 : Ajouter un retour d'information sur la précision et le temps de réponse moyen après chaque bloc\n\nFaites d'abord le supplément 3 !\n\nConseils :\n\n- Utilisez un élément FEEDBACK pour fournir des informations\n- Les variables `acc` et `avg_rt` contiennent la précision et le temps de réaction moyen en cours\n\n### Supplément 5 : Contrebalancer la règle de réponse\n\nConseils :\n\n- La variable `subject_parity` est 'even' ou 'odd'\n- Ceci nécessite un simple INLINE_SCRIPT\n- Assurez-vous que les instructions correspondent à la règle de réponse !\n\n## Références\n\nMathôt, S., Schreij, D., & Theeuwes, J. (2012). OpenSesame: An open-source, graphical experiment builder for the social sciences. *Behavior Research Methods*, *44*(2), 314–324. doi:10.3758/s13428-011-0168-7\n{: .reference}\n\n[OpenSesame runtime pour Android]: /getting-opensesame/android\n[diapositives]: /attachments/rovereto2014-workshop-slides.pdf\n[modulo]: http://fr.wikipedia.org/wiki/Opération_modulo\n[pdf]: /rovereto2014/index.pdf\n[gimp]: http://www.gimp.org/\n[capybara_photo]: https://commons.wikimedia.org/wiki/File:Capybara_Hattiesburg_Zoo_(70909b-58)_2560x1600.jpg"
  },
  "Professional support": {
    "fr": "Support professionnel"
  },
  "\nWe offer professional (paid) support for the following:\n\n- Implementation of experiments with OpenSesame\n- Analysis of experimental data\n- Organization of workshops on OpenSesame and/ or Python\n\nFor a quote or more information, please send an email to <professional@cogsci.nl>. To be helped quickly, please provide a clear and detailed description of what you are looking for.\n\nFree support is available through the community forum at <https://forum.cogsci.nl/>.\n": {
    "fr": "Nous offrons un soutien professionnel (payant) pour les éléments suivants :\n\n- Mise en œuvre d'expériences avec OpenSesame\n- Analyse des données expérimentales\n- Organisation d'ateliers sur OpenSesame et/ou Python\n\nPour un devis ou plus d'informations, veuillez envoyer un e-mail à <professional@cogsci.nl>. Pour être aidé rapidement, veuillez fournir une description claire et détaillée de ce que vous recherchez.\n\nUn soutien gratuit est disponible via le forum communautaire sur <https://forum.cogsci.nl/>."
  },
  "Important changes in OpenSesame 3": {
    "fr": "Changements importants dans OpenSesame 3"
  },
  "<notranslate>[TOC]</notranslate>\n\n\n## Changes in 3.3\n\nOpenSesame 3.3 several major improvements that make it even easier to develop experiments. OpenSesame 3.3 is fully backwards compatible with 3.2.\n\n\n### Rapunzel: a new code editor\n\nRapunzel is a code editor, focused on numerical computing with Python and R. Technically, Rapunzel is a set of extensions for OpenSesame. But it looks and behaves as a standalone program. Happy coding!\n\n- <https://rapunzel.cogsci.nl/>\n\n\n### A new inline_script editor\n\nRelated to the development of Rapuznel: The INLINE_SCRIPT item now uses a different library (`PyQode`) for the code editor. As a result, the code editor now supports many of the features that you would expect from a modern code editor, including code introspection and static code checking.\n\n\n### More color spaces\n\nOpenSesame now natively supports the HSV, HSL, and CIElab color spaces.\n\n- %link:manual/python/canvas%\n\n\n### New sound backend based on PsychoPy\n\nThe default backend is now *psycho*. One of the advantages of this backend is that the timing of sound presentation should be better. If you experience stuttering (clicky sound playback), you can still fall back to the the *psycho_legacy* backend, which uses the old PyGame-based sound system.\n\n\n### Support for inline_script items in coroutines\n\nYou can now use `inline_script` items in `coroutines`. This makes it easier to combine Python scripting with coroutines, as compared to the old method of writing a custom generator function.\n\n- %link:coroutines%\n\n\n\n### OpenSesame: \n\n\n## Changes in 3.2\n\nOpenSesame 3.2 brings several major improvements that make it even easier to develop experiments. OpenSesame 3.2 is fully backwards compatible with 3.1.\n\n\n### A better, PEP-8-compliant Python API\n\nPEP-8 is a style guide for Python. Much modern Python software follows the PEP-8 guidelines—but, for historical reasons, OpenSesame didn't. As of 3.2, the public API now follows the guideline that the names of classes (and factory functions that generate classes) should be `CamelCase`, while names of objects and functions should be `underscore_case`. Practically speaking, this means that you now create `Canvas` object as follows:\n\n~~~ .python\nmy_canvas = Canvas() # Note the capital C!\nmy_canvas.fixdot()\nmy_canvas.show())\n~~~\n\nOf course, the old `underscore_case` names are still available as aliases, so backwards compatibility is preserved.\n\nThe API for forms has also been simplified. You no longer need to import `libopensesame.widgets`, and you no longer need to pass `exp` as the first argument:\n\n~~~ .python\nform = Form()\nbutton = Button(text=u'Ok!')\nform.set_widget(button, (0, 0))\nform._exec()\n~~~\n\n\n### Improvements to the sketchpad and Canvas\n\n#### Access and modify Canvas elements\n\nElements of a `Canvas` are now objects that can be named, accessed, and modified. This means that you no longer need to redraw an entire canvas to change a single element. For example, you can draw a rotating arm as follows:\n\n~~~ .python\nmy_canvas = Canvas()\nmy_canvas['arm'] = Line(0, 0, 0, 0)\nfor x, y in xy_circle(n=100, rho=100):\n\tmy_canvas['arm'].ex = x\n\tmy_canvas['arm'].ey = y\n\tmy_canvas.show()\n\tclock.sleep(10)\n~~~\n\nThe SKETCHPAD also allows you to name elements.\n\nFor more information, see:\n\n- %link:manual/python/canvas%\n\n\n#### Improved support for HTML and non-Latin script\n\nText is now rendered by Qt, which is a modern library (the same library that is also used for the graphical interface). This means that you can now use real HTML in your text. This also means that left-to-right script and other non-Latin scripts are rendered much better.\n\n\n#### Images can be rotated\n\nImages can now be rotated. This work both in SKETCHPAD items and `Canvas` objects.\n\n\n#### Work with polar coordinates": {
    "fr": "<notranslate>[TOC]</notranslate>\n\n\n## Changements dans la version 3.3\n\nOpenSesame 3.3 apporte plusieurs améliorations majeures qui facilitent encore plus le développement d'expériences. OpenSesame 3.3 est entièrement compatible avec la version 3.2.\n\n\n### Rapunzel : un nouvel éditeur de code\n\nRapunzel est un éditeur de code, axé sur le calcul numérique avec Python et R. Techniquement, Rapunzel est un ensemble d'extensions pour OpenSesame. Mais il ressemble et se comporte comme un programme autonome. Bon codage !\n\n- <https://rapunzel.cogsci.nl/>\n\n\n### Un nouvel éditeur de inline_script\n\nEn rapport avec le développement de Rapuznel : L'élément INLINE_SCRIPT utilise maintenant une autre bibliothèque (`PyQode`) pour l'éditeur de code. En conséquence, l'éditeur de code prend désormais en charge bon nombre des fonctionnalités que l'on attend d'un éditeur de code moderne, y compris l'introspection du code et la vérification statique du code.\n\n\n### Plus d'espaces de couleurs\n\nOpenSesame prend désormais en charge de manière native les espaces de couleurs HSV, HSL et CIElab.\n\n- %link:manuel/python/canvas%\n\n\n### Un nouveau backend sonore basé sur PsychoPy\n\nLe backend par défaut est désormais *psycho*. L'un des avantages de ce backend est que la synchronisation de la présentation du son devrait être meilleure. Si vous rencontrez des saccades (lecture de son désynchronisée), vous pouvez toujours revenir au backend *psycho_legacy*, qui utilise l'ancien système de sons basé sur PyGame.\n\n\n### Support des éléments inline_script dans les coroutines\n\nVous pouvez désormais utiliser des éléments `inline_script` dans les `coroutines`. Cela facilite la combinaison de scripts Python avec des coroutines, par rapport à l'ancienne méthode d'écriture d'une fonction de générateur personnalisée.\n\n- %link:coroutine%\n\n\n### OpenSesame : \n\n\n## Changements dans la version 3.2\n\nOpenSesame 3.2 apporte plusieurs améliorations majeures qui facilitent encore plus le développement d'expériences. OpenSesame 3.2 est entièrement compatible avec la version 3.1.\n\n### Une meilleure API Python, conforme à la PEP-8\n\nLa PEP-8 est un guide de style pour Python. Beaucoup de logiciels Python modernes suivent les directives PEP-8 - mais, pour des raisons historiques, OpenSesame ne le faisait pas. À partir de la version 3.2, l'API publique suit désormais la directive selon laquelle les noms de classes (et les fonctions d'usine qui génèrent des classes) doivent être en `CamelCase`, tandis que les noms d'objets et de fonctions doivent être en `underscore_case`. En pratique, cela signifie que vous créez maintenant un objet `Canvas` comme suit :\n\n~~~ .python\nmy_canvas = Canvas() # Notez le C majuscule !\nmy_canvas.fixdot()\nmy_canvas.show())\n~~~\n\nBien sûr, les anciens noms en `underscore_case` sont toujours disponibles en tant qu'alias, de sorte que la compatibilité ascendante est préservée.\n\nL'API pour les formulaires a également été simplifiée. Vous n'avez plus besoin d'importer `libopensesame.widgets`, et vous n'avez plus besoin de passer `exp` comme premier argument :\n\n~~~ .python\nform = Form()\nbutton = Button(text=u'Ok!')\nform.set_widget(button, (0, 0))\nform._exec()\n~~~\n\n\n### Améliorations du sketchpad et du Canvas\n\n#### Accéder et modifier les éléments du Canvas\n\nLes éléments d'un `Canvas` sont maintenant des objets qui peuvent être nommés, accessibles et modifiés. Cela signifie que vous n'avez plus besoin de redessiner un canvas entier pour changer un seul élément. Par exemple, vous pouvez dessiner un bras tournant comme suit :\n\n~~~ .python\nmy_canvas = Canvas()\nmy_canvas['arm'] = Line(0, 0, 0, 0)\nfor x, y in xy_circle(n=100, rho=100):\n\tmy_canvas['arm'].ex = x\n\tmy_canvas['arm'].ey = y\n\tmy_canvas.show()\n\tsleep(clock, 10)\n~~~\n\nLe SKETCHPAD permet également de nommer des éléments.\n\nPour plus d'informations, voir :\n\n- %link:manuel/python/canvas%\n\n\n#### Amélioration de la prise en charge du HTML et des scripts non latins\n\nLe texte est maintenant rendu par Qt, qui est une bibliothèque moderne (la même bibliothèque qui est également utilisée pour l'interface graphique). Cela signifie que vous pouvez maintenant utiliser du vrai HTML dans votre texte. Cela signifie également que les scripts de gauche à droite et d'autres scripts non latins sont rendus beaucoup mieux.\n\n\n#### Les images peuvent être tournées\n\nLes images peuvent maintenant être tournées. Cette fonction est valable à la fois pour les éléments SKETCHPAD et les objets `Canvas`.\n\n#### Travailler avec des coordonnées polaires"
  },
  "If you right-click on a SKETCHPAD elements, you can select 'Specify polar coordinates'. This allows you to calculate cartesian (x, y) coordinates based on polar coordinates, which is especially useful if you want to create circular configurations.\n\n\n### Form improvements\n\n#### Improved form performance\n\nForms are now much faster when using the *psycho* and *xpyriment* backends. This is due to the fact that `Canvas` elements can now be updated individually, as described above.\n\n\n#### Validation of form input\n\nYou can now validate the input of a form; that is, you can prevent a form from closing until certain criteria are met. In addition, you can exclude characters as input from `TextInput` widgets.\n\nFor more information, see:\n\n- %link:manual/forms/validation%\n\n\n### Keyboard Improvements\n\n#### Support for key-release events\n\nThe `Keyboard()` object now has a `get_key_release()` function, which allows you to collect key releases. Due to limitations of the underlying libraries, the function has two important limitations:\n\n- The returned `key` may be incorrect on non-QWERTY keyboard layouts\n- The function has not been implemented for the *psycho* backend\n\nFor more information, see:\n\n- %link:manual/response/keyboard%\n\n\n### Mouse Improvements\n\n#### Support for mouse-release events\n\nThe `Mouse()` object now has a `get_click_release()` function, which allows you to collect mouse-click releases. This function is currently not implemented for the *psycho* backend.\n\nFor more information, see:\n\n- %link:manual/response/mouse%\n\n#### Use sketchpads to define regions of interest\n\nYou can now define a linked SKETCHPAD in a `mouse_response` item. If you do this, the names of the elements on the SKETCHPAD will be automatically used as regions of interest (ROIs) for the mouse clicks.\n\n\n### Forcibly end your experiment\n\nYou can now forcibly end your experiment by clicking on the Kill button in the main toolbar. This means that you no longer need to open a process/ task manager to end run-away experiments!\n\n\n### Improved Mac OS support\n\nThe Mac OS packages have been rebuilt from the ground up by %-- github: {user: dschreij} --%. The Mac OS experience should now be much smoother, faster, and less crash-prone.\n\n\n### A Turkish translation\n\nA complete Turkish translation has been contributed by %-- github: {user: aytackarabay} --%. This means that OpenSesame is now fully translated into French, German, and Turkish. A partial translation is available in several other languages.\n\n\n## Changes in 3.1\n\nOpenSesame 3.1 brings many improvements that make it even easier to develop experiments. OpenSesame 3.1 is fully backwards compatible with 3.0.\n\n### A new look!\n\nOpenSesame has a new icon theme, based on [Moka](https://snwh.org/moka) by Sam Hewitt. In addition, the user interface has been redesigned based on consistent human-interface guidelines. We hope you like the new look as much as we do!\n\n### A redesigned loop\n\nThe LOOP is now easier to use, and allows you to constrain randomization; this makes it possible, for example, to prevent the same stimulus from occurring twice in a row.\n\nFor more information, see:\n\n- %link:loop%\n\n### Coroutines: doing things in parallel\n\nThe COROUTINES plugin is now included by default. COROUTINES allows you to run multiple other items in parallel; this makes it possible, for example, to continuously collect key presses while presenting a series of SKETCHPADs.\n\nFor more information, see:\n\n- %link:coroutines%\n\n### Open Science Framework integration\n\nYou can now log into the [Open Science Framework](http://osf.io) (OSF) from within OpenSesame, and effortlessly synchronize experiments and data between your computer and the OSF. Thanks to the [Center for Open Science](http://cos.io/) for supporting this functionality!\n\nFor more information, see:\n\n- %link:osf%\n\n### A responses object\n\nThere is a new standard Python object: `responses`. This keeps track of all responses that have been collected during the experiment.": {
    "fr": "Si vous faites un clic droit sur un élément SKETCHPAD, vous pouvez sélectionner 'Spécifier les coordonnées polaires'. Cela vous permet de calculer les coordonnées cartésiennes (x, y) à partir des coordonnées polaires, ce qui est particulièrement utile si vous souhaitez créer des configurations circulaires.\n\n### Améliorations des formulaires\n\n#### Performance améliorée des formulaires\n\nLes formulaires sont désormais beaucoup plus rapides lors de l'utilisation des backends *psycho* et *xpyriment*. Ceci est dû au fait que les éléments `Canvas` peuvent maintenant être mis à jour individuellement, comme décrit ci-dessus.\n\n#### Validation de l'entrée du formulaire\n\nVous pouvez désormais valider l'entrée d'un formulaire, c'est-à-dire empêcher la fermeture d'un formulaire tant que certains critères ne sont pas respectés. De plus, vous pouvez exclure des caractères en tant qu'entrée des widgets `TextInput`.\n\nPour plus d'informations, voir:\n\n- %link:manual/forms/validation%\n\n### Améliorations clavier\n\n#### Support des événements de relâchement de touches\n\nL'objet `Keyboard()` dispose désormais d'une fonction `get_key_release()`, qui vous permet de récupérer les relâchements de touches. En raison des limitations des bibliothèques sous-jacentes, cette fonction présente deux limitations importantes :\n\n- La `key` retournée peut être incorrecte sur les dispositions de clavier autres que QWERTY\n- La fonction n'a pas été mise en œuvre pour le backend *psycho*\n\nPour plus d'informations, voir:\n\n- %link:manual/response/keyboard%\n\n### Améliorations de la souris\n\n#### Support des événements de relâchement des clics de la souris\n\nL'objet `Mouse()` dispose désormais d'une fonction `get_click_release()`, qui vous permet de récupérer les relâchements des clics de souris. Cette fonction n'est actuellement pas mise en œuvre pour le backend *psycho*.\n\nPour plus d'informations, voir:\n\n- %link:manual/response/mouse%\n\n#### Utiliser des sketchpads pour définir des zones d'intérêt\n\nVous pouvez maintenant définir un SKETCHPAD lié dans un élément `mouse_response`. Si vous faites cela, les noms des éléments sur le SKETCHPAD seront automatiquement utilisés comme zones d'intérêt (ROIs) pour les clics de souris.\n\n### Mettre fin à votre expérience de force\n\nVous pouvez désormais mettre fin de force à votre expérience en cliquant sur le bouton Kill dans la barre d'outils principale. Cela signifie que vous n'avez plus besoin d'ouvrir un gestionnaire de processus/tâches pour mettre fin aux expériences incontrôlables !\n\n### Support amélioré pour Mac OS\n\nLes packages Mac OS ont été entièrement reconstruits par %-- github: {user: dschreij} --%. L'expérience Mac OS devrait désormais être bien plus fluide, rapide et moins sujette aux plantages.\n\n### Une traduction turque\n\nUne traduction turque complète a été apportée par %-- github: {user: aytackarabay} --%. Cela signifie qu'OpenSesame est maintenant entièrement traduit en français, allemand et turc. Une traduction partielle est disponible dans plusieurs autres langues.\n\n## Changements dans la version 3.1\n\nOpenSesame 3.1 apporte de nombreuses améliorations qui rendent encore plus facile le développement d'expériences. OpenSesame 3.1 est entièrement compatible avec la version 3.0.\n\n### Un nouveau look !\n\nOpenSesame a un nouveau thème d'icônes, basé sur [Moka](https://snwh.org/moka) de Sam Hewitt. De plus, l'interface utilisateur a été repensée sur la base de directives d'interface humaine cohérentes. Nous espérons que vous aimerez ce nouveau look autant que nous !\n\n### Une boucle redessinée\n\nLa LOOP est désormais plus facile à utiliser et vous permet de contraindre l'aléa; cela permet, par exemple, d'empêcher qu'un même stimulus se répète deux fois d'affilée.\n\nPour plus d'informations, voir:\n\n- %link:loop%\n\n### Coroutines: faire les choses en parallèle\n\nLe plugin COROUTINES est désormais inclus par défaut. Les COROUTINES vous permettent d'exécuter plusieurs autres éléments en parallèle ; cela permet, par exemple, de collecter en continu des pressions de touches tout en présentant une série de SKETCHPADs.\n\nPour plus d'informations, voir:\n\n- %link:coroutines%\n\n### Intégration du cadre scientifique ouvert\n\nVous pouvez désormais vous connecter à [Open Science Framework](http://osf.io) (OSF) depuis OpenSesame, et synchroniser sans effort les expériences et les données entre votre ordinateur et l'OSF. Merci au [Center for Open Science](http://cos.io/) pour le soutien de cette fonctionnalité!\n\nPour plus d'informations, voir:\n\n- %link:osf%\n\n### Un objet de réponses \n\nIl y a un nouvel objet Python standard : `responses`. Celui-ci garde une trace de toutes les réponses collectées au cours de l'expérience."
  },
  "For more information, see:\n\n- %link:responses%\n\n## Changes in 3.0\n\nOpenSesame 3.0 has brought many improvements that make it even easier to develop experiments. Most changes are backwards compatible. That is, you can still do things the old way. However, a handful of changes are backwards incompatible, and it's important to be aware of those.\n\n### Backwards incompatible changes\n\n#### Sampler properties\n\nThe SAMPLER object has a number of properties that were previously functions. This concerns:\n\n- `sampler.fade_in`\n- `sampler.pan`\n- `sampler.pitch`\n- `sampler.volume`\n\nFor more information, see:\n\n- %link:sampler%\n\n#### CSS3-compatible colors\n\nYou can now use CSS3-compatible color specifications, as described here:\n\n- %link:manual/python/canvas%\n\nIf you use color names (e.g. 'red', 'green', etc.), this may result in slightly different colors. For example, according to CSS3, 'green' is `#008000` instead (as was the case previously) of `#00FF00`.\n\n### New file format (.osexp)\n\nOpenSesame now saves experiments in `.osexp` format. Of course, you can still open the old formats (`.opensesame` and `.opensesame.tar.gz`). For more information, see:\n\n- %link:fileformat%\n\n### Simplified Python API\n\n#### No more self and exp\n\nIt is no longer necessary to prefix `self.` or `exp.` when calling commonly used functions. For example, this will programmatically set the subject number to 2:\n\n~~~ .python\nset_subject_nr(2)\n~~~\n\nFor a list of common functions, see:\n\n- %link:manual/python/common%\n\n#### The `var` object: Easy getting and setting of experimental variables\n\nThe old way of using `self.get()` to get, and `exp.set()` to set experimental variables has been replaced by a simpler syntax. For example, to set the variable `condition`, so that you can refer to it as `[condition]` in SKETCHPADs, etc.:\n\n~~~ .python\nvar.condition = 'easy`'\n~~~\n\nAnd to get an experimental variable `condition` that was, for example, defined in a LOOP:\n\n~~~ .python\nprint('Condition is %s' % var.condition)\n~~~\n\nFor more information, see:\n\n- %link:var%\n\n#### The `clock` object: Time functions\n\nTime functions are now available through the `clock` object:\n\n~~~ .python\nprint('Current timestamp: %s' % clock.time())\nclock.sleep(1000) # Sleep for 1 s\n~~~\n\nFor more information, see:\n\n- %link:clock%\n\n#### The `pool` object: Accessing the file pool\n\nThe file pool is now accessible through the `pool` object, which supports a `dict`-like interface (but is not really a Python `dict`):\n\n~~~ .python\npath = pool['image.png']\nprint('The full path to image.png is: %s' % path)\n~~~\n\nFor more information, see:\n\n- %link:pool%\n\n#### No more from openexp.* import *\n\nIt is no longer necessary to import `openexp` classes, and to pass `exp` as the first argument. Instead, to create a `canvas` object, you can simply do:\n\n~~~ .python\nmy_canvas = canvas()\n~~~\n\nThere are similar factory functions (as these are called) for `keyboard`, `mouse`, and SAMPLER.\n\nFor more information, see:\n\n- %link:manual/python/common%\n\n#### The synth is now a sampler\n\nThe SYNTH is no longer a class of its own. Instead, it's a function that returns a SAMPLER object that has been filled with a synthesized sample.\n\n### User-interface improvements\n\n#### An IPython debug window\n\nIPython, an interactive Python terminal for scientific computing, is now used for the debug window.\n\n#### A live variable inspector\n\nThe variable inspector now shows the actual values of your variables while your experiment is running, and after your experiment has finished.\n\n#### Undo\n\nYou can finally undo actions!\n\n#### A new color scheme\n\nThe default color scheme is now *Monokai*. Again a dark color scheme, but with a higher contrast than the previous default, *Solarized*. This increased should increase legibility. And it looks good!\n\n### Consistent coordinates": {
    "fr": "Pour plus d'informations, voir :\n\n- %link:responses%\n\n## Changements dans la version 3.0\n\nOpenSesame 3.0 a apporté de nombreuses améliorations qui rendent encore plus facile le développement d'expériences. La plupart des changements sont rétrocompatibles. En d'autres termes, vous pouvez toujours faire les choses de l'ancienne manière. Cependant, quelques changements ne sont pas rétrocompatibles, et il est important d'en être conscient.\n\n### Changements non rétrocompatibles\n\n#### Propriétés de Sampler\n\nL'objet SAMPLER a un certain nombre de propriétés qui étaient auparavant des fonctions. Cela concerne :\n\n- `sampler.fade_in`\n- `sampler.pan`\n- `sampler.pitch`\n- `sampler.volume`\n\nPour plus d'informations, voir :\n\n- %link:sampler%\n\n#### Couleurs compatibles CSS3\n\nVous pouvez désormais utiliser des spécifications de couleurs compatibles CSS3, comme décrit ici :\n\n- %link:manual/python/canvas%\n\nSi vous utilisez des noms de couleurs (par exemple 'red', 'green', etc.), cela peut entraîner des couleurs légèrement différentes. Par exemple, selon CSS3, 'green' est `#008000` au lieu de (comme c'était le cas auparavant) `#00FF00`.\n\n### Nouveau format de fichier (.osexp)\n\nOpenSesame enregistre désormais les expériences au format `.osexp`. Bien sûr, vous pouvez toujours ouvrir les anciens formats (`.opensesame` et `.opensesame.tar.gz`). Pour plus d'informations, voir :\n\n- %link:fileformat%\n\n### API Python simplifiée\n\n#### Plus de self et exp\n\nIl n'est plus nécessaire de préfixer `self.` ou `exp.` lors de l'appel de fonctions couramment utilisées. Par exemple, cela définira de manière programmatique le numéro de sujet à 2 :\n\n~~~ .python\nset_subject_nr(2)\n~~~\n\nPour une liste des fonctions courantes, voir :\n\n- %link:manual/python/common%\n\n#### L'objet `var` : accès et modification faciles des variables expérimentales\n\nL'ancienne façon d'utiliser `self.get()` pour obtenir et `exp.set()` pour définir des variables expérimentales a été remplacée par une syntaxe plus simple. Par exemple, pour définir la variable `condition`, afin de pouvoir la référencer en tant que `[condition]` dans les SKETCHPADs, etc. :\n\n~~~ .python\nvar.condition = 'easy`'\n~~~\n\nEt pour obtenir une variable expérimentale `condition` qui a été, par exemple, définie dans une LOOP :\n\n~~~ .python\nprint('Condition is %s' % var.condition)\n~~~\n\nPour plus d'informations, voir :\n\n- %link:var%\n\n#### L'objet `clock` : fonctions de temps\n\nLes fonctions de temps sont maintenant disponibles via l'objet `clock` :\n\n~~~ .python\nprint('Current timestamp: %s' % clock.time())\nclock.sleep(1000) # Sleep for 1 s\n~~~\n\nPour plus d'informations, voir :\n\n- %link:clock%\n\n#### L'objet `pool` : accès à la pool de fichiers\n\nLa pool de fichiers est maintenant accessible via l'objet `pool`, qui prend en charge une interface de type `dict` (mais qui n'est pas vraiment un `dict` Python) :\n\n~~~ .python\npath = pool['image.png']\nprint('The full path to image.png is: %s' % path)\n~~~\n\nPour plus d'informations, voir :\n\n- %link:pool%\n\n#### Plus de from openexp.* import *\n\nIl n'est plus nécessaire d'importer les classes `openexp`, et de passer `exp` en tant que premier argument. Au lieu de cela, pour créer un objet `canvas`, vous pouvez simplement faire :\n\n~~~ .python\nmy_canvas = canvas()\n~~~\n\nIl existe des fonctions similaires (comme on les appelle) pour `keyboard`, `mouse`, et SAMPLER.\n\nPour plus d'informations, voir :\n\n- %link:manual/python/common%\n\n#### Le synth est maintenant un échantillonneur\n\nLe SYNTH n'est plus une classe à part entière. Au lieu de cela, c'est une fonction qui retourne un objet SAMPLER qui a été rempli avec un échantillon synthétisé.\n\n### Améliorations de l'interface utilisateur\n\n#### Une fenêtre de débogage IPython\n\nIPython, un terminal Python interactif pour le calcul scientifique, est désormais utilisé pour la fenêtre de débogage.\n\n#### Un inspecteur de variables en direct\n\nL'inspecteur de variables affiche désormais les valeurs réelles de vos variables pendant l'exécution de votre expérience et après la fin de celle-ci.\n\n#### Annuler\n\nVous pouvez enfin annuler vos actions !\n\n#### Un nouveau schéma de couleurs\n\nLe schéma de couleurs par défaut est maintenant *Monokai*. Encore un schéma de couleurs sombres, mais avec un contraste plus élevé que le précédent par défaut, *Solarized*. Cette augmentation devrait augmenter la lisibilité. Et ça a l'air bien !\n\n### Coordonnées cohérentes"
  },
  "Previously, OpenSesame used mixed, inconsistent screen coordinates: `0,0` was the display top-left when using Python code, and the display center when working in SKETCHPAD items etc. As of 3.0, the display center is always `0,0`, also in Python code.\n\nIf you want to switch back to the old behavior, you can disable the 'Uniform coordinates' option in the general tab. For backwards compatibility, 'Uniform coordinates' are automatically disabled when you open an old experiment.\n\n### Using Python in text strings\n\nYou can now embed Python in text strings using the `[=...]` syntax. For example, the following text string in a SKETCHPAD:\n\n~~~\nTwo times two equals [=2*2]\n~~~\n\n... will show:\n\n~~~\nTwo times two equals 4\n~~~\n\nFor more information, see:\n\n- %link:text%\n\n### Support for Python 3\n\nOpenSesame now supports Python >= 3.4. However, many of OpenSesame's dependencies, notably PsychoPy and Expyriment, are Python 2-only. Therefore, Python 2.7 remains the default version of Python.\n": {
    "fr": "Précédemment, OpenSesame utilisait des coordonnées d'écran mixtes et incohérentes : `0,0` était en haut à gauche de l'affichage lors de l'utilisation du code Python, et au centre de l'affichage lors de l'utilisation des éléments SKETCHPAD, etc. À partir de la version 3.0, le centre de l'affichage est toujours `0,0`, également dans le code Python.\n\nSi vous souhaitez revenir à l'ancien comportement, vous pouvez désactiver l'option \"Coordonnées uniformes\" dans l'onglet général. Pour des raisons de compatibilité ascendante, les \"Coordonnées uniformes\" sont automatiquement désactivées lorsque vous ouvrez une ancienne expérience.\n\n### Utiliser Python dans les chaînes de texte\n\nVous pouvez désormais intégrer Python dans les chaînes de texte en utilisant la syntaxe `[=...]`. Par exemple, la chaîne de texte suivante dans un SKETCHPAD :\n\n~~~\nDeux fois deux égale [=2*2]\n~~~\n\n... affichera :\n\n~~~\nDeux fois deux égale 4\n~~~\n\nPour plus d'informations, consultez :\n\n- %link:text%\n\n### Support de Python 3\n\nOpenSesame prend désormais en charge Python >= 3.4. Cependant, plusieurs dépendances d'OpenSesame, notamment PsychoPy et Expyriment, sont uniquement en Python 2. Par conséquent, Python 2.7 reste la version par défaut de Python."
  },
  "Leuphana 2021 workshop": {
    "fr": "Atelier Leuphana 2021"
  },
  "\n<notranslate>[TOC]</notranslate>\n\n\n## Practical information\n\n- Host: Leuphana Universität Lüneburg\n- Location: online\n- Dates: Nov 10, 17, and 24, 13:00 – 17:00\n- Presenter: Sebastiaan Mathôt\n- [Spreadsheet with participant overview](https://docs.google.com/spreadsheets/d/1QCUwNuHX5OkadF4kWj0xp9Aj9NDM7WxWJ0q6NBlUWhI/edit?usp=sharing)\n\n\n## Description\n\nIn this three-day, hands-on, online workshop, you will learn how to implement psychological experiments with the open-source software OpenSesame. You will learn how to run experiments online as well as in a traditional laboratory set-up, and about the limitations and advantages of both approaches. You will also learn how to include eye tracking in laboratory-based experiments. Finally, using the skills that you will learn during the workshop, you will design and implement an experiment for your own research.\n\nPlease install OpenSesame on your computer before the workshop. You can download OpenSesame for free from this link:\n\n- %link:download%\n\nNo prior experience with OpenSesame, Python, or JavaScript is required.\n\nI’m looking forward to meeting you all!\n\n— Sebastiaan\n\n\n## Program \n\n\n### Day 1 (Nov 10): Introduction\n\nSlides: %static:attachments/leuphana2021/leuphana-day-1.pdf%\n\n- 13:00 – 14:30: __Introduction to OpenSesame__. A general introduction to the software OpenSesame, followed by a hands-on tutorial in which you will learn the basic concepts of the software. [Click here](%url:/tutorials/capybara%) to go to the tutorial.\n- Break\n- 15:00 – 16:00: __Introduction to OpenSesame (continued)__.\n- 16:00 – 17:00: __Free time to develop your own experiment.__ What kind of experiment would you like to build for your own research? You will draft a design for your own experiment, which you will implement during the next two days. Use [this spreadsheet](https://docs.google.com/spreadsheets/d/1QCUwNuHX5OkadF4kWj0xp9Aj9NDM7WxWJ0q6NBlUWhI/edit?usp=sharing) to indicate which experiment you'd like to create.\n\n\n### Day 2 (Nov 17): Online experiments\n\nSlides: %static:attachments/leuphana2021/leuphana-day-2.pdf%\n\n- 13:00 – 14:30: __Building an online time-reproduction task.__ We will start this session with a general introduction to online experiments. This is followed by a hands-on tutorial in which you will implement a time-reproduction task that is suitable for running online.\n- Break\n- 15:00 – 16:00: __Using <https://mindprobe.eu> (a JATOS server) to run experiments online.__ In this session, you will learn how to use mindprobe.eu, which is a free server that runs the open-source software JATOS, to actually run an experiment online. A mindprobe.eu account will be provided to each participant.\n- 16:00 – 17:00: __Free time to develop your own experiment.__ During this session, you will continue to work on your own experiment.\n\n\n### Day 3 (Nov 24): Eye-tracking experiments\n\nSlides: %static:attachments/leuphana2021/leuphana-day-3.pdf%\n\n- 13:00 – 14:30: __Building a self-paced reading task with eye-tracking.__ We will start this session with a general introduction to eye tracking. This is followed by a hands-on tutorial in which you will implement a self-paced reading task with basic eye tracking. We will focus on using the EyeLink, which is a specific eye tracker. However, concepts and techniques are largely also applicable to other eye trackers.\nBreak\n- Break\n- 15:00 – 16:00: __Gaze-contingent eye-tracking.__ You will learn how to implement experiments that react to the eye movements of the participant, that is, gaze-contingent experiments.\n- 16:00 – 17:00: __Free time to develop your own experiment.__ During this session, you will continue to work on your own experiment.\n- 17:00 – 17:30: __Q&A.__ We will close the workshop with time for questions and remarks.\n": {
    "fr": "<notranslate>[TOC]</notranslate>\n\n## Informations pratiques\n\n- Hôte : Leuphana Universität Lüneburg\n- Lieu : en ligne\n- Dates : 10, 17 et 24 novembre, de 13h00 à 17h00\n- Animateur : Sebastiaan Mathôt\n- [Tableur avec un aperçu des participants](https://docs.google.com/spreadsheets/d/1QCUwNuHX5OkadF4kWj0xp9Aj9NDM7WxWJ0q6NBlUWhI/edit?usp=sharing)\n\n## Description\n\nDans cet atelier en ligne de trois jours, vous apprendrez à mettre en œuvre des expériences de psychologie avec le logiciel open-source OpenSesame. Vous apprendrez comment réaliser des expériences en ligne ainsi que dans un laboratoire traditionnel, et vous découvrirez les limites et les avantages de ces deux approches. Vous apprendrez également à inclure le suivi oculaire dans les expériences en laboratoire. Enfin, en utilisant les compétences que vous aurez acquises lors de l'atelier, vous concevrez et mettrez en œuvre une expérience pour votre propre recherche.\n\nVeuillez installer OpenSesame sur votre ordinateur avant l'atelier. Vous pouvez télécharger OpenSesame gratuitement à partir de ce lien :\n\n- %link:download%\n\nAucune expérience préalable en OpenSesame, Python ou JavaScript n'est requise.\n\nJ'ai hâte de vous rencontrer tous !\n\n— Sebastiaan\n\n## Programme\n\n### Jour 1 (10 novembre) : Introduction\n\nDiapositives : %static:attachments/leuphana2021/leuphana-day-1.pdf%\n\n- 13h00 – 14h30 : __Introduction à OpenSesame__. Une introduction générale au logiciel OpenSesame, suivie d'un tutoriel pratique dans lequel vous apprendrez les concepts de base du logiciel. [Cliquez ici](%url:/tutorials/capybara%) pour accéder au tutoriel.\n- Pause\n- 15h00 – 16h00 : __Introduction à OpenSesame (suite)__.\n- 16h00 – 17h00 : __Temps libre pour développer votre propre expérience__. Quel type d'expérience aimeriez-vous réaliser pour votre propre recherche ? Vous élaborerez un plan de votre propre expérience, que vous mettrez en œuvre au cours des deux prochains jours. Utilisez [ce tableur](https://docs.google.com/spreadsheets/d/1QCUwNuHX5OkadF4kWj0xp9Aj9NDM7WxWJ0q6NBlUWhI/edit?usp=sharing) pour indiquer l'expérience que vous souhaitez créer.\n\n### Jour 2 (17 novembre) : Expériences en ligne\n\nDiapositives : %static:attachments/leuphana2021/leuphana-day-2.pdf%\n\n- 13h00 – 14h30 : __Création d'une tâche de reproduction du temps en ligne__. Nous commencerons cette session par une introduction générale aux expériences en ligne. Elle sera suivie d'un tutoriel pratique dans lequel vous mettrez en œuvre une tâche de reproduction du temps adaptée pour fonctionner en ligne.\n- Pause\n- 15h00 – 16h00 : __Utilisation de <https://mindprobe.eu> (un serveur JATOS) pour réaliser des expériences en ligne__. Au cours de cette session, vous apprendrez à utiliser mindprobe.eu, qui est un serveur gratuit qui utilise le logiciel open source JATOS, pour mener réellement une expérience en ligne. Un compte mindprobe.eu sera fourni à chaque participant.\n- 16h00 – 17h00 : __Temps libre pour développer votre propre expérience__. Au cours de cette session, vous continuerez à travailler sur votre propre expérience.\n\n### Jour 3 (24 novembre) : Expériences de suivi oculaire\n\nDiapositives : %static:attachments/leuphana2021/leuphana-day-3.pdf%\n\n- 13h00 – 14h30 : __Création d'une tâche de lecture auto-déclenchée avec suivi oculaire__. Nous commencerons cette session par une introduction générale au suivi oculaire. Elle sera suivie d'un tutoriel pratique dans lequel vous mettrez en œuvre une tâche de lecture auto-déclenchée avec un suivi oculaire basique. Nous nous concentrerons sur l'utilisation de l'EyeLink, qui est un dispositif de suivi oculaire spécifique. Toutefois, les concepts et les techniques sont également en grande partie applicables à d'autres dispositifs de suivi oculaire.\n- Pause\n- 15h00 – 16h00 : __Suivi oculaire dépendant du regard__. Vous apprendrez à mettre en œuvre des expériences qui réagissent aux mouvements oculaires des participants, c'est-à-dire des expériences dépendantes du regard.\n- 16h00 – 17h00 : __Temps libre pour développer votre propre expérience__. Au cours de cette session, vous continuerez à travailler sur votre propre expérience.\n- 17h00 – 17h30 : __Questions-réponses__. Nous clôturerons l'atelier par un temps consacré aux questions et remarques."
  },
  "Degrees of visual angle": {
    "fr": "Degrés d'angle visuel"
  },
  "You will often see that the size of visual stimuli is expressed in degrees of visual angle (°). Visual degrees express the angle between the straight lines from the extremities of the stimulus to the eye's lens. Therefore, visual angle is related to the size that a stimulus subtends on the retina, but only indirectly: It is an angle measured from the eye's lens, as illustrated in %FigEye\n\n<notranslate>[TOC]</notranslate>\n\n<notranslate>\nfigure:\n id: FigEye\n source: fig-eye.png\n caption: A schematic illustration of degrees of visual angle. (Image adapted from [WikiMedia Commons](http://commons.wikimedia.org/wiki/File:Schematic_diagram_of_the_human_eye.svg).)\n</notranslate>\n\nThe reason for using this somewhat odd measure of size is that it reflects the perceived size of a stimulus, which in psychological experiments is often more important than its real size. For example, if you present a picture with a real width of 100 pixels on the monitor, the visual angle may correspond to 3°. If you move the monitor further away, the visual angle of the picture will decrease to, say, 2°. Visual angle thus reflects that the distance between a stimulus and an observer is important.\n\nSee also:\n\n- <http://en.wikipedia.org/wiki/Visual_angle>\n\n## Convert pixels to visual degrees\n\nYou will need to know three things in order to convert pixels to visual degrees:\n\n- `h` is the height of the monitor in centimeters, which you can measure with a ruler. (e.g., 25cm)\n- `d` is the distance from the participant to the monitor in centimeters, which you can measure with a ruler. (e.g., 60cm)\n- `r` is the vertical resolution of the monitor in pixels, which you can find in your operating system's display settings (e.g., 768 px)\n\nYou can calculate angular size of your stimulus as shown below. You can execute this script in the OpenSesame debug window. Of course, you need to substitute all values so that they correspond to your setup. Note that a single visual degree typically corresponds to 30 - 60 pixels, depending on the distance and size of the monitor. Conversely, a single pixel typically corresponds to 0.01 to 0.03 visual degrees. If you obtain values that are far outside of this range, you have probably made a mistake.\n\n```python\nfrom math import atan2, degrees\n\nh = 25           # Monitor height in cm\nd = 60           # Distance between monitor and participant in cm\nr = 768          # Vertical resolution of the monitor\nsize_in_px = 100 # The stimulus size in pixels\n# Calculate the number of degrees that correspond to a single pixel. This will\n# generally be a very small value, something like 0.03.\ndeg_per_px = degrees(atan2(.5 * h, d)) / (.5 * r)\nprint(f'{deg_per_px} degrees correspond to a single pixel')\n# Calculate the size of the stimulus in degrees\nsize_in_deg = size_in_px * deg_per_px\nprint(f'The size of the stimulus is {size_in_px} pixels and {size_in_deg} visual degrees')\n```\n\n## Convert visual degrees to pixels\n\nConverting visual degrees to pixels is simply the inverse of the procedure described above, and can be done as follows:\n\n```python\nfrom math import atan2, degrees\nh = 25           # Monitor height in cm\nd = 60           # Distance between monitor and participant in cm\nr = 768          # Vertical resolution of the monitor\nsize_in_deg = 3. # The stimulus size in pixels\n# Calculate the number of degrees that correspond to a single pixel. This will\n# generally be a very small value, something like 0.03.\ndeg_per_px = degrees(atan2(.5 * h, d)) / (.5 * r)\nprint(f'{deg_per_px} degrees correspond to a single pixel')\n# Calculate the size of the stimulus in degrees\nsize_in_px = size_in_deg / deg_per_px\nprint(f'The size of the stimulus is {size_in_px} pixels and {size_in_deg} visual degrees')\n```\n": {
    "fr": "Vous verrez souvent que la taille des stimuli visuels est exprimée en degrés d'angle visuel (°). Les degrés visuels expriment l'angle entre les lignes droites des extrémités du stimulus jusqu'à la lentille de l'œil. Par conséquent, l'angle visuel est lié à la taille qu'un stimulus sous-tend sur la rétine, mais seulement indirectement : c'est un angle mesuré à partir de la lentille de l'œil, comme illustré dans %FigEye\n\n<notranslate>[TOC]</notranslate>\n\n<notranslate>\nfigure:\n id: FigEye\n source: fig-eye.png\n caption: Une illustration schématique des degrés d'angle visuel. (Image adaptée de [WikiMedia Commons](http://commons.wikimedia.org/wiki/File:Schematic_diagram_of_the_human_eye.svg).)\n</notranslate>\n\nLa raison d'utiliser cette mesure de taille quelque peu étrange est qu'elle reflète la taille perçue d'un stimulus, qui dans les expériences psychologiques est souvent plus importante que sa taille réelle. Par exemple, si vous présentez une image avec une largeur réelle de 100 pixels sur le moniteur, l'angle visuel peut correspondre à 3°. Si vous éloignez le moniteur, l'angle visuel de l'image diminuera à, disons, 2°. L'angle visuel reflète ainsi que la distance entre un stimulus et un observateur est importante.\n\nVoir aussi :\n\n- <http://en.wikipedia.org/wiki/Visual_angle>\n\n## Convertir les pixels en degrés visuels\n\nVous devez connaître trois choses pour convertir les pixels en degrés visuels :\n\n- `h` est la hauteur du moniteur en centimètres, que vous pouvez mesurer avec une règle. (par exemple, 25cm)\n- `d` est la distance entre le participant et le moniteur en centimètres, que vous pouvez mesurer avec une règle. (par exemple, 60cm)\n- `r` est la résolution verticale du moniteur en pixels, que vous pouvez trouver dans les paramètres d'affichage de votre système d'exploitation (par exemple, 768 px)\n\nVous pouvez calculer la taille angulaire de votre stimulus comme indiqué ci-dessous. Vous pouvez exécuter ce script dans la fenêtre de débogage d'OpenSesame. Bien sûr, vous devez remplacer toutes les valeurs pour qu'elles correspondent à votre configuration. Notez qu'un degré visuel unique correspond généralement à 30 - 60 pixels, selon la distance et la taille du moniteur. Inversement, un seul pixel correspond généralement à 0,01 à 0,03 degrés visuels. Si vous obtenez des valeurs qui sont bien en dehors de cette plage, vous avez probablement commis une erreur.\n\n```python\nfrom math import atan2, degrees\n\nh = 25           # Hauteur du moniteur en cm\nd = 60           # Distance entre le moniteur et le participant en cm\nr = 768          # Résolution verticale du moniteur\nsize_in_px = 100 # La taille du stimulus en pixels\n# Calculez le nombre de degrés qui correspondent à un seul pixel. Cela sera\n# généralement une valeur très petite, quelque chose comme 0,03.\ndeg_per_px = degrees(atan2(.5 * h, d)) / (.5 * r)\nprint(f'{deg_per_px} degrés correspondent à un seul pixel')\n# Calculez la taille du stimulus en degrés\nsize_in_deg = size_in_px * deg_per_px\nprint(f'La taille du stimulus est de {size_in_px} pixels et {size_in_deg} degrés visuels')\n```\n\n## Convertir les degrés visuels en pixels\n\nConvertir les degrés visuels en pixels est simplement l'inverse de la procédure décrite ci-dessus et peut être fait comme suit :\n\n```python\nfrom math import atan2, degrees\nh = 25           # Hauteur du moniteur en cm\nd = 60           # Distance entre le moniteur et le participant en cm\nr = 768          # Résolution verticale du moniteur\nsize_in_deg = 3. # La taille du stimulus en pixels\n# Calculez le nombre de degrés qui correspondent à un seul pixel. Cela sera\n# généralement une valeur très petite, quelque chose comme 0,03.\ndeg_per_px = degrees(atan2(.5 * h, d)) / (.5 * r)\nprint(f'{deg_per_px} degrés correspondent à un seul pixel')\n# Calculez la taille du stimulus en degrés\nsize_in_px = size_in_deg / deg_per_px\nprint(f'La taille du stimulus est de {size_in_px} pixels et {size_in_deg} degrés visuels')\n```\n"
  },
  "Kurt Lewin Institute Workshop 2020": {
    "fr": "Atelier de l'Institut Kurt Lewin 2020"
  },
  "Release notes for 3.1.7": {
    "fr": "Notes de version pour 3.1.7"
  },
  "\n<notranslate>[TOC]</notranslate>\n\n\n## About this update\n\nOpenSesame 3.1.7 *Jazzy James* is the seventh maintenance release in the 3.1 series. It contains bug fixes and minor improvements, and should be a pleasant and safe upgrade for everyone who is using the 3.1 series.\n\nIf you are upgrading from OpenSesame 3.0 or earlier, please see the list of important changes:\n\n- %link:important-changes-3%\n\n## Credits\n\nThanks to:\n\n- Daniel Schreij (%-- github: {user: dschreij} --%) for the Mac OS package, and his code contributions\n- Jarik den Hartog (%-- github: {user: JdenHartog} --%) for his code contributions\n- %-- github: {user: juliencegarra} --% for his code contributions\n\n## Bug fixes and improvements\n\nopensesame:\n\n- Updated to 3.1.7\n- %-- github: { repo: \"smathot/opensesame\", issue: 495 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 509 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 511 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 513 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 514 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 515 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 516 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 517 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 519 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 520 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 521 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 524 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 525 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 526 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 528 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 529 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 530 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 531 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 534 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 535 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 536 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 538 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 539 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 540 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 541 } --%\n\nqprogedit:\n\n- Updated to 4.0.11\n\ndatamatrix:\n\n- Updated to 0.4.15\n\nqdatamatrix:\n\n- Updated to 0.1.15\n\nqosf:\n\n- Updated to 1.2.2\n\nopensesame-extension-osf\n\n- Updated to 1.1.1\n\n\n## Packages (Windows Python 2.7 package)\n\n\n### Detailed package information": {
    "fr": "<notranslate>[TOC]</notranslate>\n\n## À propos de cette mise à jour\n\nOpenSesame 3.1.7 *Jazzy James* est la septième version de maintenance de la série 3.1. Elle contient des corrections de bugs et des améliorations mineures, et devrait être une mise à niveau agréable et sûre pour tous ceux qui utilisent la série 3.1.\n\nSi vous mettez à niveau depuis OpenSesame 3.0 ou une version antérieure, veuillez consulter la liste des changements importants :\n\n- %link:important-changes-3%\n\n## Crédits\n\nMerci à :\n\n- Daniel Schreij (%-- github: {user: dschreij} --%) pour le package Mac OS et ses contributions au code\n- Jarik den Hartog (%-- github: {user: JdenHartog} --%) pour ses contributions au code\n- %-- github: {user: juliencegarra} --% pour ses contributions au code\n\n## Corrections de bugs et améliorations\n\nopensesame :\n\n- Mise à jour vers 3.1.7\n- %-- github: { repo: \"smathot/opensesame\", issue: 495 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 509 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 511 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 513 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 514 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 515 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 516 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 517 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 519 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 520 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 521 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 524 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 525 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 526 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 528 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 529 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 530 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 531 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 534 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 535 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 536 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 538 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 539 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 540 } --%\n- %-- github: { repo: \"smathot/opensesame\", issue: 541 } --%\n\nqprogedit :\n\n- Mise à jour vers 4.0.11\n\ndatamatrix :\n\n- Mise à jour vers 0.4.15\n\nqdatamatrix :\n\n- Mise à jour vers 0.1.15\n\nqosf :\n\n- Mise à jour vers 1.2.2\n\nopensesame-extension-osf\n\n- Mise à jour vers 1.1.1\n\n## Paquets (Package Windows Python 2.7)\n\n### Informations détaillées sur les paquets"
  },
  "~~~ .yaml\nname: opensesame_3.1.7-py2.7-win32-1\nchannels:\n- cogsci\n- defaults\ndependencies:\n- python==2.7.12\n- anaconda-client=1.4.0=py27_0\n- backports=1.0=py27_0\n- backports_abc=0.4=py27_0\n- bzip2=1.0.6=vc9_3\n- clyent=1.2.2=py27_0\n- arrow=0.7.0=py_0\n- humanize=0.5.1=py_0\n- oauthlib=1.0.3=py_0\n- psychopy=1.82.01=py27_0\n- pygame=1.9.2a0=py27_0\n- pyglet=1.2.4=py27_0\n- python-datamatrix=0.4.15=py_0 # updated in 3.1.7\n- python-fileinspector=1.0.2=py_0 # updated in 3.1.3\n- python-opensesame=3.1.7=py_0 # updated in 3.1.7\n- python-pseudorandom=0.2.2=py27_0\n- python-pygaze=0.6.0a21=py_0 # Updated in 3.1.3\n- python-qdatamatrix=0.1.15=py_0 # updated in 3.1.7\n- python-qnotifications=1.1.1=py_0 # updated in 3.1.3\n- python-qosf=1.2.2=py_0 # updated in 3.1.7\n- python-qprogedit=4.0.11=py_0 # updated in 3.1.7\n- qscintilla2=2.9.1=py27_vc9_0\n- requests-oauthlib=0.6.1=py_0\n- webcolors=1.5=py27_0\n- configparser=3.5.0b2=py27_1\n- decorator=4.0.10=py27_0\n- entrypoints=0.2.2=py27_0\n- freetype=2.5.5=vc9_1\n- functools32=3.2.3.2=py27_0\n- get_terminal_size=1.0.0=py27_0\n- ipykernel=4.3.1=py27_0\n- ipython=4.2.0=py27_0\n- ipython_genutils=0.1.0=py27_0\n- ipywidgets=4.1.1=py27_0\n- jinja2=2.8=py27_1\n- jpeg=8d=vc9_0\n- jsonschema=2.5.1=py27_0\n- jupyter=1.0.0=py27_3\n- jupyter_client=4.3.0=py27_0\n- jupyter_console=4.1.1=py27_0\n- jupyter_core=4.1.0=py27_0\n- libpng=1.6.22=vc9_0\n- libtiff=4.0.6=vc9_2\n- markdown=2.6.6=py27_0\n- markupsafe=0.23=py27_2\n- mistune=0.7.2=py27_0\n- mkl=11.3.3=1\n- nbconvert=4.2.0=py27_0\n- nbformat=4.0.1=py27_0\n- notebook=4.2.1=py27_0\n- numpy=1.11.1=py27_0\n- openssl=1.0.2h=vc9_0\n- path.py=8.2.1=py27_0\n- pickleshare=0.5=py27_0\n- pillow=3.2.0=py27_1\n- pip=8.1.2=py27_0\n- pyflakes=1.2.3=py27_0\n- pygments=2.1.3=py27_0\n- pyopengl=3.1.1a1=np111py27_0\n- pyopengl-accelerate=3.1.1a1=np111py27_0\n- pyqt=4.11.4=py27_6\n- pyreadline=2.1=py27_0\n- pyserial=2.7=py27_0\n- python=2.7.12=0\n- python-dateutil=2.5.3=py27_0\n- pytz=2016.4=py27_0\n- pyyaml=3.11=py27_4\n- pyzmq=15.2.0=py27_0\n- qt=4.8.7=vc9_8\n- qtawesome=0.3.3=py27_0\n- qtconsole=4.2.1=py27_0\n- qtpy=1.0.2=py27_0\n- requests=2.10.0=py27_0\n- scipy=0.17.1=np111py27_1\n- setuptools=23.0.0=py27_0\n- simplegeneric=0.8.1=py27_1\n- singledispatch=3.4.0.3=py27_0\n- sip=4.16.9=py27_2\n- six=1.10.0=py27_0\n- sqlite=3.13.0=vc9_1\n- ssl_match_hostname=3.4.0.2=py27_1\n- tornado=4.3=py27_1\n- traitlets=4.2.1=py27_0\n- vs2008_runtime=9.00.30729.1=2\n- wheel=0.29.0=py27_0\n- yaml=0.1.6=0\n- zlib=1.2.8=vc9_3\n- pip:\n  - cffi==1.7.0\n  - expyriment==0.8.0 # Upgrade manually to 0.8.1.opensesame2\n  - imageio==1.5\n  - mediadecoder==0.1.5\n  - moviepy==0.2.2.11  \n  - opensesame-extension-osf==1.1.1 # Updated in 3.1.7\n  - opensesame-plugin-media-player-mpy==0.1.6\n  - opensesame-windows-launcher==0.4.1\n  - pycparser==2.14\n  - python-bidi==0.4.0\n  - sounddevice==0.3.3\n  - tqdm==4.7.6\n  - pyaudio==0.2.9 # Added in 3.1.3\n  - openpyxl==2.4.0 # Added in 3.1.3\n  - fastnumbers==1.0.0 # Added in 3.1.5\n  - prettytable==0.7.2 # Added in 3.1.5\nprefix: opensesame_3.1.7-py2.7-win32-1\n~~~\n": {
    "fr": "~~~ .yaml\nname: opensesame_3.1.7-py2.7-win32-1\nchannels:\n- cogsci\n- defaults\ndependencies:\n- python==2.7.12\n- anaconda-client=1.4.0=py27_0\n- backports=1.0=py27_0\n- backports_abc=0.4=py27_0\n- bzip2=1.0.6=vc9_3\n- clyent=1.2.2=py27_0\n- arrow=0.7.0=py_0\n- humanize=0.5.1=py_0\n- oauthlib=1.0.3=py_0\n- psychopy=1.82.01=py27_0\n- pygame=1.9.2a0=py27_0\n- pyglet=1.2.4=py27_0\n- python-datamatrix=0.4.15=py_0 # mis à jour dans 3.1.7\n- python-fileinspector=1.0.2=py_0 # mis à jour dans 3.1.3\n- python-opensesame=3.1.7=py_0 # mis à jour dans 3.1.7\n- python-pseudorandom=0.2.2=py27_0\n- python-pygaze=0.6.0a21=py_0 # Mis à jour dans 3.1.3\n- python-qdatamatrix=0.1.15=py_0 # mis à jour dans 3.1.7\n- python-qnotifications=1.1.1=py_0 # mis à jour dans 3.1.3\n- python-qosf=1.2.2=py_0 # mis à jour dans 3.1.7\n- python-qprogedit=4.0.11=py_0 # mis à jour dans 3.1.7\n- qscintilla2=2.9.1=py27_vc9_0\n- requests-oauthlib=0.6.1=py_0\n- webcolors=1.5=py27_0\n- configparser=3.5.0b2=py27_1\n- decorator=4.0.10=py27_0\n- entrypoints=0.2.2=py27_0\n- freetype=2.5.5=vc9_1\n- functools32=3.2.3.2=py27_0\n- get_terminal_size=1.0.0=py27_0\n- ipykernel=4.3.1=py27_0\n- ipython=4.2.0=py27_0\n- ipython_genutils=0.1.0=py27_0\n- ipywidgets=4.1.1=py27_0\n- jinja2=2.8=py27_1\n- jpeg=8d=vc9_0\n- jsonschema=2.5.1=py27_0\n- jupyter=1.0.0=py27_3\n- jupyter_client=4.3.0=py27_0\n- jupyter_console=4.1.1=py27_0\n- jupyter_core=4.1.0=py27_0\n- libpng=1.6.22=vc9_0\n- libtiff=4.0.6=vc9_2\n- markdown=2.6.6=py27_0\n- markupsafe=0.23=py27_2\n- mistune=0.7.2=py27_0\n- mkl=11.3.3=1\n- nbconvert=4.2.0=py27_0\n- nbformat=4.0.1=py27_0\n- notebook=4.2.1=py27_0\n- numpy=1.11.1=py27_0\n- openssl=1.0.2h=vc9_0\n- path.py=8.2.1=py27_0\n- pickleshare=0.5=py27_0\n- pillow=3.2.0=py27_1\n- pip=8.1.2=py27_0\n- pyflakes=1.2.3=py27_0\n- pygments=2.1.3=py27_0\n- pyopengl=3.1.1a1=np111py27_0\n- pyopengl-accelerate=3.1.1a1=np111py27_0\n- pyqt=4.11.4=py27_6\n- pyreadline=2.1=py27_0\n- pyserial=2.7=py27_0\n- python=2.7.12=0\n- python-dateutil=2.5.3=py27_0\n- pytz=2016.4=py27_0\n- pyyaml=3.11=py27_4\n- pyzmq=15.2.0=py27_0\n- qt=4.8.7=vc9_8\n- qtawesome=0.3.3=py27_0\n- qtconsole=4.2.1=py27_0\n- qtpy=1.0.2=py27_0\n- requests=2.10.0=py27_0\n- scipy=0.17.1=np111py27_1\n- setuptools=23.0.0=py27_0\n- simplegeneric=0.8.1=py27_1\n- singledispatch=3.4.0.3=py27_0\n- sip=4.16.9=py27_2\n- six=1.10.0=py27_0\n- sqlite=3.13.0=vc9_1\n- ssl_match_hostname=3.4.0.2=py27_1\n- tornado=4.3=py27_1\n- traitlets=4.2.1=py27_0\n- vs2008_runtime=9.00.30729.1=2\n- wheel=0.29.0=py27_0\n- yaml=0.1.6=0\n- zlib=1.2.8=vc9_3\n- pip:\n  - cffi==1.7.0\n  - expyriment==0.8.0 # Mettre à jour manuellement vers 0.8.1.opensesame2\n  - imageio==1.5\n  - mediadecoder==0.1.5\n  - moviepy==0.2.2.11  \n  - opensesame-extension-osf==1.1.1 # Mis à jour dans 3.1.7\n  - opensesame-plugin-media-player-mpy==0.1.6\n  - opensesame-windows-launcher==0.4.1\n  - pycparser==2.14\n  - python-bidi==0.4.0\n  - sounddevice==0.3.3\n  - tqdm==4.7.6\n  - pyaudio==0.2.9 # Ajouté dans 3.1.3\n  - openpyxl==2.4.0 # Ajouté dans 3.1.3\n  - fastnumbers==1.0.0 # Ajouté dans 3.1.5\n  - prettytable==0.7.2 # Ajouté dans 3.1.5\nprefix: opensesame_3.1.7-py2.7-win32-1\n~~~"
  },
  "The prepare-run strategy": {
    "fr": "La stratégie de préparation-exécution"
  }
}